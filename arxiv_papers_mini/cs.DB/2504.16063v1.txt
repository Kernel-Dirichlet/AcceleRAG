1 A Python Tool for Reconstructing Full News Text from GDELT  
 
Andrea Fronzetti Colladon a, Roberto Vestrelli  b, c 
 
a Department of Civil, Computer Science and Aeronautical Technologies Engineering , Roma Tre 
University, Via della Vasca Navale 79 , 00146  Rome , Italy  
b Department of Engineering, University of Perugia, Via G. Duranti 93, 06125 Perugia, Italy  
c Sloan School of Management, Massachussetts Institute of Technology, 100 Main St, Cambridge, 
MA, 02142, USA  
 
Abstract  
News data have become an essential resource across various disciplines, including economics, 
finance, management, social sciences, and computer science. Researchers leverage newspaper 
articles to study economic trends, market dynamics, corporate strategies , public perception, 
political discourse, and the evolution of public opinion. Additionally, news datasets have been 
instrumental in training large -scale language models, with applications in sentiment analysis, fake 
news detection, and automated news summ arization. Despite their significance, access to 
comprehensive news corpora remains a key challenge. Many full -text news providers, such as 
Factiva and LexisNexis, require costly subscriptions, while free alternatives often suffer from 
incomplete data and transparency issues. This paper presents a novel approach to obtaining full -
text newspaper articles at near -zero cost by leveraging data from the Global Database of Events, 
Language, and Tone (GDELT). Specifically, we focus on the GDELT Web News NGrams 3.0  
dataset, which provides high -frequency updates of n -grams extracted from global online news 
sources. We provide Python code to reconstruct  full-text articles from these n -grams by identifying 
overlapping textual fragments and intelligently merging them. Our method enables researchers to 
access structured, large -scale newspaper data for text analysis while overcoming the limitations of 
existi ng proprietary datasets. The proposed approach enhances the accessibility of news data for 
empirical research, facili tating applications in economic forecasting, computational social science, 
and natural language processing.  
 
Keywords:  GDELT, Online News, Big Data , Python  
 
2 1. Introduction  
Newspaper data present  an extremely valuable resource, as demonstrated by their 
applicability across various  research areas, spanning economics, finance, management, social 
sciences, computational science, and computer science.  
In economics and finance, news has been used to predict stock price reversals (Tetlock, 2007)  
and to examine the impact of negative sentiment toward companies on their financial 
performance  (Tetlock et al., 2008) . Other studies have used news analysis to forecast financial 
markets  (Fronzetti Colladon et al., 2023; Garc√≠a, 2013)  or to explain the cross -section of stock 
returns  (Garc√≠a, 2013) . In the macroeconomic field, newspaper articles have been employed to 
construct economic indicators. For example,  Davis  (2016)  developed an index to quantify 
political uncertainty on a global scale, demonstrating its correlation with recessions and 
employment changes.  
Additionally, news data are widely studied due to their informative role. In this regard, 
numerous studies have investigated the impact of fake news and viral stories, given their power 
to influence public opinion and amplify economic trends (Shiller, 2020) . For example,  Zhang et 
al. (2023)  trained a deep learning model to classify news reliability and detect fake news. 
Because of their ability to shape public opinion, news media have been analyzed for their role in 
corporate brand building and reputation management. Researchers have shown t hat media 
coverage influences IPOs and ICOs by enhancing a company‚Äôs legitimacy in the eyes of 
investors  (Fronzetti Colladon et al., 2025; Pollock & Rindova, 2003) . Additionally, scholars have 
interpreted media presence as a form of public scrutiny: the more media coverage a company or 
political figure receives, the greater the public‚Äôs attention. For example,  K√∂lbel et al.  (2017)  
 
3 demonstrated that media coverage of corporate irresponsibility incidents (e.g., environmental 
accidents and labor scandals) affected a company‚Äôs risk profile.  
In social sciences, communication scholars have long examined the media‚Äôs ability to shape 
public perceptions of a wide range of (Entman, 1993; Ross, 2006; Scheufele, 1999)  and to 
influence which topics gain prominence in social and political discourse  (Dearing & Rogers, 
1996; McCombs, 1997) . Additionally, social scientists have relied on the news to  investigate  
protests  (Jenkins & Perrow, 1977; McAdam, 1999, 1999; Olzak, 1994) , forecast elections  
(Fronzetti Colladon, 2020) , and evaluate the evolution of public opinion (Galambos, 2019) . 
Historically, studies employing news -based data required labor -intensive manual 
analysis of news content. However, recent computational advancements have 
significantly reduced the time and cost of these studies by automating data  and natural 
language processing  (NLP) . As a result, more  studies now utilize large -scale, 
automatically coded news datasets across various disciplines.  
This trend has also emphasized the importance of news data for current research in 
computational linguistics. The advancement of language models has increased the 
demand for large -scale datasets needed for training and validating these models. For 
example,  82% of the raw tokens used to train GPT -3 come from news data  (Brown et 
al., 2020) . Today‚Äôs Large Language Models (LLMs) are employed for automated news 
analysis, ranging from sentiment analysis (Chandra et al., 2024)  to tasks such as 
automated news summarization  (Zhang et al., 2023) . 
Despite the proven importance of news data in empirical research, one of the main 
challenges in their analysis is the access to articles. While providers such as Factiva, 
 
4 LexisNexis, and Event Registry offer full -text access, using these platforms is often 
costly. Free alternatives exist, such as datasets available on Kaggle or GitHub, but 
they frequently suffer from limitations in completeness and transparency regarding 
data collection. Another option consists in relying on providers that do not directly 
offer news text but instead provide pre -calculated text -based metrics, such as 
sentiment scores and other linguistic variables. Notable examples include RavenPack1. 
However, the lack of full -text access has limited the ability to customize analyses and 
has prevented researchers from verifying the reliability of the provided metrics (M. 
Hoffmann et al., 2022) . 
In this study, we present a novel approach implemented in Python to obtain the full 
text of newspaper articles at nearly zero cost by utilizing data provided by GDELT. 
Compared to the sources mentioned above, GDELT offers several advantages: it 
provides gl obal coverage, enables access to news in multiple languages from various 
countries , and includes not only newspaper articles but also content from other media 
sources. Even more importantly, its use is free and accessible through Google 
BigQuery, making it  a fundamental resource for research based on news analysis.  
 
2. GDELT Overview  
GDELT is a vast, open dataset that captures and analyzes news media worldwide 
in real -time. Its system continuously monitors print, broadcast, and online news 
sources in over 100 languages, extracting structured information on global events. By 
 
1 https://www.ravenpack.com/  
 
5 leveraging advanced NLP techniques, GDELT translates, classifies, and organizes 
media content to create a comprehensive record of global events. These events are 
then encoded using the Conflict and Mediation Event Observations (CAMEO) 
framework, which categorizes interactions be tween actors, identifies event types, and 
assigns geographic and temporal markers.  
Studies using GDELT span various domains, including the spread of 
misinformation and fake news during the COVID -19 pandemic  (Bruns et al., 2021, 
2022) ; global news coverage of disasters and refugee crises  (Kwak & An, 2014; 
Thomson et al., 2021) ; the influence of fake news on the online media ecosystem 
during the 2016 U.S. Presidential election  (Guo & Vargo, 2020) ; and the relationship 
between news framing and socio -political events  (Hopp et al., 2020) . Additionally, 
GDELT has been used to analyze protests, revolutions, and other forms of civil unrest  
(Christensen, 2019; Fengcai et al., 2020; C. Hoffmann et al., 2018; Ponticelli & V oth, 
2020; Wu & Gerber, 2017) ;  as well as state repression of such movements  
(Christensen & Garfias, 2018) . Furthermore, researchers have leveraged GDELT to 
examine institutional and civil society responses to the COVID -19 pandemic   (David 
Williams et al., 2021; Fu & Zhu, 2022, 2022; Yuen et al., 2021) . 
Studies using GDELT data have been published in some of the most prestigious 
academic journals, including Science (Wang et al., 2016) , Scientific Reports (Ferreira 
et al., 2021) , The Quarterly Journal of Economics (Campante & Yanagizawa -Drott, 
2018) , and Organization Science (Odziemkowska & Henisz, 2021) . 
 
 
6 2.1. The GDELT News Ngrams Dataset  
The Python code provided alongside this paper leverages one of the many datasets 
provided by GDELT: the Web News NGrams 3.0 Dataset. This dataset includes 
unigrams  and a brief contextual snippet showing  each unigram in context. The data is 
extracted from worldwide online news sources monitored by GDELT since January 1, 
2020, and is updated every 15 minutes. According to GDELT, the dataset covers 42 
billion words of news content in 152 different languages . 
Each entry in the dataset consists of a different ‚Äúunigram ‚Äù, and has several 
metadata fields, including a brief contextual snippet showing the unigram surrounding 
context. These snippets are brief textual fragments preceding and following the 
unigram, allowing for relevance filtering and context determination. Typically, they 
include up to seven words for space -segmented languages or an equivalent amount of 
semantic information for scriptio continua languages. In addition to these contextual 
words, GDELT pr ovides other metadata, such as  the URL of the original article , the 
date when the article was detected by GDELT , the language of the article , and t he 
language type, which can take one of two values:  
‚Äì 1: Indicates that the language uses spaces to segment words, meaning that 
n-grams correspond to words.  
‚Äì 2: Indicates that the language follows a scriptio continua structure (e.g., 
Chinese or Japanese), where words are not separated by spaces, meaning 
that n -grams correspond to characters.  
 
7 The Python code provided with this paper only handles type 1 languages, but we 
plan to extend it to type 2 in the future.  
Each entry‚Äôs unigram is assigned  a position indicator, represented by a decile value 
based on the portion of the article where it appears. This allows researchers to assess 
whether a given word is mentioned at the beginning or end of an article.  
Despite its strengths, the News NGrams dataset presents certain challenges. Its 
main limitation is the inability to access the full corpus of news articles. To address 
this issue, this paper proposes a methodology implemented in an open -source Python 
code that is freely available on GitHub (https://github.com/iandreafc/gdeltnews), 
enabling the reconstruction of news articles ‚Äô text from th e collection of n -grams.  
3. Reconstructing News Text  
The input  used in our code  is a JSON dictionary from the News Ngrams dataset, 
available through the BigQuery API  or directly downloadable from GDELT . Each 
entry represents an n -gram extracted from a news article, along with metadata such as 
its surrounding context ( ‚Äúpre‚Äù and ‚Äúpost‚Äù fields ), URL, language, and a positional 
indicator ( ‚Äúpos‚Äù) that reflects its approximate location in the article, divided into 
deciles . 
Initially, the code organizes entries by their source URL to identify and separate 
those originating  from distinct articles . Each entry's ‚Äúpre‚Äù, ‚Äúngram ‚Äù, and ‚Äúpost‚Äù fields 
are then joined to form a text fragment. While this does  not usually result in a full 
sentence, it gives a snippet , or fragment,  of the article centered around the n -gram.  If 
 
8 the snippet  position is less than 20  (meaning it should appear at the beginning of an 
article)  and it contains the string " / ", it is assumed that this may be a GDELT artifact 
where the end of the article has been appended to the beginning  (in the ‚Äúpre‚Äù field) . In 
this case, the snippet  is split at " / " , and the first part is discarded to remove the 
erroneous content.   
Once all fragments for an article have been created, the corpus of the article is 
reconstructed , starting with a single fragment and iteratively appending or prepending  
the others based on maximum word overlap, guided by the pos values. Although pos is 
only a decile approximation, it helps prevent the incorrect combination of fragments 
from distant parts of the article.  
Finally, the code removes overlapping segments that may have been introduced 
during reconstruction to produce a cleaner version of the article te xt. 
The full code and a more detailed description are provided here: 
https://github.com/iandreafc/gdeltnews . 
 
4. Validation  
To validate our method , we require a benchmark corpus of original news articles to compare 
against the reconstructed articles generated by our pipeline. For this purpose, we use 
EventRegistry (Leban et al., 2014) , a widely adopted news aggregation platform that offers 
access to full -text articles from various  sources. We construct  a benchmark dataset by 
downloading online articles published during the second half of December 2023  (December 15 -
 
9 31, 2023) from several major U.S. news outlets: The New York Times (NYT), CNN, The 
Washington Post (TWP), The Wall Street Journal (WSJ), Bloomberg, and PRNewswire.  
We then collect all available articles from the GDELT Web NGrams 3.0 dataset for the same 
time window. By matching articles across the two datasets using their URLs, we construct a 
merged corpus of 2,211 articles: 167 from Bloomberg, 10 from CNN, 389 from NYT, 493 from 
TWP, 252 from WSJ, and 900 from PRNewswire.  The total number of GDELT entries, 
representing all n -grams, is 3,634,545.  
Before applying any evaluation  metric, we preprocess all articles by cleaning and tokenizing 
the text. This ensures that our comparisons are not affected by formatting differences, 
punctuation, or non -substantive inconsistencies.  
To evaluate how well our method reconstructs the original text, we compare each GDELT -
reconstructed article to its corresponding EventRegistry article using two approaches. First, we 
calculate  the Levenshtein Distance2, which counts the number of single -character edits 
(insertions, deletions, substitutions) needed to transform one string into another .  From the 
Leveshtein distance, we calculate a measure of similarity given by the following formula:  
ùêøùëíùë£ùëíùëõùë† ‚Ñéùë°ùëíùëñùëõ  ùëÜùëñùëöùëñùëôùëéùëüùëñùë°ùë¶  = 1 ‚àí ùêøùëíùë£ùëíùëõùë† ‚Ñéùë°ùëíùëñùëõ  ùëëùëñùë†ùë°ùëéùëõùëêùëí
ùëôùëíùëõ(ùë°1)+ùëôùëíùëõ(ùë°2) 
where ùëôùëíùëõ(ùë°ùë•) denotes the length of the string  ùë•. The formula gives a value of 0 when the 
two strings are completely different and a value of 1 if they are identical.  
 
2 Based on Levenshtein from Python‚Äôs Levenshtein module  (https://rapidfuzz.github.io/Levenshtein/ ) 
 
10 Second, we use the SequenceMatcher class  from Python‚Äôs difflib  module 
(https://docs.python.org/3/library/difflib.html)  to identif y the longest matching sequences 
between two strings, giving more weight to consecutive matches and preserving token order.  
Specifically, SequenceMatcher finds the longest contiguous matching subsequence between two 
strings, then recursively repeats  the process for the substrings before and after that match. This 
continues until no more matching blocks are found. The similarity ratio is then calculated as:  
ùëÜùëíùëûùë¢ùëíùëõùëêùëíùëÄùëéùë°ùëê ‚Ñéùëíùëü ùëÜùëñùëöùëñùëôùëéùëüùëñùë°ùë¶  = (2 √ó ùëÄ) / ùëáùê∂  
where M is the total number of matching characters and TC is the total number of characters 
in both strings combined.  
We selected these approaches over other similarity metrics because they account not only for 
the presence of shared words between two articles but also for the order in which those words 
appear.  This is crucial for our goal: rebuilding the article  body  in the correct sequence from 
fragmented snippets . Since one of the core challenges of working with these text fragments  is 
that they are unordered and partially overlapping, our method focuses specifically on 
reconstructing the correct order of words and sentences. For this reason, metrics that are sensitive 
to word order are especially suitable for validating the method.  
Of course, some differences between the EventRegistry and GDELT versions of the same 
article may be unrelated to our reconstruction algorithm. These can include differences in 
scraping methods (e.g., one may omit the title), updates or edits made to the ar ticle over time, or 
inconsistencies in how the content is segmented.  
 
11 To assess our method under different conditions, we conduct two analyses. First, we compute 
similarity metrics for all matched article pairs without any filtering. Second, we restrict the 
comparison to article pairs that share a minimum number of tokens  (using Jaccard Similarity to 
quantify the percentage of overlapping tokens) , regardless of order. The idea is that if two articles 
share most of their content, they likely represent the same version of the text and can serve as a 
stronger test of our method‚Äô s ability to reconstruct token order.  
 
 Common tokens (%)  
Metric  No Filter  >60%  >70%  >80%  
Levenshtein  
Similarity  0.76 0.92 0.94 0.95 
SequenceMatcher  
Similarity  0.74 0.92 0.93 0.95 
Table 1 . Validation results . 
Results are shown in Table 1. Without filtering, the average similarity between reconstructed and 
original articles is 0.75 ( Levenshtein) and 0.73 (SequenceMatcher). When we filter for article 
pairs that share at least 60% of tokens, similarity rises to 0.92 for both metrics. At 70% token 
overlap, it increases further to 0.9 4 and 0.93 , and when using the 80% threshold  of minimum  
token overlap, similarity reaches 0.9 5 for both metrics .  
These findings strongly support the validity of our reconstruction method. Even in the 
presence of minor noise, missing metadata, or article updates, our approach reliably rebuilds the 
 
12 article structure with high fidelity. In cases where the original and reconstructed articles are very 
likely to reflect the same underlying version, the similarity is nearly perfect. This confirms that 
our method not only captures the main content but also  preserves the structure and flow of the 
original text ‚Äî a key requirement for any downstream application involving natural language 
analysis .  
 
5. Conclusion  
In this paper, we introduced a method for reconstructing the full body  of news 
articles using  the GDELT‚Äôs Web News NGrams 3.0 dataset. Our open -source Python 
code allows researchers to generate structured, large -scale newspaper text data at 
nearly zero cost, facilitating a wide range of applications in economics, social 
sciences, and natural language processing.  
The codebase is actively maintained and designed to be modular and extensible. 
For example , it can be easily adapted to include filters based on specific news sources 
by leveraging the URL field provided by GDELT. This makes the tool flexible for 
targeted research use cases.  
However, the approach is not without limitations. Notably, article titles are not yet 
included in the GDELT dataset, and based on current information, we do not expect 
them to be available in the near future. This limits the metadata available for 
organizing and referencing the reconstructed articles. Additionally, the single -process  
version  of our code can be slow, especially when processing large volumes of data.  
While this is acceptable for exploratory or small -scale research, future improvements 
 
13 could involve automating the pipeline to run at regular intervals ‚Äîe.g., every 15 
minutes in sync with GDELT‚Äôs update frequency ‚Äîto enable near real -time 
processing.  In addition, we provide a code version that runs in parallel to solve the 
speed problem.  
Looking ahead, we plan to extend the code to support languages that do not use 
space -based word segmentation (e.g., Chinese or Japanese) and to improve the code 
efficiency and precision . These extensions aim to support the open dissemination of 
information and facilitate free access to high -quality research materials for scholars 
worldwide . 
References  
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, 
P., Sastry, G., Askell, A., & others. (2020). Language models are few -shot learners. 
Advances in Neural Information Processing Systems , 33, 1877 ‚Äì1901.  
Bruns, A., Harrington, S., & Hurcombe, E. (2021). Coronavirus conspiracy theories: Tracing 
misinformation trajectories from the fringes to the mainstream. Communicating COVID -
19: Interdisciplinary Perspectives , 229 ‚Äì249. 
Bruns, A., Hurcombe, E., & Harrington, S. (2022). Covering conspiracy: Approaches to reporting 
the COVID/5G conspiracy theory. Digital Journalism , 10(6), 930 ‚Äì951. 
Campante, F., & Yanagizawa -Drott, D. (2018). Long -range growth: Economic development in the 
global network of air links. The Quarterly Journal of Economics , 133(3), 1395 ‚Äì1458.  
 
14 Chandra, R., Zhu, B., Fang, Q., & Shinjikashvili, E. (2024). Large language models for sentiment 
analysis of newspaper articles during COVID -19: The Guardian. arXiv Preprint 
arXiv:2405.13056 . 
Christensen, D. (2019). Concession stands: How mining investments incite protest in Africa. 
International Organization , 73(1), 65 ‚Äì101. 
Christensen, D., & Garfias, F. (2018). Can you hear me now? How communication technology 
affects protest and repression. Quarterly Journal of Political Science , 13(1), 89.  
David Williams, O., Yung, K. C., & Gr√©pin, K. A. (2021). The failure of private health services: 
COVID -19 induced crises in low -and middle -income country (LMIC) health systems. 
Global Public Health , 16(8‚Äì9), 1320 ‚Äì1333.  
Davis, S. (2016). An Index of Global Economic Policy Uncertainty  (No. w22740; p. w22740). 
National Bureau of Economic Research. https://doi.org/10.3386/w22740  
Dearing, J. W., & Rogers, E. M. (1996). Agenda -setting  (V ol. 6). Sage.  
Entman, R. M. (1993). Framing: Toward Clarification of a Fractured Paradigm. Journal of 
Communication , 43(4), 51 ‚Äì58. https://doi.org/10.1111/j.1460 -2466.1993.tb01304.x  
Fengcai, Q., Jinsheng, D., & Li, W. (2020). An online framework for temporal social unrest event 
prediction using news stream. 2020 International Conference on Cyber -Enabled 
Distributed Computing and Knowledge Discovery (CyberC) , 176 ‚Äì182. 
Ferreira, L. N., Hong, I., Rutherford, A., & Cebrian, M. (2021). The small -world network of global 
protests. Scientific Reports , 11(1), 19215.  
 
15 Fronzetti Colladon, A. (2020). Forecasting election results by studying brand importance in online 
news. International Journal of Forecasting , 36(2), 414 ‚Äì427. 
https://doi.org/10.1016/j.ijforecast.2019.05.013  
Fronzetti Colladon, A., Grassi, S., Ravazzolo, F., & Violante, F. (2023). Forecasting financial 
markets with semantic network analysis in the COVID‚Äê19 crisis. Journal of Forecasting , 
42(5), Article 5. https://doi.org/10.1002/for.2936  
Fronzetti Colladon, A., Toschi, L., & Ughetto, E. (2025). The power of narrative: Engaging 
different audiences through media content in ICOs. Venture Capital , 1‚Äì19. 
https://doi.org/10.1080/13691066.2025.2469503  
Fu, K., & Zhu, Y . (2022). Did the world overlook the media‚Äôs early warning of COVID -19? In 
COVID -19 (pp. 215 ‚Äì219). Routledge.  
Galambos, L. (2019). The public image of big business in America, 1880 -1940: A quantitative 
study in social change . JHU Press.  
Garc√≠a, D. (2013). Sentiment during Recessions. The Journal of Finance , 68(3), 1267 ‚Äì1300. 
https://doi.org/10.1111/jofi.12027  
Guo, L., & Vargo, C. (2020). ‚ÄúFake news‚Äù and emerging online media ecosystem: An integrated 
intermedia agenda -setting analysis of the 2016 US presidential election. Communication 
Research , 47(2), 178 ‚Äì200. 
Hoffmann, C., Molter, H., & Sorge, M. (2018). The Parameterized Complexity of Centrality 
Improvement in Networks. In A. Tjoa, L. Bellatreche, S. Biffl, J. van Leeuwen, & J. 
Wiedermann (Eds.), SOFSEM 2018: Theory and Practice of Computer Science  (pp. 111 ‚Äì
124). Springer. https://doi.org/10.1007/978 -3-319-73117 -9_8 
 
16 Hoffmann, M., Santos, F. G., Neumayer, C., & Mercea, D. (2022). Lifting the veil on the use of 
big data news repositories: A documentation and critical discussion of a protest event 
analysis. Communication Methods and Measures , 16(4), 283 ‚Äì302. 
Hopp, F. R., Fisher, J. T., & Weber, R. (2020). Dynamic transactions between news frames and 
sociopolitical events: An integrative, hidden markov model approach. Journal of 
Communication , 70(3), 335 ‚Äì355. 
Jenkins, J. C., & Perrow, C. (1977). Insurgency of the powerless: Farm worker movements (1946 -
1972). American Sociological Review , 249 ‚Äì268. 
K√∂lbel, J. F., Busch, T., & Jancso, L. M. (2017). How Media Coverage of Corporate Social 
Irresponsibility Increases Financial Risk: Media Coverage of Corporate Social 
Irresponsibility. Strategic Management Journal , 38(11), 2266 ‚Äì2284. 
https://doi.org/10.1002/smj.2647  
Kwak, H., & An, J. (2014). Understanding news geography and major determinants of global news 
coverage of disasters. Computation + Journalism Symposium , 1‚Äì7. 
Leban, G., Fortuna, B., Brank, J., & Grobelnik, M. (2014). Event registry: Learning about world 
events from news. Proceedings of the 23rd International Conference on World Wide Web , 
107‚Äì110. 
McAdam, D. (1999). Political process and the development of black insurgency, 1930 -1970 . 
University of Chicago Press.  
McCombs, M. (1997). Building Consensus: The News Media‚Äôs Agenda -Setting Roles. Political 
Communication , 14(4), Article 4. https://doi.org/10.1080/105846097199236  
 
17 Odziemkowska, K., & Henisz, W. J. (2021). Webs of influence: Secondary stakeholder actions and 
cross -national corporate social performance. Organization Science , 32(1), 233 ‚Äì255. 
Olzak, S. (1994). The dynamics of ethnic competition and conflict . Stanford University Press.  
Pollock, T. G., & Rindova, V . P. (2003). Media Legitimation Effects in the Market for Initial Public 
Offerings. Academy of Management Journal , 46(5), 631 ‚Äì642. 
https://doi.org/10.5465/30040654  
Ponticelli, J., & V oth, H. -J. (2020). Austerity and anarchy: Budget cuts and social unrest in Europe, 
1919 ‚Äì2008. Journal of Comparative Economics , 48(1), 1 ‚Äì19. 
Ross, S. (2006). Don‚Äôt Think of an Elephant: Know Your Values and Frame the Debate. Melbourne 
Journal of Politics , 31, 145 ‚Äì149. 
Scheufele, D. A. (1999). Framing as a Theory of Media Effects. Journal of Communication , 49(1), 
103‚Äì122. https://doi.org/10.1111/j.1460 -2466.1999.tb02784.x  
Shiller, R. J. (2020). Narrative economics: How stories go viral and drive major economic events . 
Tetlock, P. C. (2007). Giving Content to Investor Sentiment: The Role of Media in the Stock 
Market. The Journal of Finance , 62(3), 1139 ‚Äì1168. https://doi.org/10.1111/j.1540 -
6261.2007.01232.x  
Tetlock, P. C., Saar -Tsechansky, M., & Macskassy, S. (2008). More Than Words: Quantifying 
Language to Measure Firms‚Äô Fundamentals. The Journal of Finance , 63(3), 1437 ‚Äì1467. 
https://doi.org/10.1111/j.1540 -6261.2008.01362.x  
Thomson, R., Hussain, M. N., Dancy, C., & Pyke, A. (2021). Social, Cultural, and Behavioral 
Modeling: 14th International Conference, SBP -BRiMS 2021, Virtual Event, July 6 ‚Äì9, 
2021, Proceedings  (V ol. 12720). Springer Nature.  
 
18 Wang, W., Kennedy, R., Lazer, D., & Ramakrishnan, N. (2016). Growing pains for global 
monitoring of societal events. Science , 353(6307), 1502 ‚Äì1503.  
Wu, C., & Gerber, M. S. (2017). Forecasting civil unrest using social media and protest 
participation theory. IEEE Transactions on Computational Social Systems , 5(1), 82 ‚Äì94. 
Yuen, S., Cheng, E. W., Or, N. H., Gr√©pin, K. A., Fu, K. -W., Yung, K. -C., & Yue, R. P. (2021). A 
tale of two city -states: A comparison of the state -led vs civil society -led responses to 
COVID -19 in Singapore and Hong Kong. Global Public Health , 16(8‚Äì9), 1283 ‚Äì1303.  
Zhang, T., Ladhak, F., Durmus, E., Liang, P., McKeown, K., & Hashimoto, T. B. (2023). 
Benchmarking Large Language Models for News Summarization  (No. arXiv:2301.13848). 
arXiv. https://doi.org/10.48550/arXiv.2301.13848