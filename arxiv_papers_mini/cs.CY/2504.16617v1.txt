arXiv:2504.16617v1  [cs.CR]  23 Apr 2025Security Science (SecSci)
— Basic Concepts and Mathematical Foundations —
Dusko Pavlovic and Peter-Michael Seidel
Tamper-resistant tokens and their clay envelope from Uruk, 4500 BC

Contents
Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . v
1 Introduction: On bugs and elephants . . . . . . . . . . . . . . . . . . . 1
2 Security concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
3 Static resource security: access control and multi-level security . . . 21
4 Dynamic resource security: authorization and availabili ty. . . . . . . 35
5 Geometry of security⋆. . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
6 Relational channels and noninterference . . . . . . . . . . . . . . . . . 63
7 Communication channels, protocols, and authentication . . . . . . . . 83
8 Information channels and secrecy . . . . . . . . . . . . . . . . . . . . . 107
9 Privacy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
Epilogue . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
Appendix A. Prerequisites . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164
iii

Preface
Origins. This book originates from lecture notes started by the ﬁrst a uthor in Oxford in 2008
and developed by the coauthors together in Hawaii since 2015 . The lecture notes evolved in
style and content, but the approach to security through scie nce persisted and did not evolve
much.
The idea of security science originates from the research ne twork initiated by Brad Martin of
the NSA in 2008, allegedly in response to the questions asked by a member of the NSA Science
Board:
Why is it that communications security is based on a science, and for cybersecurity,
we are competing who will hire more hackers? Why is there no ge neral science of
security?
Background. At the time, many software and protocol development e ﬀorts were plagued by
a remarkable phenomenon: carefully designed protocols, th oroughly scrutinized and analyzed
for years by top expert committees, would ﬁnally get standar dized after everyone agreed that
they were secure — only to turn out to be insecure upon deploym ent1. The vulnerabilities
varied from case to case and the only commonality was that the y were abstracted away from
the analyzed models. This seemed hard to avoid since you cann ot model everything. So each
case was dismissed as a ﬂuke. For an outsider coming from math ematics, though, the regularity
with which security experts were ﬁnding attacks on systems a nd protocols that other security
experts had proven secure was mystifying [34, 43, 48, 53]. In most cases, both the security
proofs and the insecurity proofs were logically sound, just talking about diﬀerent things. Time
after time.
Revelation. On this background, the concept of security science came as a revelation. It
suggested that security claims should not be viewed as mathematical theore ms but as scientiﬁc
theories !
A mathematical theorem is a statement about a mathematical m odel, derived from mathematical
axioms. It remains valid as long as the axioms are considered valid. This may be forever, or
until more interesting axioms are encountered.
A scientiﬁc theory is a statement about reality, derived fro m the hypotheses that survive ex-
perimental testing. It remains valid until new tests or obse rvations disprove it. For example,
Newton’s theory of gravitation survived for 350 years, unti l it was observed that the orbit of
Mercury disproved it. Einstein’s General Relativity impro ved it. While mathematicians work
1The examples include the protocols from the IPSec IKE [1, 28, 29, 33, 46], and the IPSec GDoI [41, 42, 61]
protocol suites, as well as the relatives of the MQV protocol [31, 34, 37], which kicked o ﬀthe long series of
“another look” articles [32], questioning the utility of th e product line of security proofs and experts, and in
some cases even its soundness.
v
toprove their theorems, scientists work to disprove and improve their theories [35, 51]. The
idea of Security Science is that security is like science . This is spelled out in Sec. 1.2.5.
Latency. However, the research network for the science of security di d not pursue this idea.
The leaders of the community argued that the security models that they studied in the 1980s
were science, and the community followed suit. No one should be blamed for this. Community
behaviors, of course, arise from community dynamics, not fr om individual qualities and short-
comings. The dynamics of security research are driven by inc entives that do not always align
to make the world more secure.
Drift. History is a quest for security. Every war is fought to secure something. In the year
1990, the US won the Cold War. The nuclear threat disappeared from one day to another. The
Internet, developed as the communication infrastructure c apable of surviving the fragmentation
caused by a nuclear war, was released to the general public. E mail brought together social and
professional lives and the World Wide Web elevated the greed -is-good spirit of Reaganomics
into a global, economy-driven engine of science, culture, a nd technology. The times of dark
libraries with dusty books were behind us. All the informati on you could ever need was under
the tips of your ﬁngers. You could type a question on the keybo ard; the world would process
it and respond on the screen. The world could display a questi on on the screen; you would
process it and respond on the keyboard. The world became a com puter. Security became
computer security. Privacy became data ownership.
Throughout history, security has been the job of guards, gen erals, and diplomats. Since 1990,
security became a career choice of software engineers. The e choes of this narrow view still
dominate security education. Security professionals coll ect professional certiﬁcates issued by
software giants, who in the meantime rule the world. Wars are fought in cyberspace. Political
movements are implemented in social networks. Physical sec urity is based on cybersecurity.
Parents rely on cyber devices for their children’s security , while predators rely on the same
devices for prey. Security is a natural process. Together wi th everyone else, security researchers
and their students are grains of sand in the rising sandstorm s of their subject.
Studying security. This book is an eﬀort to study security itself. Some of the obstacles to
thinking about security on its own are that a part of it (natio nal security) doesn’t like to be
observed, whereas another part (security industry) is hamp ered by the Principal-Agent Prob-
lem2. But we owe it to the students to try. It is unlikely to help the m get any particular security
certiﬁcates, but we hope that it will help them understand th e underlying processes of security.
Or at least that it will help some of them to write a better book .
Thanks and apologies
2Look it up if you are not sure what it is! The Agent defends the P rincipal from risks and becomes Principal’s
only source of information about the risks. Praetorian Guar d was hired to defend Roman emperors, but ended
up auctioning the position of Roman emperor for 400 years.
vi
1 Introduction: On bugs and elephants
1.1 On security engineering . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.1.1 What is security engineering? . . . . . . . . . . . . . . . . . . . 2
1.1.2 Where did security engineering come from? . . . . . . . . . . . . 2
1.1.3 Where is security engineering going? . . . . . . . . . . . . . . . 2
1.2 On security science . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.2.1 Security space . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.2.2 Range of security science . . . . . . . . . . . . . . . . . . . . . 5
1.2.3 The Earth is ﬂat, and its perimeter is secure . . . . . . . . . . . . 6
1.2.4 Science and deceit . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.2.5 The best-kept secret . . . . . . . . . . . . . . . . . . . . . . . . 7
A number of blind men came to an elephant. Somebody told them t hat it
was an elephant. The blind men asked, "What is the elephant li ke?" and
they began to touch its body. One of them said: "It is like a pil lar." This
blind man had only touched its leg. Another man said, "The ele phant is
like a husking basket." This person had only touched its ears . Similarly,
he who touched its trunk or its belly talked of it di ﬀerently.
Ramakrishna Paramahamsa
1.1 On security engineering
Security means many things to many people. For a software eng ineer, it often means that
there are no buﬀer overﬂows or dangling pointers in the code. For a cryptogra pher, it means
that any successful attack on the cipher can be reduced to an a lgorithm for computing discrete
logarithms, or to integer factorization. For a diplomat, se curity means that the enemy cannot
read the conﬁdential messages. For a credit card operator, i t means that the total costs of the
fraudulent transactions and of the measures to prevent them are low, relative to the revenue.
For a bee, security means that no intruder into the beehive wi ll escape her sting. . .
Is it an accident that all these di ﬀerent ideas go under the same name? What do they really have
in common? They are studied in di ﬀerent sciences, ranging from computer science to biology,
by a wide variety of di ﬀerent methods. Would it be useful to study them together?
1
1.1.1 What is security engineering?
If all avatars of security have one thing in common, it is sure ly the idea that there are attackers
out there . All security concerns, from computation to politics and bi ology, come down to
averting the adversarial processes in the Environment, tha t are poised to subvert the goals of
the System. There are, for instance, many kinds of bugs in sof tware, but only those that the
hackers use are a security concern.
In all engineering disciplines, the System guarantees a fun ctionality, provided that the Envi-
ronment satisﬁes some assumptions. This is the standard assume-guarantee format of the en-
gineering correctness statements. Such statements are use ful when the Environment is passive
so that the assumptions about it remain valid for a while. The essence of security engineering
is that the Environment actively seeks to invalidate the Sys tem’s assumptions.
Security is thus an adversarial process . In all engineering disciplines, failures usually arise
from engineering errors. In security, failures arise in spite of compliance with the best en-
gineering practices of the moment. Failures are the ﬁrst class citizens of security. In most
software systems, we normally expect security updates, whi ch usually arise from attacks, and
often inspire them.
1.1.2 Where did security engineering come from?
The earliest examples of security technologies are found am ong the earliest documents of civ-
ilization. Fig. 1.1 shows security tokens with tamper prote ction technology from almost 6000
years ago. Fig. 1.2 depicts the situation where this technol ogy was probably used. Alice has a
lamb and Bob has built a secure vault, perhaps with multiple s ecurity levels, spacious enough
to store both Bob’s and Alice’s assets. For each of Alice’s as sets deposited in the vault, Bob
issues a clay token, with an inscription identifying the ass et. Alice’s tokens are then encased
into a bulla , a round, hollow "envelope" of clay, which is then baked to pr event tampering.
When she wants to withdraw her deposits, Alice submits her bu lla to Bob, he breaks it, extracts
the tokens, and returns the goods. Alice can also give her bul la to Carol, who can also submit
it to Bob, to withdraw the goods, or to pass it on to Dave. Bullæ can thus be traded, and they
facilitate an exchange economy. The tokens used in the bullæ evolved into the earliest forms
of money, and the inscriptions on them led to the earliest num eral systems, as well as to the
Sumerian cuneiform script, which was one of the earliest alp habets. Security thus predates
literature, science, mathematics, and even money.
1.1.3 Where is security engineering going?
Throughout history, security technologies evolved gradua lly, serving the purposes of war and
peace, and protecting public resources and private propert y. As computers pervaded all aspects
of social life, security became interlaced with computatio n, and security engineering came to
be closely related to computer science. The developments in the realm of security are nowa-
2
Figure 1.1: Tamper protection from 3700
BC: Bulla with tokens from
Uruk (Iraq)
Figure 1.2: To withdraw her sheep from
Bob’s secure vault, Alice sub-
mits a tamper-proof token, like
in Fig. 1.1.
days inseparable from the developments in the realm of compu tation. The most notable such
development is the cyberspace .
A brief history of cyberspace
In the beginning, engineers built computers and wrote progr ams to control computations. The
platform of computation was the computer, and it was used to e xecute algorithms and calcu-
lations, allowing people to discover, e.g., the fractals, a nd to invent compilers, that allowed
them to write and execute more algorithms and more calculati ons more eﬃciently. Then the
operating system became the platform of computation, and so ftware was developed on top of
it. The era of personal computing and enterprise software br oke out. And then the Internet
happened, followed by cellular networks, and wireless netw orks, and ad hoc networks, and
mixed networks. Cyberspace emerged as the distance-free space of instant, c ostless communi-
cation. Nowadays, software is developed to run in cyberspace. The We b is, strictly speaking,
just a software system, albeit a formidable one. A botnet is a lso a software system. As social
space blends with cyberspace, many social (business, colla borative) processes can be usefully
construed as software systems, that run on social networks a s hardware. Many social and
computational processes become inextricable. Table 1.1 su mmarizes the crude picture of the
paradigm shifts that led to this remarkable situation.
But as every person got connected to a computer, and every com puter to a network, and ev-
ery network to a network of networks, computation became int erlaced with communication
and ceased to be programmable. The functioning of the Web and of web applications is not
determined by the code in the same sense as in a traditional so ftware system: after all, web
applications do include human users as a part of their runtim e. The fusion of social and compu-
tational processes gave rise to the new sociotechnical spac e and led to new kinds of information
processing, where the purposeful program executions at the network nodes are supplemented
by the spontaneous data-driven evolution of network links. While the network emerges as the
3
age ancient times middle ages modern times
platform computer operating system network
applications Quicksort, compilers MS Word, Oracle WWW, botnets
requirements correctness, termination liveness, safety trust, privacy
tools programming languages speciﬁcation languages scripting languages
Table 1.1: Paradigms of computation
new computer, data and metadata become inseparable, and new kinds of security problems
arise.
A brief history of cybersecurity
In early computer systems, security tasks were mainly conce rned with sharing of the computing
resources. In computer networks, security goals expanded t o include information protection.
Those developments are reﬂected in the structure of the book , which progresses from resources
to information. Both computer security and information sec urity depend on a clear distinction
between the secure areas and the insecure areas, separated b y a security perimeter. Security
engineering caters to both by providing tools and methods fo r building security perimeters. In
cyberspace, the secure areas are separated from the insecur e areas by “walls” of cryptography;
and they are connected by the “gates” of protocols.
But as networks of computers and devices spread through phys ical and social spaces, the dis-
tinctions between the secure and the insecure areas become b lurred. In such areas of the so-
age middle ages modern times postmodern times
space computer center cyberspace sociotechnical space
assets computing resources information attention
requirements availability, authorization integrity, conﬁdentiality trust, privacy
tools locks, tokens, passwords cryptography, protocols search and intelligence
Table 1.2: Paradigms of security
ciotechnical space, information processing does not yield to programming anymore. It cannot
be secured just by cryptography and protocols. Security can not be assured anymore by the
engineering methodologies alone. Data mining, concept ana lysis, search, and intelligence span
and cover new spaces. Like our cities, our sociotechnical sp aces were originally built by engi-
neers, but life took over, and some of our problems cannot be e ngineered away anymore. Enter
science.
4
1.2 On security science
Science inputs processes that exist in nature and outputs th eir descriptions. Engineering inputs
World Mindscience
engineering
Figure 1.3: Civilization as conversation
descriptions of processes that do not exist in nature and out puts their realizations. Fig. 1.3
shows the conversation between the two which gave rise to our civilization.
1.2.1 Security space
In the beginning, computation was engineered: we built comp uters and wrote programs. After
we built lots of computers and wrote lots of programs, we conn ected them, and cyberspace
emerged from the Big Bang of the Internet. Cyberspace expand ed into many galaxies, and
life emerged in it: viruses and chatbots, inﬂuencers and cel ebrities, global media, and genomic
databases. Cyberspace is the space where we live. It was buil t by engineers, just like our cities,
but it took a life of its own. Cyberspace is the space of natural processes .
Every science is characterized by its space. Natural scienc es place the world in physical space,
connected by light, full of dark matter and energy. Security science deals with the world in
cyberspace, connected by networks, full of lies and deceit. The physical space is described
using vectors and coordinate systems, cyberspace is struct ured by traces and protocols. This
book provides an overview. The early chapters are suitable f or a ﬁrst course. Later chapters
have been used in advanced courses. Research problems lurk t hroughout.
1.2.2 Range of security science
The challenge of security is that it deﬁes spatial intuition s:we are used to thinking locally,
whereas security requirements concern non-local interact ions. Strategic thinking about secu-
rity requires expanding our views beyond our local horizons and taking into account the non-
local interactions. Formal models are not just convenient m eans to increase precision. They
are indispensable tools for non-local reasoning, just like airplanes are our indispensable tool
for ﬂying. Without such tools, we could not overcome our limi tations.
Using tools that overcome our limitations is, of course, a ch allenge in itself. It requires science.
5
We need science to build airplanes. Taming a horse requires a form of science: developing
a common language with the horse, interpreting his behavior , hypotheses, experiments, and
better hypotheses. Understanding other people, members of our community, and members of
distant cultures, requires science. Interacting with thie ves, attackers, deceivers, inﬂuencers, and
all kinds of security experts, including textbook writers, requires a science. You need security
science even to secure science.
Science consists of theories and experiments. Theories and experiments need intuitive inter-
faces. The geometric view provides an intuitive interface f or security science.
1.2.3 The Earth is ﬂat, and its perimeter is secure
The problem with security is that things are not what they see m. A website provides free advice
for dog owners, but tracks them and sells their private infor mation. A bank oﬀers a credit line
for recent graduates but repossesses one in ﬁve of their hous es. You look around, and you see
that the Earth is obviously ﬂat. The horizons where valleys a nd oceans meet the sky are straight.
If Earth wasn’t ﬂat, oceans would ﬂow like rivers. It’s obvio usly ﬂat. To understand that the
Earth is round, you need science. To understand security, yo u also need science. Security
Science. We call it SecSci.
Security science is easier than other sciences because it is more intuitive. We have a better
sense for lies than for the roundness of the Earth. But securi ty science is also harder than
other sciences because its subject does not like to be observ ed. National security requires
secrecy. Personal security requires privacy. Enterprise s ecurity requires top dollars to be paid
to security experts. Otherwise, they say, other security ex perts, called hackers, will penetrate
your defenses, and you will have to pay them more. You have to p ay the good guys to defend
you from the bad guys. But if the defenders are rational, they have to maximize their revenue,
and they charge you just a penny less than what the attackers w ould steal from you. And the
estimates of how much the attackers would steal are provided by the defenders, as a free service.
So for all you know, they may be charging you a penny more than t he attackers would steal.
1.2.4 Science and deceit
Science begins from the idea that testing reality and elimin ating false theories is a good thing.
The underlying assumption is that reality strikes against f alse beliefs sooner or later. Employing
science, our species took control of large parts of reality. However, the assumption that reality
strikes against false beliefs is just another theory. In rea lity, disproving false beliefs takes time
and resources. If time is short and the resources are scarce, deception is a rational strategy.
While science is the e ﬀort to ﬁnd and disprove unintentionally false hypotheses, s ecurity is
the eﬀort to ﬁnd and disprove intentional deceptions. Scientiﬁc h ypotheses are the simplest
explanations of past observations, chosen so that they are e asy to test and disprove. Deceptions
are the explanations beneﬁcial for the deceiver, chosen so t hat they are hard to test and disprove.
6
The rational strategy is to apply one to the other. Science an d deceit are the only ones who stand
a chance against each other.
1.2.5 The best-kept secret
Feynman on science. "If we have a deﬁnite theory, from which we can compute the con se-
quences which can be compared with experiment, then in princ iple we can prove that theory
wrong. But notice that we can never prove it right! Suppose th at you invent a theory, calculate
the consequences, and discover every time that the conseque nces agree with the experiment.
The theory is then right? No, it is simply not proven wrong. In the future you could compute
a wider range of consequences, there could be a wider range of experiments, and you might
then discover that the thing is wrong. [. . . ] We never are deﬁnitely right; we can only be sure
when we are wrong. "[26]
The best-kept secret of science is thus that it does not provi de
persistent laws, or deﬁnite assertions of truth. Science on ly
provides methods to disprove and improve theories.
The best-kept secret of security is that all security claims have
a lifetime. In cryptography, the fact that every key has a lif e-
time is well known. But the empiric fact that security proto-
cols regularly fail is not well understood and is often recei ved
with astonishment. How can a veriﬁed security protocol stil l
fail? Just like every scientiﬁc theory is stated with respec t to
a given set of observables, which may need to be expanded in
the future, every security claim is stated with respect to a g iven
system and attack model, which may need to be reﬁned.
Feynman never said much about security, but if he did, he coul d have said something like this:
"Feynman on security". If we have a precisely deﬁned security claim about a system, f rom
which we can derive the consequences which can be tested, the n in principle we can prove
that the system is insecure. But notice that we can never prov e that it is secure! Suppose that
you design a system, calculate some security claims, and dis cover every time that the system
remains secure under all tests. The system is then secure? No , it is simply not proven insecure.
In the future you could reﬁne the security model, there could be a wider range of tests and
attacks, and you might then discover that the thing is insecu re.We never are deﬁnitely secure;
we can only be sure when we are insecure.
7
Figure 1.4: The process of science and the process of securit y
ObservationTheory Prediction
inducededuce
test
All theories are incomplete
and become wrong.AttackPolicy Defense
designimplement
deploy
All policies have a lifetime
and become insecure.
8
2 Security concepts
2.1 The dark side . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2.2 The timing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.3 The good and the bad . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.4 Know, Have, Be . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.5 What did we learn? . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
2.5.1 Process properties . . . . . . . . . . . . . . . . . . . . . . . . . 17
2.5.2 So what? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
We separate concepts to be able to reason. When concepts depe nd on each other, separating
them seems wrong. How can I make sense of an egg without a chick en? But I also cannot
make sense if I mix them up. Many security concepts depend on e ach other, don’t make sense
without each other, but get mixed up. Let us try to make sense.
2.1 The dark side
We all need security. We are anxious when we are insecure. We s eek a secure home, we try to
ﬁnd a secure job. We crave security so much that we even study i t. But what is it?
Figure 2.1: A race can be won by being the best and the fastest. . .
Yes, it means many things to many people: one thing to the soft ware engineer, another to the
soldier, and yet another to the diplomat and the little bee. W e kicked oﬀwith that in Sec. 1.1.3.
But what is the common denominator of all the di ﬀerent notions of security?
9
InStar Wars terminology, security is the battle against the dark side of the Force . In words of
Vilfredo Pareto1[47],
"The eﬀorts of humans are organized in two directions:
•to the production of goods, or else
•to the appropriation of goods produced by others."
Security is concerned with the second direction: those who p roduce goods want to secure them
from being appropriated by others. When that fails, then tho se who have appropriated the
goods want to secure them from being taken back; and so on. Security is the realm of such
ownership conﬂicts. But note that Pareto, when he speaks of " the eﬀorts of humans", restricts a
broader process, as the ownership conﬂicts also rage among a nimals, and many of the security
solutions that we will study emerged as evolutionary strate gies. Even the simplest forms of life
already compete to secure their resources from others in one way or another. Wherever t here
is competition, the is security. In terms of racing, Pareto’ s observation is illustrated in Fig. 2.1
and Fig. 2.2.
Figure 2.2: . . . or by tripping up the opponent.
The war against the Dark Side of the Force, of course, persist s in the world of computers and
networks. The cyberspace only makes it harder to tell who is w ho, and what is what. In
cyberspace, every force has a dark side. Is Facebook my frien d or my enemy? How about
YouTube? The memes all together seem to be taking me somewher e, one leading to the other;
but where are they taking me? Am I safe there? Am I secure? What is the diﬀerence? We need
Security Science (SecSci) because the eyes may deceive, and the minds are actively dece ived.
Spam is obviously spam, an attack is obviously an attack, but not always. When should I start
defending myself?
1Pareto was one of the fathers of political economy. His most f amous quote is: "The rich get richer" .
10
Figure 2.3: If we are all plugged into a Matrix, then it is hard to tell what is secure.
2.2 The timing
Security can be implemented
•before attacks: toprevent them by cryptography, protocols, ﬁrewalls;
•during attacks: todetect them by intrusion detection systems, or to detect earlier ph ases
by forensic methods;
•after attacks: todeter them by punishing (by legal measures) or attacking the attac kers
(by illegal measures), and by balancing the incentives: the likely cost of an attack must
be higher than the likely proﬁt.
2.3 The good and the bad
In life and regarding software, our needs and requirements a re always expressed in the same
way:
•we want that good stu ﬀhappens, and
•we want that bad stu ﬀdoesn’t happen.
For example, the good stu ﬀthat we want from a computational process is that it eventual ly
terminates and gives the correct output. The bad stu ﬀthat should not happen is that the process
should not crash or get hijacked.
So there are two kinds of bad stu ﬀthat we want to avoid: accidental, and intentional. A requir e-
ment that good stuﬀhappens is called a liveness requirement. A requirement that the accidental
11
Figure 2.4: Types of requirement speciﬁcations
Requirements
Good stuﬀ
should
happenBad stuﬀ
should not
happen
liveness securityfunctionality no accidents
(natural)no attacks
(adversarial)
safety
Afunctional dwelling,
 a shelter from hazards ,
a lock
against intruders .
bad stuﬀshould not happen is called a safety requirement. A requirement that the intentional
bad stuﬀperpetrated by an attacker should not happen is called a security requirement. Note
that
•theliveness requirements specify some desired functionality that should be achieved;
•thesafety requirements specify some undesirable and unintentional hazards that should
12
be prevented;
•thesecurity requirements specify some undesirable intentional attacks that should be
defeated.
This subdivision of the requirement speciﬁcations is displ ayed in Fig. 2.4. Further down the
Figure 2.5: A modest car provides modest functions, average safety, and aﬀordable security
road, cars are also built to satisfy the same three kinds of re quirements:
•theengine provides the driving functionality, /squiggleleftliveness
•thebrakes prevent accidents and the loss of control, /squiggleleftsafety
•thelocks and alarm prevent theft and intrusions. /squiggleleftsecurity
Summary. The conceptual distinctions made so far are that
•safety is a negative requirement that bad stu ﬀdoes not happen — in contrast with live-
ness, which is a positive requirement that good stu ﬀhappens;
•security protects from intentional bad stuﬀ— in contrast with safety, which protects from
theunintentional bad stuﬀ.
For more instances of these high-level distinctions, see Ta ble 2.1. The same distinctions arise
in all areas of engineering, since every system speciﬁcatio n includes a liveness requirement,
most of them include some safety requirements, and all those that involve network interactions
also include some security requirements.
2.4 Know, Have, Be
All security requirement speciﬁcations begin by distingui shing three types of entities:
•data : what you know,
13
positive requirement negative requirements
domain liveness safety security
mountain reach the peak do not slip on ice do not get pushed
kitchen prepare fooddo not bite your
tonguedo not get poisoned
airport board passengers mark slippery ﬂoor prevent terrorism
cryptography D(k,E(k,m))=m no bugsA(E(k,m))=m⇒A(y)=D(k,y)
Table 2.1: Positive and negative requirements in various ap plication domains
•things : what you have,
•traits : what you are.
This know-have-be tripod , displayed in Fig. 7.6, provides a ’metaphysical foundatio n’ for se-
curity analyses and designs. The three types are distinguis hed by two actions:
•what you know can be copied and given away;
•what you have cannot be copied, but it can be given away; and
•what you are cannot be either copied, or given away.
For example, in a data network, Alice can give copies of her di gital keys to Bob, and then both
Alice and Bob will know Alice’s keys. Digital keys are data because they can be copied and
given away. In a physical network, on the other hand, it is not as easy for Alice to copy her
physical key, or her tamper-resistant smart card; yet she ca n still give them to Bob, and then
Bob will have them, but Alice will not. The physical keys are t herefore things , which means
that they cannot be copied but can be given away. Finally, sin ce Alice cannot easily copy or
give away her traits , such as genes, iris patterns, ﬁngerprints, or handwriting , her identity is
often identiﬁed with such indivisible individual biometric features; they are who she is. This
security typing is summarized in Table 2.4 and Fig. 2.6.
type what can be copied can be given away
data what you know /check/check
things what you have ×/check
traits what you are × ×
Table 2.2: Distinguishing the security types
Comment. There are, of course, methods to clone a smart card so that Alice still has it after
14
she gives it to Bob; and there are methods to forge handwriting and ﬁngerprints, which may
make Alice and Bob indistinguishable even biometrically. B ut these methods provide attack
avenues on particular implementations of security based on what you are, or what you have.
Here we are not talking about the implementations, but about the basic ideas of security. The
particular attack avenues can always be eliminated by impro ved implementations. The basic
ideas remain the same.
Security
Resource
securityData
securitywhat you have what you knowPhysical
security
what you are
Figure 2.6: The security tripod
Using the tripod. Of the three legs of the security tripod displayed in Fig. 7.6 , we leave aside
the top one, the Physical Security, and focus on the remainin g two: Resource Security and Data
Security. So we ignore what you are , and explore the methods to secure and to use what you
have andwhat you know . In each case, the security requirements can again be subdiv ided into a
“good-stuﬀ-should-happen” part and a “bad-stuﬀ-should-not-happen” part. This is displayed
in Fig. 2.7. Security requirements specify the good stu ﬀthat should eventually happen (“yes
good”). Security constraints specify the bad stu ﬀthat should never happen (“no bad”).
The coming chapters roughly correspond to the properties at the bottom of Fig. 2.7.
Resource security speciﬁcations will be studied in Ch. 3, re source security requirements ex-
pressed in terms of availability andauthorization (a.k.a. authority ), in Ch.4. Channel security
is discussed in Ch. 6, and it brings us to network security in 7 . The crucial security properties
required from channel and network ﬂows are integrity andsecrecy . The crucial security re-
quirements concerning channel sources are authenticity andconﬁdentiality . We say that the
chapters roughly correspond to the “know-have-be” types because data can als o be resources,
not just things; and things can also ﬂow through the channels , not just data. But at least the
rough correspondence follows the typical examples, and it s eems useful early on.
Remark about CIA. Most security courses and textbooks begin with the CIA-triad of secu-
rity properties, which refers to Conﬁdentiality, Integrity, and Availability. The question: “Why
these properties, and not some other?” often arises in conversations. The usual answer is that
their importance has been established through years of expe rience and expertise. In special
15
Security
Resource
securityData
securitywhat you have what you know
yes
goodyes
goodno
badno
bad
secrecy
conﬁdentialityauthenticity
integrityauthorization availabilityPhysical
security
what you arefreedom health
yes
goodno
bad
Figure 2.7: Security requirements are “yes good”. Security constraints are “no bad”
16
situations, other properties, such as non-repudiation , are also considered, with similar justiﬁca-
tions. Fig. 2.7 suggests that the CIA-canon could perhaps be naturally reconstructed along the
good stuﬀ/bad stuﬀaxis, by saying that
•Conﬁdentiality means that bad data ﬂows do not happen;
•Integrity means good data ﬂows do happen;what you know
•Availability means that good resource calls are accepted. }what you have
However, the requirement that bad resource calls are rejected is missing. It is called autho-
rization in Fig. 2.7. Maybe the “A” in CIA should be double counted, alt hough we never
encountered such a suggestion. On the other hand, the proble m that non-repudiation is not a
part of CIA is often discussed, and there was a proposal that t he triad should be extended to
CIAN. It is easy to see that non-repudiation is in fact a “good-information-ﬂows-do-happen”
requirement. It was interpreted as authentication-after-the-fact by some researchers. We wrote
authenticity and integrity in the same box in Fig. 2.7, and th ey are often used interchangeably.
The whole CIA thing should probably be taken with a grain of sa lt.
2.5 What did we learn?
2.5.1 Process properties
2.5.1.1 Dependability
Insoftware engineering , the requirements are expressed in terms of dependability properties
that include
•safety: bad stuﬀ(actions) does not happen , and
•liveness: good stuﬀ(actions) does happen ,
and their logical combinations. For sequential computatio ns, every ﬁrst order property can be
expressed as a conjunction of safety and liveness propertie s. Safety and liveness are semanti-
cally independent properties: whether one property is sati sﬁed or not does not depend on the
other property.
2.5.1.2 Security
Insecurity engineering , the requirements are expressed as security properties .
Theresource security requirements are expressed in terms of the access control properties ,
which are the combinations of
17
•authority:2bad resource requests are not granted , and
•availability: good resource requests are granted .
In distributed computation (e.g., within a computer system , or controlled by an operating sys-
tem), all security requirements are conjunctions of author ization and availability requirements.
These are just the local versions of safety and liveness properties: to specify auth ority means
to specify what is considered safe for a particular user; to s pecify availability means to specify
what kind of liveness guarantees are provided for each parti cular user’s resource calls.
Thechannel security requirements are expressed as combinations of
•secrecy: bad data ﬂows do not happen ; and
•integrity: good data ﬂows do happen .
Since the data ﬂows that identify the originator often requi re special treatment, the same chan-
nel security requirements instantiated to identiﬁcations are often called by special names:
•conﬁdentiality: bad identiﬁcations do not happen ; and
•authenticity: good identiﬁcations do happen .
But the usage varies, and conﬁdentiality and secrecy are oft en bundled together, confused, or
switched, as are authenticity and integrity. We have to be ﬂe xible with words but precise with
concepts.
2.5.1.3 Static vs dynamic
While safety and liveness are independent of each other and f reely combined to express arbi-
trary dependability properties, and while the resource sec urity properties just extend the de-
pendability properties by subjects’ identities, and thus l eave authority and availability indepen-
dent, and even orthogonal in a certain formal sense, the chan nel security properties dynamically
depend on each other, and require a substantially di ﬀerent treatment.
Innetwork computation (where computers communicate by messages), all data ﬂow con-
straints are logical combinations of secrecy and authenticity. Note , however, that the secrecy
and the authenticity properties are not independent. In fac t, if only data are processed (while
the objects and the subjects are ﬁxed), then
•every secret must be authenticated, and
•every authentication is based on some secrets.
In more general systems, authentications can also be based o n secure tokens, and on biometric
properties, but establishing and maintaining secrecy stil l requires authentications. Table 2.3
2The process of verifying authority is called authorization .
18
compares and contrasts dependability and security propert ies.
processing dependability security
System centralized distributed
observations global local
Environment neutral adversarial
threats accidents attacks
Table 2.3: The diﬀerences between dependability and security
Terminology. The concept of secrecy is closely related to the concept of conﬁdentiality . They
are not entirely synonymous in the colloquial usage (e.g., a "conﬁdential meeting" is not the
same thing as a "secret meeting"), but we will not make a disti nction yet, since both require
that some undesired data and information ﬂows do not happen.
Similarly, the concept of authenticity is closely related t o the concept of integrity . In this case,
the diﬀerence in the usage is perhaps easier to pin down: authentici ty of a message means that
if it is signed by Bob, then it is really a message from Bob; whe reas the integrity of the same
message means that no part of it was altered on the way. While s uch distinctions are, of course,
of great interest in particular analyses, in order to keep th e general conceptual framework as
simple as possible, we shall use both authenticity and integ rity to refer to the same requirement
that some desired data and information ﬂows (e.g., that Bob i s online) do happen.
2.5.2 So what?
Don’t skip the questions in the next section. There are sever al correct answers in some cases,
and it is useful to discuss them. It may be equally easy to argu e for diﬀerent answers. When
you need to decide which security policy to impose, you need c lear and unique answers. You
will need methods to unify di ﬀerent answers. Remembering the dimensions of security, its
frame of reference discussed in this chapter, will help.
We learned that security concepts are
•simple and easy to understand and deﬁne, butthat they are
•complicated and hard to reason about, protect, and implemen t.
The complications arise from the ease with which we think and talk about security, and de-
velop the narratives we repeat until we and everyone around u s believe them. Until they fail,
and then we develop other narratives. Our common sense is pri med for compelling security
arguments, and the common-sense arguments are primed to eve ntually fail and be replaced by
other arguments. The common-sense security arguments are primed for security failures, which
are usually based on deceit, which is usually based on self-d eceit. That is why we need to go
beyond common sense reasoning and study security by the methods of science .
19
The language that evolved to address the shortcomings of our common-sense reasoning and
provide foundations for scientiﬁc testing is the language o f mathematics. In the coming chap-
ters, we take up the task of spelling out the mathematical mod els of security.
20
3 Static resource security: access
control and multi-level security
3.1 What . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
3.1.1 What is a resource? . . . . . . . . . . . . . . . . . . . . . . . . . 21
3.1.2 What does it mean to secure a resource? . . . . . . . . . . . . . . 22
3.2 Access Control (AC) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
3.2.1 Access control in computer security . . . . . . . . . . . . . . . . 25
3.2.2 Implicit clearance and classiﬁcation . . . . . . . . . . . . . . . . 26
3.3 Multi-Level Security (MLS) . . . . . . . . . . . . . . . . . . . . . . . . 26
3.3.1 Explicit clearance and classiﬁcation . . . . . . . . . . . . . . . . 27
3.3.2 Reading and writing . . . . . . . . . . . . . . . . . . . . . . . . 28
3.3.3 All resource security boils down to access control . . . . . . . . . 31
3.1 What is resource security?
Recall that the two basic types of resource security require ments are
•authority: bad resource requests are rejected , and
•availability: good resource requests are fulﬁlled .
Before studying such requirements, we describe the methods ofaccess control that are used
to distinguish the good resource requests from the bad. But w e ﬁrst need to deﬁne what is a
resource request.
3.1.1 What is a resource?
The typical examples of resources are the fossil fuels: coal , petroleum, and natural gas. They
gave us power to race in cars, 10 times faster than we can run, a nd to ﬂy in planes, higher
than any bird. They store the energy from the Sun, accumulate d by plants and bacteria for 550
million years, and then fossilized. We have been burning tha t energy for about 200 years, and
we will probably burn it up in another 50 years, or less.
Resources are one-way functions: they are easy to use, but ha rd to get by. Burning the hydro-
carbons from fossil fuels releases energy; making the hydro carbons requires energy. Capturing
that energy took 550 million years; releasing it took 250 yea rs. Going down a hill is easier
than going back up. That is the essence of a resource: it is a on e-way function. The idea is
21
coal
ashesburnstore
A resource is easy to use
but hard to come by.Resource
Residue/squigglerightutility investment/squiggleright
A resource provides utility
but requires investment.11,213·756,839
8,486,435,707/squigglerightsecurity attack/squiggleright
A resource is
a one-way function.
Figure 3.1: The essence of resources.
illustrated in Fig. 3.1 on the left and in the middle.
Modern cryptography is based on one-way functions as comput ational resources. That is illus-
trated in the ﬁgure on the right. A one-way function is easy to compute, but hard to "uncom-
pute" . To compute a function f:A→Bmeans to take an input aof type A, and compute the
output f(a) of type B. To "uncompute" the function f:A→Bmeans to take a value b∈B
and ﬁnd a value xof type Asuch that f(x)=b. For instance, if we take the inputs to be pairs
of natural numbers (nonnegative integers), i.e., A=N×N, and multiply them, i.e., take B=N
and set the function fto be the multiplication ( ·) :N×N→N, then the "uncomputing" can
be construed as factoring a number y∈Binto primes, and partitioning them into two numbers,
x0,x1∈N, such that x0·x1=y. Ifyis a product of two primes, then there is a unique way to
"uncompute" the computation of y=x0·x1. However, while multiplying x0andx1takes the
number of computational steps proportional to the lengths ofx0andx1, factoring y=x0·x1
requires that we somehow try to divide ywith the various primes below it. This is why "un-
computing" a product of large primes is thought to be much har der than computing it, and the
product is used as a one-way function. It is a computational r esource of modern cryptography.
(Sorry for repeating. It is sometimes useful.)
3.1.2 What does it mean to secure a resource?
A resource can be useful for many people. For example, a water well can be used by people and
animals from a wide area. If it is so large that none of them can prevent others from accessing
it, then they have to share it. It remains a public resource. H owever, if some of them can control
access to the water well, then they can assert private owners hip over it and claim it as an asset .
Resource security is access control . It makes resources into assets. In a sense, access control
is the stepping stone both into security, and into economy, w hich at this level boil down to
the same. An asset is a resource that can be secured, owned, so ld, and hence it acquires an
economic value. Without security, there is no economy. Howe ver, if the cost of securing a
resource is greater than its value, then claiming any owners hip makes no sense. You don’t
spend $200 on a lock to secure a $20 bike. Without economy, the re is no security. In a sense,
22
economy and security are two sides of the same coin.
3.2 Access control
Example 1. Access control had to be implemented long before there were c omputers and
operating systems, as soon as there were some people, and the y owned some goods. But things
get more general in the science of security, so we will call pe ople subjects , and their goods
objects . This will be formalized in Def. 3.1. To begin, consider our h ero subjects and their
valuable objects in Fig. 3.2: Alice had a sheep, and Bob had a j ar of oil. The situation (or
Alice Bobsheep oil
Figure 3.2: Subjects Alice and Bob, and their objects sheep a nd oil
state of the world) s, where only Alice has access to sheep’s wool, milk, and event ually meat,
whereas only Bob has access to cooking with his oil, can be rep resented using a matrix like Ms,
on the left in Fig. 3.3. If Alice gives Bob a bottle of her sheep ’s milk in exchange for a bottle
s sheep oil
Alice {milk, wool, meat} ∅
Bob∅ {cook}→q sheep oil
Alice {milk, wool, meat} {bottle of oil}
Bob {bottle of milk} {cook}
Figure 3.3: Alice and Bob trade their private resources
of Bob’s oil, then the state schanges to the state q, which is represented by the matrix Mq, in
Fig. 3.3 on the right. Such representations are formalized a s follows.
Deﬁnition 3.1. Anaccess control (AC) model consists of
•access control types :
–objects (oritems )J={i,j,...},
–subjects (orusers )S={s,u,...}, and
–actions (orlabels )A={a,b,...},
•access control matrices in the form M,B:S×J→℘A, where℘Ais the set of subsets
ofA, and where
–M=(Mui)S×Jis the permission matrix : each entry Muispeciﬁes which actions the
subject uis permitted to apply on the object i;
23
–B=(Bui)S×Jis the access matrix : each entry Buispeciﬁes which actions has the
subject urequested for the object i.
Theaccess control (AC) requirement is that for all u∈Sandi∈Jholds
Bui⊆Mui. (3.1)
Remarks. The terminological alternatives object /subject/action vs item/user/label are used in
diﬀerent communities that study the same phenomena from slight ly diﬀerent angles. There
are still other alternatives. While using several terminol ogical alternatives can be confusing,
diﬀerent alternatives sometimes support di ﬀerent intuitions, which can be valuable. We initially
stick with the object /subject/action variant.
How is access controlled? Although the matrices MandBhave the same types, their
purposes and implementations are substantially di ﬀerent. The permission matrix Mis speciﬁed
at design time by some access policy authority , e.g., the system designer or administrator. The
sets of actions Mui∈℘Aare usually implemented as bitstrings, assigning each acti ona∈Athe
value 1 if it is permitted and the value 0 otherwise; we write t he permitted actions as subsets
for convenience. The access matrix Bis maintained by the system itself, usually by a system
monitor . Whenever a subject u∈Sissues a request to access an object i∈Jby an action
a∈A, the monitor tests whether a∈Mui, and if the test succeeds, it adds atoBui. The monitor
thus maintains the AC requirement (3.1) as a system invarian t. The set Buithus consists of the
actions athat the subject uhas requested for the object i, and the permissions were granted. An
action that was requested but not permitted by Mui, or an action that is permitted by Mui, but
not requested, will not be recorded in Bui.1
Example 2. Access control applies not only to goods but also to space, wh ich is also a resource.
When Alice and Bob build houses, like in Fig. 3.4 on the left, t hen the separation of the private
Figure 3.4: Access control of public and private spaces
#31
#30 Uruk Lane
Architectures House #30 House #31 Uruk Lane
Alice {occupy}∅ {use}
Bob∅ {occupy} {use}
Carol∅∅ {use}
Permissions
1The requested actions must, of course, be recorded before th e permission is issued or denied. An implementer
might thus be tempted to record all requests in Bui, and to purge them to maintain (3.1). However, a separate
list of all resource calls is maintained as a basic component of the operating system, not just for the purposes
of access control. The access matrix Bis the intersection of that list of the actual calls and of the matrix Mof
permissible calls.
24
space inside their houses and the public space that they shar e is an instance of access control.
The permission matrix is displayed in Fig. 3.4 on the right. T he public space is accessible to
all public, represented by Carol, who does not own a house in t he neighborhood.
Matchstick names. To minimize clutter, we avoid writing out subjects’ names in the dia-
grams, but include their initials in their bodies: Alice’s l egs form an A, Carol’s arms form a C,
and Bob uses his arms and legs to form a B.
Exercise. Specify permission matrices that describe the states where Alice invites Bob and
Carol to her house.
Remark. Note that the current locality of a subject cannot be directl y expressed using permis-
sion matrices alone. For example, the situation in Fig. 3.4, where Alice is not in her private
home but in the public space of Uruk Lane, cannot be expressed . Location speciﬁcations will
be introduced in Sec. 3.3.
3.2.1 Access control in computer security
It is easy to see that each of the AC matrices is, in fact, a tern ary relation, as there is a function
S×JR− →℘A
/hatwideR∈℘(S×J×A)
This means that the size of AC matrices grows linearly with th e product of the number of sub-
jects, objects, and actions. This presents an implementati on problem, since the access matrix B
needs to be updated, and compared with the permission matrix M, at each access, potentially
in each clock cycle. So a direct implementation of abstract a ccess control is impractical.
The space of practical implementation strategies is spanne d between the two extremal ap-
proaches:
•access control lists (ACLs): J→℘(A×S)
•capability-based AC: S→℘(A×J)
In both cases, the AC matrices are thus decentralized: in the former case, the rows of the
permission matrix Mare distributed among the objects of the system as the ACLs, o ne for each
i∈J; in the latter case, the columns of Mare distributed among the subjects as capabilities,
one for each u∈S. The access matrix Bis not stored at all: each access request of uforiis
checked as it comes, be it against u’s capability, or against i’s ACL.
The ACL-based approach was ﬁrst implemented in UNIX. This is the common-sense approach,
since the number of objects in Jis usually much greater than the number of subjects in Sor
actions inA. The fact that the datatype ℘(A×S) still appears exponential is resolved by
reducingAto some standard access tools. In UNIX, they are just read,write and execute, and
25
the Windows operating systems advanced the state of the art b y extending this set to up to 7
elements. The ACL entries for subjects are also reduced to user,others, andgroup. The user
isu∈Sextracted from the login, the entry others accommodates all of S, and the rest of the
℘Sare the possible groups. The impractical part of the ACLs is t hus deferred to the group
management, which is the task of the system manager. Problem solved.
The capability-based approaches go back to Symbian, which w as Ericson’s early operating
system for mobile phones. The development was driven by the f act that the early mobile devices
were not powerful enough to manage ACLs. For di ﬀerent reasons, a version of capability-based
AC was adopted in SELinux, and that AC model was adopted in And roid and then modiﬁed
many times. It is not easy to tell whether there is still a capa bility-based model.
3.2.2 Implicit clearance and classiﬁcation
Intuitively, Alice’s authority is at least as great as Bob’s authority if she can do anything he
can do. Such comparisons can be derived from any AC model as a clearance preordering of
subjects , deﬁned by
u≤v⇐⇒ ∀ i∈J.Mui⊆Mvi
for all u,v∈S. It is easy to see that ≤is a reﬂexive and transitive relation on S. It is not
antisymmetric, because di ﬀerent subjects can have the same authority, i.e., there may b eu/nequalv
with Mui=Mvi, and thus u≤vandu≥v.
An analogous classiﬁcation preordering of objects can be derived similarly, except tha t it
is more meaningful to use access requests than permissions. For example, oil is at least as
classiﬁed as the sheep if anyone who requests the sheep must a lso request oil, i.e.,
i≤j⇐⇒ ∀ u∈S.Bui⊆Bu j
In this way, an AC model implicitly assigns some clearance le vels to the subjects and some
classiﬁcation levels to the objects. It is often more conven ient to specify access policies by
making such security levels explicit.
3.3 Multi-level security
While the AC models have been formalized to facilitate acces s control in operating systems,
the multi-level security (MLS)models are familiar from the various military, diplomatic, indus-
trial, and banking security systems. There, the access to re sources is controlled by assigning
diﬀerent security classiﬁcation levels to the objects and di ﬀerent security clearance levels to
the subjects. While these two security frameworks have quit e diﬀerent origins, and di ﬀerent
formalisms, they turn out to be logically equivalent. In Sec . 3.2.2, we saw that every AC model
26
contains an implicit MLS model. In this section, we shall sho w how to reduce any given MLS
model to an AC model.
3.3.1 Explicit clearance and classiﬁcation
In the meantime, as the Neolithic broke out, Bob built some se cure vaults to store his assets,
with a high-security chamber where he stored his crate of oil . This is displayed in Fig. 3.5.
Alice and her sheep remained outside at the lowest security l evel, denotedℓ1.
Figure 3.5: Aspects of multi-level security (MLS)
ℓ1ℓ2
ℓ3 ℓ4ℓ5
Architectureℓ1ℓ2ℓ3ℓ4ℓ5
Security levelsq0 pℓq0cℓq0
Aliceℓ1ℓ4
Bobℓ2ℓ5
sheepℓ1
oilℓ5
Security assignments
The location of each subjectand each objectis expressed by t he function pℓ:S∪J→L, which
we tabulated in the same diagram. The current location of eac h objectexpresses its security
classiﬁcation . Authorized subjectscan change their classiﬁcation level by moving it to another
security level, provided that they are cleared to access tha t level themselves. The clearance
level of each subjectis speciﬁed by the function cℓ:S→L, also tabulated in the diagram. A
subject uis cleared to access the security levels up to its clearance l evelcℓu. This is formalized
in the following deﬁnition.
Deﬁnition 3.2. Amulti-level security (MLS) model consists of:
•security typesSandJlike in Def. 3.1, and moreover, the type of
–security levelsL={ℓ1,ℓ2,...}, partially ordered by ≤,
•twosecurity assignment functions :
–clearance cℓ:S→L, and
–location pℓ:S∪J→L.
Themulti-level security (MLS) requirement is that for all u∈Sholds
pℓu≤cℓu. (3.2)
27
For the Neolithic example above, the MLS requirement means t hat Alice cannot enter Bob’s
vault because she is not cleared for anything above the level ℓ1.
While AC models and MLS models appear quite di ﬀerent and are used for di ﬀerent purposes,
in Sec. 3.2.2, we saw that every AC model contains an implicit MLS model. Thm. 3.3 below
tells that any AC model can, in fact, be expressed as an MLS mod el. The price to be paid is that
capturing the distinct access control permissions for dist inct objects in terms of security levels
may require lots of security levels. Further down, Thm. 3.5 w ill show that any MLS model,
even extended by additional authority requirements, can be reduced to an AC model.
Theorem 3.3. Given an AC model, let the type of security levels Lbe the set of functions
ℓ0,ℓ1,..., :J→℘A, ordered by pointwise inclusion, i.e.
L=/parenleftbig℘A/parenrightbigJwith the partial order ℓ0≤ℓ1⇐⇒ ∀ i∈J.ℓ0(i)⊆ℓ1(i).
Deﬁne the corresponding MLS model by:
cℓ:S→/parenleftbig℘A/parenrightbigJpℓ:S→/parenleftbig℘A/parenrightbigJ
u∝mapsto−→cℓu(i)=Mui u∝mapsto−→pℓu(i)=Bui.
The MLS requirement for this MLS model is satisﬁed if and only if the AC requirement was
satisﬁed for the original AC model. This means that for every subject u∈Sholds
pℓu≤cℓu⇐⇒ ∀ i∈J.Bui⊆Mui.
The proof is easy, and it is left as an exercise.
3.3.2 Reading and writing
The object classiﬁcations and the subject clearances are es tablished and managed by authorized
subjects. This is, in a sense, where security as a process begins. Classifying an object and
requiring a clearance from a subject are the most basic secur ity operations. Declassifying an
object or increasing a subject’s clearance level are the sim plest ways to relax security. Many
diﬀerent views of the problems of clearance and declassiﬁcatio n, arising in many di ﬀerent
contexts where these operations are needed, led to diverse b ut closely related security models
and architectures, from which the early security research e merged [8, 15, 10, 30, 12]. The
process of classifying and declassifying objects is modele d in terms of two basic actions which
we write wտandrց. Since most security models have been concerned with data, t hese are
usually called write andread; but the formalism applies to other action /reaction couples, such as
give/take, and send/receive. It is often convenient and sometimes fun to call the m all write/read.
The formal models in this area of security research largely s ubsume under the same structure
that we will present, although the application domains vary vastly, and the interpretations are
often genuinely diﬀerent. We stick with a Neolithic one.
28
Suppose that Alice is leaving for a vacation and wants to "dep osit" her sheep in Bob’s secure
vault (a Neolithic bank of sorts). Since Alice is not cleared to enter the secure vault, she gives
the sheep to Bob, and he takes it to a higher security level. When Alice returns from the
vacation, Bob has to come out of the vault to return the sheep t o the lower security level. This
security process evolves through the states q0→q1→q2→q3→q0, displayed in Fig. 3.6.
q0
ℓ1ℓ2
ℓ3 ℓ4AliceBobℓ5
q3
ℓ1ℓ2
ℓ3 ℓ4Alice Bobℓ5q1
ℓ1ℓ2
ℓ3 ℓ4Bobℓ5
Alice
q2
ℓ1ℓ2
ℓ3 ℓ4ℓ5
Alice BobwտA:ℓ1sh−→ℓ2:rցB
pℓB:=ℓ1 pℓB:=ℓ2
wտB:ℓ1sh−→ℓ1:rցA
Figure 3.6: The process of storing and withdrawing Alice’s s heep from Bob’s secure vault
The pairs of complementary actions analogous to the give/take, applied to sheep, are usually
called write/read when applied to data. Alice, in a sense, “writes” her sheep up the security
ladder, but she is not permitted to “read” the sheep back down from the higher security level
of the vault to her lower clearance level. Bob does have a clea rance to be in his vault, but
29
“writing” down is also not generally permitted since that wo uld allow a malicious insider with
a high clearance to declassify everything. The general prin ciple is
•no-read-up :
–only receive data at or below your clearance level
and
•no-write-down :
–only send data at or above your classiﬁcation level.
To remember this, we write rցandwտfor the actions of reading and writing data (or taking and
giving objects), respectively.
To return the sheep to Alice, Bob has to descend from his vault ℓ2to the levelℓ1, where he can
“write” the sheep to Alice. The state transition wտA:ℓ1sh−→ℓ2:rցBis an interaction between
Alice whose action wտA:ℓ1sh−→ℓ2elicits Bob’s reaction ℓ1?− →ℓ2:rցB. The question mark means
that Bob is ready to take on this channel, whatever Alice give s. Alice writes the sheep from
ℓ1toℓ2, and Bob accepts to read on the same levels. Similar coupling s arise, e.g., when Alice
sends a message, and Bob receives it. Alice’s action and Bob’s reaction are coupled into an
interaction. The action and the interaction are bound toget her with an object: in the above case
the sheep, and in the case of the send /receive cases the message. For the moment, though, we
are only interested in the fact that security is this process , evolving from state to state and from
transition to transition:
•q0:The crate of oil is highly classiﬁed. The sheep is unclassiﬁed .
–wտA:ℓ1sh−→ℓ2:rցB:Alice writes up, Bob reads down: they classify the sheep .
•q1:Alice cannot read up, Bob cannot write down: the sheep is classiﬁed .
–cℓB:=ℓ1:To be able to return the sheep, Bob descends to Alice’s securi ty level.
•q2:Bob is cleared to read the sheep in his vault; Alice is not.
–wտB:ℓ1sh−→ℓ1:rցA:Bob writes and Alice reads at the same level: they declassify
the sheep .
•q3:All (except oil) are insecure.
–cℓB:=ℓ2:Bob is back to security. The sheep is unclassiﬁed .
Deﬁnition 3.4. An abstract (resource) security model consists of:
•security typesS,J,A, andL, as in Deﬁnitions 3.1 and 3.2, together with
30
–distinguished actions rց,wտ∈A, called respectively read andwrite ,
•the AC and the MLS structures:
–permission and access matrices M,B:S×J→℘A,
–clearance and location assignments cℓ,pℓ:S→L,
–location assignment pℓ:J→L.
Theno-read-up andno-write-down requirements are respectively
rց∈ Bui=⇒cℓu≥pℓi, (3.3)
wտ∈ Bui=⇒ pℓu≤pℓi. (3.4)
The states where resource security models satisfy the no-re ad-up and the no-write-down re-
quirements are often called secure states .
Duality of reading and writing. Requirements (3.3) and (3.4) can be written together:
pℓuwտ
≤pℓirց
≤cℓu.
Bob is thus permitted to both read and write the sheep if the sh eep’s classiﬁcation is in the
interval between Bob’s location and his clearance, i.e., if pℓsh∈[pℓB,cℓB]. In the early days of
security research, the duality of reading and writing was vi ewed as echoing the duality of the
conﬁdentiality and integrity pair [10]. It is probably more justiﬁed to view it in terms of the
duality of authority and availability.
Remarks. The term "secure state" does not imply that the state satisﬁes any particular securi ty
requirement. A security process may satisfy the no-read-up and no-write-down requirements
and still contain a state where all subjects have the top clea rance, and all objects are declassiﬁed
to the lowest security level so that everyone can access all r esources. Some subjects would con-
sider such a state undesirable and completely insecure, whi le others would consider it desirable
and completely secure. Secure states are secure only with re spect to the two particular, very ba-
sic security requirements: no-read-up and no-write-down. More reﬁned security requirements
capture more reﬁned concepts of security.
3.3.3 All resource security boils down to access control
Since every authorization model subsumes an AC model, exten ds it by an MLS model, and
moreover imposes the no-read-up and no-write-down require ments on the combination, it seems
that the authorization model formalism is more expressive t han the AC model formalism. The
next theorem says that this is not the case: every resource se curity policy speciﬁed by an au-
thorization model can be equivalently expressed as an acces s control policy.
31
Theorem 3.5. For every authorization model
•M,B:S×J→℘A
•cℓ:S→Land pℓ:S∪J→L
there is an access control model
•/hatwideM,/hatwideB:S×J→℘/hatwideA
such that
Bui⊆Mui∧
pℓu≤cℓu∧
rց∈ Bui⇒cℓu≥pℓi∧
wտ∈ Bui⇒pℓu≤pℓi⇐⇒/parenleftig/hatwideBui⊆/hatwideMui/parenrightig
(3.5)
Proof . Deﬁne the new set of actions as the disjoint union /hatwideA=A+L, so that℘/hatwideA/simequal℘A×℘L.
Deﬁne furthermore the new permission and access matrices /hatwideM,/hatwideB:S×J→℘/hatwideAto have the
following entries:
/hatwideMui=/coproductdisplay
a∈Muiµui(a) /hatwideBui=/coproductdisplay
a∈Buiβui(a) (3.6)
where/coproducttexts denote the disjoint unions of the families
µui(a)=∇cℓu∩∇pℓiifa=wտ
∇cℓu otherwiseβui(a)=∇pℓu∪∇pℓiifa=rց
∇pℓu otherwise(3.7)
and∇ℓ={x∈L|x≤ℓ}is the set of security levels at or below ℓ. Using the fact that ℓ≤ℓ′
holds if and only if ∇ℓ⊆∇ℓ′holds, the direction =⇒of (3.5) is straightforward.
Towards the direction ⇐=, ﬁrst note that /hatwideBui⊆/hatwideMuialways implies Bui⊆Mui. To see why, ﬁrst
note that, by deﬁnition in (3.6), an element of /hatwideBuiis a pair∝a\}b∇ack⌉tl⌉{ta,ℓ∝a\}b∇ack⌉t∇i}htwhere a∈Buiandℓ∈βui(a).
The inclusion /hatwideBui⊆/hatwideMuiimplies that∝a\}b∇ack⌉tl⌉{ta,ℓ∝a\}b∇ack⌉t∇i}ht∈Mui, i.e., a∈Muiandℓ∈µui(a). So if we take an
arbitrary a∈Bui, thenℓ=pℓu∈βui(a) gives∝a\}b∇ack⌉tl⌉{ta,ℓ∝a\}b∇ack⌉t∇i}ht∈/hatwideBui⊆/hatwideMui, which implies a∈Mui. Hence,
Bui⊆Mui, since awas arbitrary.
Furthermore, whenever Muidoes not contain either rցorwտ, then Bui⊆Muidoes not contain
them either, and therefore
/hatwideMui=/coproductdisplay
a∈Mui∇cℓu/hatwideBui=/coproductdisplay
a∈Bui∇pℓu
Using the fact that pℓu≤cℓuholds if and only if ∇pℓu⊆∇cℓu, we get
Bui⊆Mui∧pℓu≤cℓu⇐⇒ /hatwideBui⊆/hatwideMui
32
This means that for the case without rցorwտ, the claim is proven, because the two bottom
conjuncts on the left-hand side of (3.5) are true trivially s ince the premises rց∈ Buiandwտ∈ Bui
are false.
When rց∈ Bui, then/hatwideBui⊆/hatwideMuiimpliesβui(rց)⊆µui(rց), i.e.,∇pℓu∪∇pℓi⊆∇cℓu. Hence,
∇pℓi⊆∇cℓuand thus pℓi≤cℓu, so the no-read-up requirement, third conjunct on the left- hand
side of (3.5) is proven.
Finally, when wտ∈ Bui, then/hatwideBui⊆/hatwideMuiimpliesβui(wտ)⊆µui(wտ), which means that ∇pℓu⊆
∇cℓu∩∇pℓi. Hence,∇pℓu⊆∇pℓiand thus pℓu≤pℓi. This means that the no-write-down
requirement, the fourth conjunct on the left-hand side of (3 .5), is also proven. /boxempty
Remark. Theorem 3.3 showed how any AC policy can be expressed as an MLS policy. The
price to be paid was a large and usually unintuitive poset of s ecurity levels. Theorem 3.5 showed
how any authorization policy (and thus any MLS policy as a spe cial case) can be expressed as
an AC policy. The price to be paid in this direction is that the security levels are captured as
actions. This is not so unintuitive since each security leve l can be construed as the action of
accessing that level. In practice, though, separating acti ons and security levels is usually more
convenient than bundling them together.
The upshot of Theorems 3.3 and 3.5 is that the apparently di ﬀerent languages of AC, MLS and
authorization are logically equivalent. However, it is use ful to have diﬀerent languages that
express the same things.
33

4 Dynamic resource security:
authorization and availability
4.1 Histories, properties, safety, liveness . . . . . . . . . . . . . . . . . . . 35
4.1.1 What happens: Sequences of events and properties . . . . . . . . 35
4.1.2 Dependability properties: safety and liveness . . . . . . . . . . . 39
4.2 Authority and availability . . . . . . . . . . . . . . . . . . . . . . . . . 42
4.2.1 Strictly local events and properties . . . . . . . . . . . . . . . . . 42
4.2.2 Resource security as localized safety and liveness . . . . . . . . . 44
4.2.3 General relativity of histories . . . . . . . . . . . . . . . . . . . 50
4.3 Denial-of-Service . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
4.1 Histories, properties, safety, liveness
4.1.1 What happens: Sequences of events and properties
In Fig. 3.3, diﬀerent states of the world are presented as di ﬀerent permission matrices. In Fig. 3.6,
transitions between the di ﬀerent states arise from subjects’ actions. Now, we need to st udy what
happens as time goes by.
Notation. Our notational conventions for lists and strings can be foun d in Prerequisites 1.
Deﬁnition 4.1. For a typeΣofevents , the ﬁnite sequences
x=/parenleftbigx0x1...xn/parenrightbig∈Σ∗
are called theΣ-traces , orhistories , orcontexts . AΣ-property , is expressed as a set of histories
P⊆Σ∗.
Remark. The words “trace” ,“history” , and “context” are used in diﬀerent communities to
denote the same thing: a sequence of events. “Traces” are use d in automata theory, “histories”
in process theory, and “contexts” in computational linguis tics. Sequences of events are also
studied in other research areas and perhaps called di ﬀerent names. The notion of events in time
is ubiquitous.
Trivial properties. The simplest properties are the set of all traces Σ∗, and the empty set
∅. If properties are thought of as requirement constraints, t henΣ∗is the least constraining
35
requirement since every history satisﬁes it, whereas ∅is the impossible requirement, satisﬁed.
A singleton{t}⊆Σ∗is a maximally constraining non-trivial requirement. The l arger properties
are less constraining.
Standard and reﬁned Σtypes. In most models, an event is an action of a subject on an
object. Formally, an event x∈Σis thus a triple x=∝a\}b∇ack⌉tl⌉{ta,i,u∝a\}b∇ack⌉t∇i}ht, and the event type is the product
Σ =A×J×S (4.1)
However, not all actions are applicable to all objects by all subjects. Alice can drive a car and
eat an apple, but she cannot drive an apple or eat a car. And Bob might be prohibited from
driving Alice’s car or eating her apple. Formally, a subject u∈Sis often given a speciﬁed
subtype of actions Aui⊆Athat they can apply to an object i∈J. SometimesAuiis empty,
which means that the subject uis not permitted to use the object i. That can also be captured
by specifying a subtype Ju⊆Jof objects accessible by the subject u∈S., The set of events that
may actually occur can then be expressed more explicitly as t he disjoint union1
/tildewideΣ =/coproductdisplay
u∈S/coproductdisplay
i∈JuAui (4.2)
But since /tildewideΣ⊆Σ, the crude form (4.1) generally su ﬃces, and the more precise form (4.2) is
used when additional clarity is needed, e.g., in Sec. 4.2.2.
Access matrices as event spaces. If an event is an action of a subject on an object, then a
permission matrix corresponds to a set of permitted events a long the one-to-one correspondence
S×JM−→℘A
/hatwideM⊆Σ
deﬁned by
∝a\}b∇ack⌉tl⌉{ta,i,u∝a\}b∇ack⌉t∇i}ht∈/hatwideM⇐⇒ a∈Mui
Since any access matrix S×JB− →℘Asimilarly corresponds to a set of events /hatwideBq⊆Σ, the access
control security requirement in (3.1) now becomes /hatwideBq⊆/hatwideM. The access matrices from Ch. 3
correspond to subsets of events. At each moment in time, the m atrix view, and the event space
view are equivalent. The latter is, however, more convenien t when we need to take a "historic
perspective", and study resource security of processes in t ime.
To get going, let us take a look at a couple of examples of histo ries and properties. In general,
events are actions of subjects on objects. But when subjects don’t matter, or when there is just
one, then the events are actions on objects. When objects don ’t matter, or there is just one,
then the events are just actions. We begin from this simplest case, and then broaden to richer
1In dependent type theory, this type would be written as/summationtextu:S/summationtexti:J(u).A(u,i). The notational clash of the
symbolΣused for labels, alphabets, and events in semantics of compu tation, and for sum types in type theory,
is accidental.
36
frameworks.
Example 1: sheep. We begin with Alice as the only subject, and her sheep as the on ly object,
so that an event is just one of Alice’s actions on the sheep, i. e.,
Σ =A={milk,wool,meat}.
The trace
m=/parenleftbigmilk milk milk milk wool milk/parenrightbig
means that Alice milked her sheep 4 times, then sheared the wo ol from the sheep, and then
milked her again. Consider the following trace properties:
MilkWool={milk,wool}∗(4.3)
MilkWoolMeat={milk,wool}∗:: meat (4.4)
MilkWoolWool={milk,wool}∗:: wool (4.5)
MilkWoolMeatMeat ={milk,wool,meat}∗:: meat (4.6)
MeatWoolMilkMilk ={meat,wool,milk}∗:: milk (4.7)
MilkWoolAnnual =milk∗∪/bracketleftbigmilk milk...milk/bracehtipupleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehtipdownright/bracehtipdownleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehtipupright
365 timeswool/bracketrightbig:: MilkWoolAnnual (4.8)
They constrain how Alice uses her sheep resources as follows :
•(4.3) says that Alice can milk and shear her sheep at will, but cannot use its meat;
•(4.4) says that Alice can milk and shear her sheep at will, but only until she eats its meat;
•(4.5) says that Alice can milk and shear her sheep at will, and in the end has to shear it;
•(4.6) says that Alice can milk, shear, and eat her sheep at wil l, and in the end has to eat
it;
•(4.7) says that Alice can milk, shear, and eat her sheep at wil l, and in the end has to milk
it;
•(4.8) says that Alice can use milk and wool, provided that she shears the wool at most
once per year
Example 2: elevator. Time passed, and Alice and Bob moved from their houses on Uruk
Lane into apartments in a tall building with an elevator, lik e in Fig. 4.1. They are now sharing
not only the public spaces in their neighborhood but also the elevator in their building. Making
sure that the elevator is safe and secure requires understan ding how it works. For simplicity,
we ﬁrst assume that it works the same for everyone and omit sub jects from the model. The
relevant types are thus:
•objectsJ=/braceleftbig0,1,2,..., n/bracerightbig, where
37
Figure 4.1: The elevator has a call button at each ﬂoor, and th e call buttons for all ﬂoors in the
cabin.
–idenotes the i-th ﬂoor of the building;
•actionsA=/braceleftbig∝a\}b∇ack⌉tl⌉{t∝a\}b∇ack⌉t∇i}ht,( )/bracerightbig, where
–∝a\}b∇ack⌉tl⌉{t∝a\}b∇ack⌉t∇i}htmeans “call” and
–( )means “go”;
•eventsΣ =J×A=/braceleftbig∝a\}b∇ack⌉tl⌉{ti∝a\}b∇ack⌉t∇i}ht,(i)/vextendsingle/vextendsingle/vextendsinglei=0,1,2,..., n/bracerightbig, where2
–∝a\}b∇ack⌉tl⌉{ti∝a\}b∇ack⌉t∇i}htmeans “call elevator to ﬂoor i” and
–(i)means “go in elevator to ﬂoor i”.
An elevator history is thus a sequence of calls ∝a\}b∇ack⌉tl⌉{ti∝a\}b∇ack⌉t∇i}htand services (i). Such sequences of events
are the elements of the set
Σ∗=/braceleftig/bracketleftbig∝a\}b∇ack⌉tl⌉{tx0∝a\}b∇ack⌉t∇i}ht∝a\}b∇ack⌉tl⌉{tx1∝a\}b∇ack⌉t∇i}ht(x0)∝a\}b∇ack⌉tl⌉{tx2∝a\}b∇ack⌉t∇i}ht∝a\}b∇ack⌉tl⌉{tx3∝a\}b∇ack⌉t∇i}ht...(xk)/bracketrightbig/vextendsingle/vextendsingle/vextendsinglex0,x1,..., xk∈J/bracerightig
.
The variables xk:Jdenote the ﬂoor numbers where the elevator is called or sent. The elevator
receives the calls as the inputs and provides its services as the outputs. A process diagram is
displayed in Fig. 4.2. The stream of calls is entered on the le ft, and the process trace streams
out on the right, where the elevator arrivals to where it was c alled are inserted between the calls.
In-between is the service scheduler, which maintains the qu eue of open elevator calls. The calls
are pushed on a queue and popped from it after they are respond ed to. This is implemented
as a feedback loop from the elevator controller, which infor ms the scheduler about the elevator
arrivals. When the scheduler matches an arrival with a previ ous call, it pops the call. The
2An action aperformed on an object iis usually written as ai. Here we write∝a\}b∇ack⌉tl⌉{ti∝a\}b∇ack⌉t∇i}htinstead of∝a\}b∇ack⌉tl⌉{t∝a\}b∇ack⌉t∇i}htiand(i)instead of
()i.
38
x:=[x(0)<···<x(k)]delete x0,x2...
x ∝a\}b∇ack⌉tl⌉{tx0∝a\}b∇ack⌉t∇i}ht∝a\}b∇ack⌉tl⌉{tx1∝a\}b∇ack⌉t∇i}ht,... ∝a\}b∇ack⌉tl⌉{tx0∝a\}b∇ack⌉t∇i}ht∝a\}b∇ack⌉tl⌉{tx1∝a\}b∇ack⌉t∇i}ht(x0)∝a\}b∇ack⌉tl⌉{tx2∝a\}b∇ack⌉t∇i}ht(x2)...
01n
calls trace
call queue
Figure 4.2: Elevator requests ﬂow in on the left and come shu ﬄed with services on the right
process can be viewed as a channel with feedback. The schedul er usually sorts the calls in
order of the ﬂoors and changes the priorities depending on th e state: it pops the next ﬂoor up
on the way up and the next ﬂoor down on the way down. This is obvi ously a crude, high-level
picture that needs to be reﬁned for actual implementations.
But even this crude picture illustrates the main point: that the required properties of processes,
such as an elevator, or Bob’s sheep bank from Sec. 3.3, can be e xpressed as trace properties,
which we discuss next.
4.1.2 Dependability properties: safety and liveness
The idea of safety and liveness is that a process is
•safe if bad stuﬀnever happens , and
•alive if good stuﬀeventually happens .
As they emerged in software engineering [36], the propertie s expressible in terms of safety and
liveness came to be called dependability properties. When a geometric view of such properties
got worked out [2], it turned out that allproperties expressible as sets, as in Def. 4.1, can be
expressed as an intersection of a safety and a liveness prope rty. This will be spelled out in
Sec. 5.3.
Unsafety implements the idea that there is no good stu ﬀ(actions) happening in any future:
Uis an unsafety property if every unsafe history remains unsa fe in all futures.
Safety implements the idea that there is no bad stu ﬀ(actions) happening in the past:
Sis a safety property if every safe history has always been saf e in the past.
A convenient way to formalize safety in terms of traces is to d eclare that unsafety is persistent :
39
once bad stuﬀ(actions) happens, it cannot become good in any future. An unsafe history
remains unsafe forever. More precisely, if a history xis unsafe, then no future zcan become
safe again, i.e.
∀xz∈Σ∗.x/nelementS∧x⊑z=⇒ z/nelementS. (4.9)
The other way around (and equivalently), if a history is safe , then its past must also be safe, i.e.
∀xz∈Σ∗.x⊑z∈S=⇒x∈S. (4.10)
Liveness implements the idea that there is always a good stu ﬀ(actions) in the future:
Lis a liveness property if every history can reach it in some fu ture.
Intuitively, a set of histories is a liveness property if eve ry given history may become alive3in
the future. Formally, L⊆Σ∗is a liveness property if
∀x∈Σ∗∃z∈L.x⊑z. (4.11)
In terms of the preﬁx ordering ⊑from Prerequisites 1(3), this means that safety properties are
the⊑-lower closed sets, whereas liveness properties are reache d from below.
Deﬁnition 4.2. The families of safety properties and liveness properties o ver the event setΣ
are respectively
SafeΣ=/braceleftbigS∈℘(Σ∗)|∀x∈Σ∗∀y∈Σ∗.x::y∈S=⇒x∈S/bracerightbig, (4.12)
LiveΣ=/braceleftbigL∈℘(Σ∗)|∀x∈Σ∗∃y∈Σ∗.x::y∈L/bracerightbig. (4.13)
When confusion seems unlikely, we omit the subscript Σfrom SafeΣandLiveΣ.
Remark. Note that the deﬁnition of safety in (4.12) is equivalent to ( 4.10), whereas the deﬁni-
tion of liveness in (4.13) is equivalent to (4.11).
Different situations require different safety and livenes s requirements. There are ex-
amples where both S⊆Σ∗and¬S=Σ∗\Sare safety properties. There are examples where
both L⊆Σ∗and¬L=Σ∗\Lare liveness properties. This does not mean that a system can be
both safe and unsafe at the same time, or both alive and dead. I t means that desirable concepts
of safety and liveness may vary from situation to situation. The intent of Def. 4.2 is not to
provide objective or universal properties that every safe p rocess should satisfy. Def. 4.2 de-
scribes the properties of properties that can be meaningful ly declared to be the desired safety
or liveness notions for a particular family of processes.
While there are many properties that are both safe and unsafe and properties that are both alive
3This is not everyone’s idea of the meaning of the word "alive" , but it does make sense for computations. You
look at a process, do not see that it works, and ask yourself "I s this process alive?" The answer "yes" usually
means that you will see that it works sometime in the future.
40
and dead, the only property that is both safe and alive is the t rivial one, i.e., Safe∩Live={Σ∗},
whereby every history is safe. More examples follow.
Example 1: Safety and liveness of sheep. The properties of Alice’s interactions with her
sheep usingΣ={milk,wool, meat}are as follows:
•(4.3) MilkWool∈Safe\Live;
•(4.4) MilkWoolMeat /nelementSafe∪Live;
•(4.5) MilkWoolWool /nelementSafe∪Live;
•(4.6) MilkWoolMeatMeat ∈Live\Safe ;
•(4.7) MeatWoolMilkMilk ∈Live\Safe ;
•(4.8) MilkWoolAnnual ∈Safe\Live.
Example 2: Safety and liveness of an elevator. An example of reasonable dependability
properties required from an elevator are:
•safety: the elevator should only come to a ﬂoor to which it was called, and
•liveness: the elevator should eventually go to every ﬂoor where it was called.
Using the notation from Prerequisites 1, these properties c an be formally stated as
(i)⊆∃∝a\}b∇ack⌉tl⌉{ti∝a\}b∇ack⌉t∇i}ht≺(i),/squiggleleftsafety
∝a\}b∇ack⌉tl⌉{ti∝a\}b∇ack⌉t∇i}ht ⊆∝a\}b∇ack⌉tl⌉{ti∝a\}b∇ack⌉t∇i}ht≺∃ (i)./squiggleleftliveness
Safety says that every arrival (i)must be preceded by a call ∝a\}b∇ack⌉tl⌉{ti∝a\}b∇ack⌉t∇i}ht. Liveness says that every call
∝a\}b∇ack⌉tl⌉{ti∝a\}b∇ack⌉t∇i}htmust eventually be followed by an arrival (i). The properties that we speciﬁed are thus
SafElev=/braceleftbigg
t∈Σ∗/vextendsingle/vextendsingle/vextendsingle/vextendsingle∀i∈J.t∈(i)=⇒t∈∃∝a\}b∇ack⌉tl⌉{ti∝a\}b∇ack⌉t∇i}ht≺(i)/bracerightbigg
=n/intersectiondisplay
i=0¬(i)∪∃∝a\}b∇ack⌉tl⌉{ti∝a\}b∇ack⌉t∇i}ht≺(i)
(4.14)
LivE=/braceleftig
t∈Σ∗|∀i∈J.t∈∝a\}b∇ack⌉tl⌉{ti∝a\}b∇ack⌉t∇i}ht=⇒t∈∝a\}b∇ack⌉tl⌉{ti∝a\}b∇ack⌉t∇i}ht≺∃ (i)/bracerightig
=n/intersectiondisplay
i=0¬∝a\}b∇ack⌉tl⌉{ti∝a\}b∇ack⌉t∇i}ht∪∝a\}b∇ack⌉tl⌉{ti∝a\}b∇ack⌉t∇i}ht≺∃ (i),
(4.15)
where¬X=Σ∗\Xdenotes the complement set.
41
4.2 Authority and availability
4.2.1 Strictly local events and properties
As indicated in Sec. 4.1.1, we model security properties as s ets of histories P⊆Σ∗, where
Σ =A×J×S, or more generally Σ =A×J×S×L, when security levels need to be taken into
account. An event x∈Σis thus an action of a subject on an object, i.e., a tuple x=∝a\}b∇ack⌉tl⌉{ta,i,u∝a\}b∇ack⌉t∇i}ht, or
x=∝a\}b∇ack⌉tl⌉{ta,i,u,ℓ∝a\}b∇ack⌉t∇i}ht, whereℓis a security level. A more general notion of events will be ne eded for
channel modeling in Sec. 6.3.2.1, but for the moment we stick with the tuples. The reason is
that they conveniently display the strict localities of each event as the component of the tuple
that represents it.
Strictly local views of events. The typeΣof events thus comes with the projections
sbjt:Σ→S, objt:Σ→J, actn:Σ→A, levl:Σ→L(4.16)
assigning to each event x∈Σthe unique subject sbjt(x) that observes or enacts it, the unique
action actn(x) that takes place, the unique object objt(x) that is acted on, and the security level
levl(x) where the event takes place. All such projections p:Σ→Vinduce the strictly local
event typesΣv={x∈Σ|p(x)=v}, which partition the event type as the disjoint unions
Σ =/coproductdisplay
v∈VΣv whereV∈{A,J,S,L} (4.17)
The strictly local event types that we will be working with he re are thus:
•Σu=/braceleftbig∝a\}b∇ack⌉tl⌉{ta,i,u∝a\}b∇ack⌉t∇i}ht|a∈A,i∈J/bracerightbig— the events observable by the subject u;
•Σi=/braceleftbig∝a\}b∇ack⌉tl⌉{ta,i,u∝a\}b∇ack⌉t∇i}ht|a∈A,u∈S/bracerightbig— the events involving the object i;
•Σa=/braceleftbig∝a\}b∇ack⌉tl⌉{ta,i,u∝a\}b∇ack⌉t∇i}ht|i∈J,u∈S/bracerightbig— the events where someone performs the action aon
something.
These types are strictly local in the sense that they are disjoint, i.e., Σv∩Σw=∅as soon as
v/nequalw, for v,wboth of any of the above types V∈{A,J,S,L}.
Locality vs strict locality. Localizing the events at the various security levels, vario us sub-
jects, various actions, and various objects is necessary fo r studying security dynamics. In
Chapters 4–5, we always assume the strict localities, partitioning the events into the disjoint
families in (4.17). In the later chapters (starting from Sec . 6.3.2.1), we will use a more general
and more realistic notion of locality. We start with the spec ial case of strict localities, to keep
things simple until we get used to them. In Chapters 4–5, “local” always means “strictly
local” , unless speciﬁed otherwise. We will keep repeating “strict” to introduce new concepts,
but will elide it eventually.
Strictly local histories. Process models are usually set up so that Alice only sees her o wn
42
actions. An event x=∝a\}b∇ack⌉tl⌉{ta,i,u∝a\}b∇ack⌉t∇i}ht ∈Σis thus observable for Alice just when u=A. This
assumption is imposed on the model using the strict purge operation↾A:Σ∗→Σ∗
Adeﬁned as
/parenleftbig /parenrightbig↾A=/parenleftbig /parenrightbig, (4.18)
(x::y)↾A=(x↾A)::yify=∝a\}b∇ack⌉tl⌉{ta,i,A∝a\}b∇ack⌉t∇i}htfor some a∈Aandi∈J
x↾A otherwise.
Similar strict purge operations can be deﬁned for any of the p rojections in (4.16). The general
(nonstrict) purge operations will be deﬁned and used in Ch. 6 .
Notation. We write xA∈ΣAfor local events, and xA∈Σ∗
Afor strictly local histories. Note the
diﬀerence between
•the strict purge x↾A∈Σ∗
Aof a global history x∈Σ∗, and
•a strictly local history xA∈Σ∗
A, speciﬁed strictly locally, with no reference to a global
context.
Alice’s strictly local view of the property P⊆Σ∗is written
PA={x↾A|x∈P}. (4.19)
Strictly localized properties. A history tA∈Σ∗
Aobserved by Alice is a strict localization of
some global history z∈Σ∗such that z↾A=tA. While zmay be just tAif no one except Alice
did anything in the given global history; or there may be lots of events unobservable for Alice.
She cannot know. But she does know that there are inﬁnitely ma ny possible global histories z
consistent with her observation tA.
If Alice observes z↾A, Bob observes z↾B, and they tell each other what they have seen, they
will still not be able to derive z, even if they are alone in the world. The reason is that neithe r of
them can tell how exactly their actions were mixed: which of A lice’s actions preceded Bob’s
actions, and the other way around.
Example. LetΣ=Σ A/coproducttextΣBwhereΣA={a}andΣB={b}. Suppose that a history tsatisﬁes
the property P=/braceleftbig(aaaa ),(aabb ),(baab ),(bbbb )/bracerightbig. If Alice observes tA=(aa)and Bob
observes tB=(bb), can they be sure that the property Phas been satisﬁed? The possible global
histories consistent with Alice’s and Bob’s local observat ions are
/hatwidet=/braceleftbigz∈Σ∗|z↾A=(aa)∧z↾B=(bb)/bracerightbig
=/braceleftbig(aabb ),(abab ),(abba ),(baab ),(baba ),(bbaa )/bracerightbig.
Any of these actions could have taken place. Alice and Bob wil l only be able to verify locally
a property Pis satisﬁed if it is a strictly localized property, according to the next deﬁnition.
Deﬁnition 4.3. Thestrict localization of a property P⊆Σ∗is the set /hatwidePof all histories with the
43
projections satisfying P, i.e.
/hatwideP={z∈Σ∗|∀u∈S.z↾u∈Pu} (4.20)
where Pu={x↾u|x∈P}.
A property P⊆Σ∗isstrictly localized when P=/hatwideP, i.e.
t∈P⇐⇒ ∀ u∈S.t↾u∈Pu. (4.21)
The family of strictly localized properties is
Loc={P∈℘(Σ∗)|P=/hatwideP}. (4.22)
4.2.2 Resource security as localized safety and liveness
The simplest security properties arise as localized depend ability properties. In particular, au-
thority and availability can be construed as safety and live ness from Alice’s, Bob’s, and other
subjects’ points of view. A process is thus
•authorized if bad stu ﬀ(actions) does not happen to anyone :all bad resource requests are
rejected ;
•available if good stu ﬀ(actions) happens to everyone :all good resource requests are
eventually accepted .
On a closer look, it turns out that there are several reasonab le views of what are "good resource
requests". Do all subjects need to coordinate to make the req uest; or is it enough that some
subjects make the request, and no one objects; or should a maj ority of some sort be required?
To study such questions, we need a formalization.
Reﬁning the idea of safety from (4.9), we say that F⊆Σ∗is an authorization property if it
satisﬁes the following condition:
∀xz∈Σ∗.x/nelementF∧x⊑z=⇒ ∃ u∈S.z↾u/nelementFu. (4.23)
In words, if there is an authority breach in some history, the n in every future of that history,
some subject will observe that their authority has been brea ched. Every authority breach is a
breach of someone’s authority. The logical converse of (4.2 3), reﬁning (4.10), characterizes
authority (or synonymously, an authorization property) Fby
∀xz∈Σ∗./parenleftig
x⊑z∧∀u∈S.z↾u∈Fu/parenrightig
=⇒x∈F. (4.24)
In other words, if the local views z↾uof a history zappear authorized to all subjects u∈S, then
all past histories x⊑zmust have been authorized globally. Note that the statement packs two
intuitively diﬀerent requirements. One is that if a history zis authorized, then every past history
44
x⊑zis authorized; i.e., any authorization property is a safety property. The other one is that if
all local projections of z↾uare authorized, then zis authorized globally; i.e., any authorization
property is a local property. The fact that these two require ments are equivalent to authority is
stated in Prop. 4.5(b) as claim (4.30).
Liveness from (4.11) can be localized in more than one way. We ﬁrst consider what seems to
be the weakest reasonable localization:
∀x∈Σ∗∀u∈S∃z∈D.x↾u⊑z↾u (4.25)
In other words, an availability property is a liveness prope rty where any subject on their own
can assure the liveness.
Deﬁnition 4.4. For any family of subjects Sfor events partitioned into Σ=/coproducttext
u∈SΣu, the autho-
rization and the availability properties are respectively deﬁned
AuthΣ=/braceleftig
F∈℘(Σ∗)|∀x∈Σ∗∀y∈Σ∗./parenleftig
∀u∈S.(x::y)↾u∈Fu/parenrightig
=⇒x∈F/bracerightig
,(4.26)
AvailΣ=/braceleftig
D∈℘(Σ∗)|∀x∈Σ∗∀u∈S∃yu∈Σ∗
u.x↾u::yu∈Du/bracerightig
. (4.27)
When confusion seems unlikely, we omit the subscript Σ.
Proposition 4.5. Let P⊆Σ∗whereΣ=/coproducttext
u∈SΣu. Then the following statements are true.
a) Authorization is strictly local safety. Availability im plies strictly local liveness. Formally,
this means:
P∈AuthΣ⇐⇒ ∀ u∈S.Pu∈SafeΣu∧P=/hatwideP, (4.28)
P∈AvailΣ⇐⇒ ∀ u∈S.Pu∈LiveΣu. (4.29)
b) Authorization is just the global safety that is strictly l ocal. The strict localization of an
availability property is a liveness property. Formally:
AuthΣ={P∈℘(Σ∗)|P∈SafeΣ∧P=/hatwideP}, (4.30)
AvailΣ⊆ {P∈℘(Σ∗)|/hatwideP∈LiveΣ}. (4.31)
Proof .a)Since AuthΣ⊆LocΣfollows from (4.26) for x::y=x, proving (4.28) boils down to
showing that the following two implications are equivalent
/parenleftig
∀u∈S.(x::y)↾u∈Pu/parenrightig
=⇒/parenleftig
∀u∈S.x↾u∈Pu/parenrightig
— which means P∈AuthΣ,
/parenleftig
∀u∈S.(xu::yu)∈Pu=⇒ xu∈Pu/parenrightig
— which means∀u∈S.Pu∈SafeΣu.
for all x,y∈Σ∗and all xu,yu∈Σ∗
u. The bottom-up direction is valid for all predicates in
ﬁrst-order logic: the second implication is stronger than t he ﬁrst one. Towards the top-down
45
direction, ﬁx a subject A, take arbitrary histories xA,yA∈Σ∗
Asuch that (xA::yA)∈PA, and set
x=xAandy=yA. For an arbitrary subject u∈Swe have
(x::y)↾u=(xA::yA)↾u=(xA::yA)∈PAifu=A,/parenleftbig /parenrightbig∈Pu ifu/nequalA.
Since/parenleftbig /parenrightbig∈Pufollows from part (b)below, and (xA::yA)∈PAwas assumed, we have
∀u∈S.x↾u∈Pu, as claimed.
Towards (4.29), we need to show that the following statement s are equivalent:
∀x∈Σ∗∃yu∈Σ∗
u.(x↾u::yu)∈Pu — which means P∈AvailΣ,
∀xu∈Σ∗
u∃yu∈Σ∗
u.(xu::yu)∈Pu — which means∀u∈S.Pu∈LiveΣu.
To show that the ﬁrst implies the second, consider that the se cond is just a special case of the
ﬁrst one, obtained by taking x=xu, and noting that xu↾u=xu. For the converse direction,
we consider arbitrary xu∈Σ∗
uand any xwith x↾u=xufor all u∈S. Then, the choices
ofyuprovided by the local liveness requirements, to meet (xu::yu)∈Pu, will also meet the
requirements for availability: (x↾u::yu)∈Pu, so that availability follows.
b)Towards (4.30), consider
/parenleftig
∀u∈S.(x::y)↾u∈Pu/parenrightig
=⇒ x∈P — which means P∈AuthΣ,
(x::y)∈P=⇒ x∈P — which means P∈SafeΣ.
The fact that AuthΣ⊆SafeΣmeans that the ﬁrst implication follows from the second one. This
is true because (x::y)∈Pimplies (x::y)↾u∈Pufor all u. The converse clearly also holds as
soon as Pis local. And (4.31) is obvious, since yu∈Σ∗
u⊆Σ∗. /boxempty
Example 1: Authority and availability of sheep and oil. To model sheep security, we
zoom out again and go back to the situation where Alice and Bob need to share some of their
resources. The subject type is thus S={A,B}, the objects are from J={sheep,oil}, and the
actions areA={shear,cook}. The possible events in the simple form of 4.1 would thus be al l
triples fromA×J×S. But since Alice and Bob will not shear the oil or cook the shee p, we
take4
Σ =/braceleftbigshear sheepA,cook oil B/bracerightbig.
4See the remark about Standard and reﬁned Σtypes in Sec. 4.1.1 TheΣtype used here is in a reﬁned form of
(4.2).
46
Consider the following properties of Alice’s and Bob’s inte ractions:
Either=shear sheep∗
A∪cook oil∗
B, (4.32)
Alternate=/parenleftbigshear sheepA:: cook oil B/parenrightbig∗, (4.33)
Finish=/braceleftbigshear sheepA,cook oil B/bracerightbig∗:: shear sheepA:: cook oil B. (4.34)
These properties provide counterexamples for the converse s of the claims of Prop. 4.5:
•(4.32) Either∈SafeΣ\(AuthΣ∪Loc);
•(4.33) Alternate A∈LiveΣA, and Alternate B∈LiveΣB, but Alternate/nelementLiveΣ;
•(4.34) Finish∈LiveΣ∩AvailΣ.
Example 2: Authorization and availability of elevator. To model the security of the eleva-
tor, we consider the events from the point of view of the subje cts, i.e., in the form
Σ =J×A×S=/braceleftbig∝a\}b∇ack⌉tl⌉{ti∝a\}b∇ack⌉t∇i}htu,(i)u/vextendsingle/vextendsingle/vextendsinglei∈J,u∈S/bracerightbig
where
•J=/braceleftbig0,1,2,..., n/bracerightbigare the objects again: the ﬂoors of the building (denoted by v ariables
x0,x1,...);
•A=/braceleftbig∝a\}b∇ack⌉tl⌉{t−∝a\}b∇ack⌉t∇i}ht,(−)/bracerightbigare the actions: “call /go” and “arrive”, respectively;
•S=/braceleftbigA,B/bracerightbigare the subjects: Alice and Bob (denoted by variables Y0,Y1,...).
A history is now in the form
/parenleftbig∝a\}b∇ack⌉tl⌉{tx0∝a\}b∇ack⌉t∇i}htY0∝a\}b∇ack⌉tl⌉{tx1∝a\}b∇ack⌉t∇i}htY1(x2)Y2∝a\}b∇ack⌉tl⌉{tx0∝a\}b∇ack⌉t∇i}htY0(x0)Y2.../parenrightbig
meaning that “ Y0calls to x0,Y1calls to x1,Y2arrives to x2,Y0calls to x0again, Y2arrives to
x0. . . ”. The dependability properties (which required that th e elevator should only go where
called, and that it should eventually go where called) are now reﬁned to
•authorization: the elevator should only take some subject to a ﬂoor if they have a clear-
ance and if they requested it, and
•availability: the elevator should eventually take every subject with a clearance for the
ﬂoor that they requested,
which generalizes Example 2 from Sec. 4.1.2 to
(i)u⊆∃∝a\}b∇ack⌉tl⌉{ti∝a\}b∇ack⌉t∇i}htu≺(i)u,/squiggleleftauthority
∝a\}b∇ack⌉tl⌉{ti∝a\}b∇ack⌉t∇i}htu⊆∝a\}b∇ack⌉tl⌉{ti∝a\}b∇ack⌉t∇i}htu≺∃(i)u./squiggleleftavailability
Note , however, that here we need to assume that the subject uis somehow authorized to go
47
to the ﬂoor i. This is where the static resource security formalism from C hap. 3 needs to be
imported into the dynamic resource security formalisms of h istories and properties. In terms of
multi-level security, the clearance assumption would be cℓ(u)≥pℓ(i). In terms of permission
matrices, it would be ∝a\}b∇ack⌉tl⌉{t−∝a\}b∇ack⌉t∇i}ht,(−)∈Mui. Writing for simplicity either of these assumptions as
the predicate Cℓ(u,i), the above crude idea of authorization and availability pr operties of the
elevator can be formalized to
AuthElev=/braceleftig
t∈Σ∗|∀u∈S∀i∈J. (4.35)
t∈(i)u=⇒/parenleftig
Cℓ(u,i)∧t∈∃∝a\}b∇ack⌉tl⌉{ti∝a\}b∇ack⌉t∇i}htu≺(i)u/parenrightig/bracerightig
AvailElev=/braceleftig
t∈Σ∗|∀u∈S∀i∈J. (4.36)
/parenleftig
t∈∝a\}b∇ack⌉tl⌉{ti∝a\}b∇ack⌉t∇i}htu∧Cℓ(u,i)/parenrightig
=⇒t∈∝a\}b∇ack⌉tl⌉{ti∝a\}b∇ack⌉t∇i}htu≺∃(i)u/bracerightig
The reason why Cℓ(u,i) occurs on the left of the implication for authorization and on the
right for availability is that the lift should be available t oufori only if Cℓ(u,i) holds, and u
should be authorized to go to i whenever Cℓ(u,i) holds. Recall from Ch. 3 that permissions and
clearances may be declared di ﬀerently for diﬀerent states of the system, which means that the
clearance predicate would be in the more general form Cℓ(t,u,i), which makes it dependent on
the history t. Alice could thus have di ﬀerent authorizations in di ﬀerent historic contexts.
Example 3: Questions and answers. Suppose that Alice and Bob are having a conversation:
one asks a question, and the other one answers or asks another question. We denote the action of
asking a question by "?", and the action of answering a questi on by "!". To distinguish between
diﬀerent questions, and to identify the question that can be rep eated, and to bind questions and
the corresponding answers, we assume that there is a ﬁxed set of questionsJ, which we take to
be a set of numbers. Thus, we have the types
•J=/braceleftbig0,1,2,..., n/bracerightbigare the questions that may be asked or answered (viewed as obj ects,
and denoted by variables x0,x1,...again),
•A=/braceleftbig?,!/bracerightbigare the actions: "ask question" and "answer question", resp ectively; and
moreover
•S=/braceleftbigA,B/bracerightbigare the subjects: Alice and Bob (denoted by variables Y0,Y1,...),
which induce the event space in the form
Σ =S×J×A=/braceleftbigu:i?,u:i!/vextendsingle/vextendsingle/vextendsingleu∈S,i∈J/bracerightbig
with histories as sequences of events such as
/parenleftbigY0:x0?Y1:x1?Y2:x2!Y0:x0?Y2:x2!.../parenrightbig
which says that " Y0asks the question x0, then Y1asks the question x1, then Y2answers the
question x2, then Y0asks x0again, then Y2answers x0", etc. Formally, such conversations
between Alice and Bob are of course similar to the operations of the elevator that they share in
48
their building.
The diﬀerence is, of course, that when Alice calls the elevator, she usually wants to use the
service herself; whereas when she asks a question, then she u sually expects Bob to provide an
answer. This leads to the following properties that may be re quired in a formal conversation or
perhaps interrogation:
•all answers questioned: an answer should only be given for a question previously asked
by someone, and
•all questions answered: every question that was asked will eventually be answered by
someone.
Questions and answers in a conversation are matched similar ly like calls and services of an
elevator, but the matching is slightly more general:
u:i!⊆∃w:i?≺u:i!,/squiggleleftanswers questioned
u:i?⊆u:i?≺∃w:i!./squiggleleftquestions answered
A clearance predicate could moreover be used to constrain wh o is permitted to ask which
questions, and who is permitted to answer them. A predicate i n the form Cℓ(u,i,a), meaning
that the subject uhas permission to perform on the object ithe action a, would thus correspond
to the declaration a∈Muiin a permission matrix M. The requirements that all answers should
be preceded by corresponding questions, and that all questi ons should eventually be answered,
can be reﬁned to
AskQ=/braceleftig
t∈Σ∗|∀u∈S∀i∈J. (4.37)
/parenleftig
t∈u:i!∧Cℓ(u,i,!)/parenrightig
=⇒ ∃ w∈S./parenleftig
Cℓ(w,i,?)∧t∈∃w:i?≺u:i!/parenrightig/bracerightig
,
AnsQ=/braceleftig
t∈Σ∗|∀u∈S∀i∈J. (4.38)
/parenleftig
t∈u:i?∧Cℓ(u,i,?)/parenrightig
=⇒ ∃ w∈S./parenleftig
Cℓ(w,i,!)∧t∈u:i?≺∃w:i!/parenrightig/bracerightig
.
The fact that AskQ does not guarantee authority and that AnsQ does not guarantee availability
points to the limitations of the presented formalizations o f authority and availability as tools of
resource security. A particular way to mitigate some of thes e limitations is worked out in the
exercises. The general way to resolve the issue is authentic ation, explored in Ch. 7.
Remark. The presented examples are meant to be simple. In some cases, they are oversim-
pliﬁed. For example, the precedence relation in ∃∝a\}b∇ack⌉tl⌉{ti∝a\}b∇ack⌉t∇i}ht≺(i)does not discharge the calls that
the elevator has responded to. Even the simplest requiremen ts used in real elevators are signif-
icantly more complex than a textbook permits. And elevators are among the simplest systems
that need to be secured.
49
4.2.3 General relativity of histories
If there are just two possible events, i.e. Σ={0,1}, then both 0Σ∗and 1Σ∗are safety properties.
But they are each other’s complements, since 0 Σ∗∪1Σ∗=Σ∗and 0Σ∗∩1Σ∗=∅. Each of them
is, therefore, both a safety property and an unsafety proper ty. Similarly, there are authorization
properties whose complements are also authorizations. If A lice is Bob’s commanding o ﬃcer,
then she needs to be able to authorize him to use a weapon and ad vance in one situation; and
she also needs to be able to authorize him to not use the weapon and retreat in another situation.
Authorizations are not objective or absolute, but subjecti ve and relative. On one hand, they are
relative to the subjective standpoints. In a zero-sum game, one subject’s safe position is the
other subject’s unsafe position, and vice versa. On the othe r hand, they are relative to security
contexts, e.g., in an evolving conﬂict where Alice may need t o modify Bob’s authorizations.
Within each subject’s frame of preferences, di ﬀerent situations induce di ﬀerent authorities.
Formal models, scientiﬁc methods, and general relativity a re needed not only in astronomy
but also in security — to protect us from intuitions that the E arth is ﬂat and that authority is
absolute.
Conﬂicting availability requirements are even more common , and occasionally more subtle,
than the conﬂicting authority claims. If both Alice and Bob n eed water, but the fountain cannot
serve both at the same time, then the fountain may be alive but not available to either of them.
The concept of availability evolved to characterize such si tuations. It signals opportunities for
attacks on shared resources, whereby Bob may use the fountai n just to deny it to Alice. Other
such attacks are described in the next section.
4.3 Denial-of-Service
Although liveness is not a security property and is generall y not a matter of conﬂict, Alice
and Bob may pursue di ﬀerent good stuﬀ(actions) that they want to happen. If each of them
pursues diﬀerent good stuﬀ(actions) , and Alice’s good stuﬀ(actions) preclude Bob’s good stuﬀ
(actions) , then a system that is alive for Alice will be dead for Bob.
If, moreover, Alice’s liveness property happens to be avail ability, so that every subject can
secure it on their own, then Alice can make her good stuﬀ(actions) happen , which then means
that Bob’s opposite good stu ﬀ(actions) will not happen. In such situations, we say that Al ice
has launched a Denial-of-Service (DoS) attack against Bob.
In general, a DoS phenomenon occurs when both a property Pand its complement ¬Pare
liveness properties, and a history that is alive in one sense must be dead in another sense.
When the property ¬Pis moreover available for some subjects, then those subjects can cause
aDenial-of-Service P .
Example 3 again: More questions and answers. One thing we didn’t mention so far is
Alice’s and Bob’s ages. They are, in fact, 2 years old. They le arned to speak very recently, and
50
they are trying to learn the rules of conversation. While Ali ce believes that all questions should
be answered, Bob’s standpoint is that all answers should be q uestioned.
For Alice, a conversation is alive only when every question c an be answered by someone. That
is when Alice’s requirement is a liveness property. It is, mo reover, an availability property when
everyone knows answers to all questions. Only then can anyon e satisfy Alice’s requirement.
Bob’s view is, on the other hand, that a conversation is only a live when there are some unan-
swered questions. Bob’s requirement is also a liveness property whenever, for any question,
there is someone who is permitted to ask it. If, moreover, eve ryone is permitted to ask any
question, then Bob’s requirement is even an availability pr operty.
Formally, Alice’s requirement is thus Service A=AnsQ, whereas Bob’s requirement is oppo-
site: Service B=¬AnsQ.
On the other hand, using (4.38) and the deﬁnitions in the sect ions preceding it, it can be proven
that
∀i∈J∃w∈S.Cℓ(w,i,!)=⇒ AnsQ∈Live,
∀i∈J∀w∈S.Cℓ(w,i,!)=⇒ AnsQ∈Avail,
∀i∈J∃w∈S.Cℓ(w,i,?)=⇒ ¬ AnsQ∈Live,
∀i∈J∀w∈S.Cℓ(w,i,?)=⇒ ¬ AnsQ∈Avail.
If there is a particular question q∈Jthat Bob is permitted to ask, in the sense that Cℓ(B,q,?)
holds true, then the property Service B=¬AnsQ is available to him, as he can extend any
history of questions t∈Σ∗tot::(B:q?)∈¬AnsQ=Service B. Since Service A=AnsQ,
Alice has herewith been denied the service of a ﬁnal answer, t hat she normally requires from
conversations.
If Alice is, on the other hand, permitted to answer the questi onq, in the sense that Cℓ(A,q,!) is
true, then she can extend the current history to t::(B:q?)::(A:q!)∈AnsQ=Service A. Now
Bob has been denied service.
Alice and Bob can continue to deny service to each other like t his until one of them ﬁnds a way
to break the symmetry. For example, Alice may be able to convi nce their parents, Carol and
Dave, that the questions that have been answered in the past s hould not be asked again. Bob’s
Denial-of-Service attack will then depend on how many other questions he is permitted to ask,
i.e., for how many di ﬀerent i∈Jdoes he have a clearance Cℓ(B,i,?). Whether he will win or
not also depends on Alice’s capability Cℓ(A,i,!) to answer questions.
Example 4: TCP-ﬂooding. The internet transport layer connections are established u sing
the Transmission Control Protocol (TCP). Its rudimentary s tructure, displayed on the left in
Fig. 4.3, can be construed as an extension of the basic questi on-answer protocol, where asking
and answering a question is followed by a ﬁnal action, where t he answer is accepted. In the
TCP terminology, the action of asking a question is called SY N, the action of answering it
is called SYN-ACK, and the acceptance of the answer is called ACK. Alice really likes this
51
Figure 4.3: TCP: 3-way handshake and the SYN-ﬂooding
part, as it makes the TCP connections available, by closing t he question-answer sessions with
the answer acceptance. After an answer has been accepted, a T CP server establishes the TCP
network socket and releases the protocol state, i.e., forge ts the question. If Bob, however, never
accepts any answer, then the TCP server has in principle to re member lots of questions, i.e.,
keeps lots of TCP protocol sessions open and, at one point, ru ns out of memory. That is the
SYN-ﬂooding attack, displayed in Fig. 4.3 on the right.
This basic idea of a DoS attack on the Internet is very old, alm ost as old as the Internet, and
there are in the meantime many methods to prevent it; but ther e are even more methods to
circumvent these preventions. DoS attacks are a big busines s, both on the Internet and in
everyday conversations between the 2-year-olds.
The theory presented so far just provides a formal model of se curity properties and suggests
a basic classiﬁcation. The next chapter spells out the secur ity space where this classiﬁcation
turns out to be universal.
52
5 Geometry of security⋆
5.1 Geometry? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
5.2 Geometry of histories and properties . . . . . . . . . . . . . . . . . . . 54
5.3 Geometry of safety and liveness . . . . . . . . . . . . . . . . . . . . . . 55
5.4 Geometry of security . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
5.5 Locality and cylinders . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 7
5.6 Geometry of authority and availability . . . . . . . . . . . . . . . . . . 58
5.7 Decompositions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
5.8 What did we learn about resource security? . . . . . . . . . . . . . . . . 61
5.1 Geometry?
Geometry is the science of space. Space is the way we observe t hings. Our physical space
has 3 dimensions, and we usually subdivide it into cubes beca use we see and hear things up or
down, left or right, forward or backward. Ants observe the wo rld by way of smells and tastes,
and their physical space is structured di ﬀerently. Computers observe the world as traces of
Figure 5.1: An observation in physical space vs an observati on in cyberspace.
computations, as streams of data, as network logs, as listin gs of one sort or another. We have
been calling such listings histories because they have a known past and an unknown future,
separated by the present.
53
5.2 Geometry of histories and properties
Observations. If we record a string of past events a=(a0a1a2···an)∈Σ∗, then we have
observed that our history has the property
a::Σ∗∈℘(Σ∗).
↑ ↑
past f utures
This is an observation . Observations are collected in the family of basic sets
B={a::Σ∗|a∈Σ∗}. (5.1)
Open observables represent possible observations of bad stu ﬀ(actions) that may happen. If
such an observation occurs, it will persist, i.e., remain op en under all futures. A family Oof
open observables is thus required to satisfy the following p roperties:
a)B⊆O — any observation is open;
b)X⊆O=⇒/uniontextX∈O — any set of open observables corresponds to a possible obser va-
tion, i.e., to an open observable;
c)U,V∈O=⇒U∩V∈O— any ﬁnite set of open observables can be observed together
as a single open observable.
The set of observables Othus has to contain B, and all unions of the elements of B. It follows
that
O=/braceleftbigU∈℘Σ∗|x∈U∧x⊑y=⇒y∈U/bracerightbig, (5.2)
because U∈℘Σ∗satisﬁes x∈U∧x⊑y=⇒ y∈Uif and only if it also satisﬁes
U=/uniontext{a::Σ∗⊆U|a∈Σ∗}. It is easy to see that Ofrom (5.2) also satisﬁes (c) since already
Bis closed under∩. According to (5.2), the observables U∈Oare thus the upper-closed sets
under the preﬁx order ⊑. This captures that they are open into all futures.
Closed observables represent observations that bad stu ﬀ(actions) has not happened so far;
i.e., they are the complements F=¬Uof open observables U∈ O. Since for x⊑ythe
implication x∈U=⇒y∈Uis equivalent with y/nelementU=⇒x/nelementU, it follows that the family
of closed observables must be in the form
F=/braceleftbigF∈℘Σ∗|x⊑y∧y∈F=⇒x∈F/bracerightbig. (5.3)
The closed observables are thus the lower-closed sets under the preﬁx ordering. This captures
that they are closed under the past: if bad stu ﬀ(actions) did not happen now, then that statement
was also true at any moment in the past.
54
Dense observables are those that always remain possible: an observable is dens e if it must
be observed eventually, after any future-open observation . Each of the three previously de-
ﬁned families of properties can be used to characterize dens e properties, which leads to three
equivalent characterizations:
D=/braceleftbigD∈℘(Σ∗)|∀a∈Σ∗.D∩(a::Σ∗)/nequal∅/bracerightbig(5.4)
=/braceleftbigD∈℘(Σ∗)|∀U∈O.D∩U=∅=⇒U=∅/bracerightbig
=/braceleftbigD∈℘(Σ∗)|∀F∈F.D⊆F=⇒F=Σ∗/bracerightbig
Dense observations are used to characterize the nice stu ﬀ(actions) that is required to eventually
happen. The third characterization says that if nice stu ﬀ(actions) is not bad, then nothing is
bad; i.e., if a dense property is past-closed, then it contai ns all histories. The deﬁnition is then
relativized to any Y⊆Σ∗as
DY=/braceleftbigD∈℘(Y)|∀F∈F.D⊆F=⇒Y⊆F/bracerightbig. (5.5)
5.3 Geometry of safety and liveness
Proposition 5.1. A history is
a) safe if and only if it is closed: Safe=F
b) alive if and only if it is dense: Live=D
Proof . By inspection of (4.12)(a)⇐⇒ (5.3) and (4.13)(b)⇐⇒ (5.4). /boxempty
Corollary 5.2. Any property X⊆Σ∗factors through the induced lower set ∇X={y⊑x∈X}
X Σ∗
∇XD∇X
Live∇XF
Safe
The set∇X⊆Σ∗is closed in the sense of (5.3) and X⊆∇X is dense in the sense (5.4) .
Requirements are future-open properties. Positive properties, e.g., in the form b≺c, can
be expressed as unions of basic properties, e.g.
b≺c=/uniondisplay
x,y∈Σ∗x::b::y::c::Σ∗∈ O
55
and thus, remain open. Further properties can be expressed a s ﬁnite intersections of those, e.g.
b0≺b1≺··· bn=b0≺b1∩b1≺b2∩··· bn−1≺bn∈ O.
Constraints are past-closed because they correspond to negative requirements, e.g.,
/intersectiondisplay
i∈J¬i!≺i?∈ F because/uniondisplay
i∈Ji!≺i?∈ O
This explains why safety properties from Sec. 4.1.2 often oc cur as negative requirements.
5.4 Geometry of security
Security problems arise from conﬂicting goals: what is good for me is bad for you, and vice
versa. We discussed in Sec. 4.2.1 that they also arise from di ﬀerent standpoints, local observ-
ability, and hiding: I can deceive you if there is something t hat I see and you do not. Burglary,
Figure 5.2: Global observer:
“I think, therefore I exist.”
Figure 5.3: Local observers:
relativistic frames of reference
as an attack on physical security, is more likely to succeed i f there is no one home to see
it. Cybersecurity often requires reconciling Alice’s and B ob’s views of transaction histories.
National security also requires reconciling di ﬀerent views of history. Behind every security
problem, there are di ﬀerent views of some space of histories.
The diﬀerence between cyber security and physical security is the d iﬀerence between the un-
derlying geometries. Physical security is based on physica l distances and velocities: animals
prevent attacks from predators by maintaining a distance th at allows them to outrun the at-
tacker. Cybersecurity is not based on outrunning the attack er because the concept of distance is
unreliable in cyberspace, as many copies of a message travel in parallel. If the implementation
details of network services are abstracted away, then every two nodes in cyberspace look like
56
Figure 5.4: Linking and separating frames of reference in ph ysical space and in cyberspace
neighbors because the underlying networks, in principle, d o their best to route tra ﬃc as fast as
possible. This geometric peculiarity of cyberspace, as a sp ace where every two points are at the
same negligible distance1, is the source of many cybersecurity problems.
5.5 Locality and cylinders
In Sec. 4.2.2, we analyzed how authority and availability ca n be construed as localized versions
of safety and liveness. Here we spell out the geometric meani ng of that observation. It is based
on interpreting the purge operations ↾u:Σ∗→Σ∗
u(4.18) as spatial projections, that reduce global
histories to local views. Instantiating the cylinder localizations from Sec. 3 in the Appendix to
the family of views V=/braceleftbig↾u:Σ∗→Σ∗
u|u∈S/bracerightbig, the task of localizing security properties boils
down to determining how well they are approximated by the cor responding cylinders, which
are the local, subjective views.
Lemma 5.3. A property is localized in the sense of Def. 4.3 if and only if i t is external cylindric
in the sense of Def. A.1 in the Prerequisites:
Loc=ExCyl
Proof . We prove that the localization /hatwideYfrom Def. 4.3 is the special case of cylindriﬁcation /dblbracketleftY/dblbracketright
from Def. A.1:
/hatwideY={y|∀u∈S.y↾u∈Yu}=/intersectiondisplay
u∈S{y|y↾u∈Yu}=/intersectiondisplay
u∈Su∗u!(Y)=/dblbracketleftY/dblbracketright
Hence Loc={Y∈℘(Σ∗)|Y=/hatwideY}={Y∈℘(Σ∗)|Y=/dblbracketleftY/dblbracketright}=ExCyl . /boxempty
1In terms of a disc, as a set of points bounded by circle, as a set of points at equal distances from a center,
cyberspace can be viewed as a disc whose center is everywhere , and whose bounding circle is nowhere. That
very same geometric property was often proposed as the deﬁni ng characteristic of God. The proposal is said
to have originated from Hermes Trismegistos, but it was also repeated e.g., by V oltaire.
57
5.6 Geometry of authority and availability
Proposition 5.4. A history is
a) authorized if and only if it is closed and cylindric:
Auth=/braceleftbigP∈℘(Σ∗)|P∈F∧ P=/dblbracketleftP/dblbracketright/bracerightbig=/dblbracketleftF/dblbracketright
b) available only if its cylindriﬁcation is dense:
Avail⊆/braceleftbigP∈℘(Σ∗)|/dblbracketleftP/dblbracketright∈D/bracerightbig=/dblbracketleftD/dblbracketright−1
Proof . (a) follows from (4.30) and Lemma 5.3. (b) follows from (4.2 5). /boxempty
Corollary 5.5. The closure operator ∇/dblbracketleft−/dblbracketright:℘(Σ∗)→℘(Σ∗)factors any property X ⊆Σ∗
through its cylindric closure ∇/dblbracketleftX/dblbracketright={y⊑x|∀u∈S.x↾u∈Xu}
X Σ∗
∇X∇/dblbracketleftX/dblbracketrightAvail Auth
where Auth is the set of close cylindric sets /dblbracketleft∇X/dblbracketright⊆Σ∗, whereas Avail is the set of dense
cylindric sets X⊆/dblbracketleft∇X/dblbracketright.
Remark. The closure∇/dblbracketleftX/dblbracketrightis obtained by sending Xthrough℘(Σ∗)/dblbracketleft−/dblbracketright−−→℘(Σ∗)∇−−→℘(Σ∗),
which is the composite of the lower closure operator ∇from Prerequisites 2(4), instantiated to
the space of histories, and the cylinder closure operator /dblbracketleft−/dblbracketrightfrom Prerequisites 3(6). It is easy
to check that/dblbracketleft∇X/dblbracketright=∇/dblbracketleftX/dblbracketright.
5.7 Normal decompositions of properties
Corollaries 5.2 and 5.5 establish how any speciﬁed constrai ntX⊆Σ∗can be approximated by
a safety constraint ∇Xor by an authority constraint /dblbracketleft∇X/dblbracketright. This is useful because safety con-
straints and authority constraints are generally easier to implement and validate than arbitrary
constraints.
Safety-driven decompositions. The closure∇X, in general, declares more histories to be
safe than intended by X. This deviation of ∇Xfrom Xcan, however, always be corrected by
58
intersecting∇Xwith a liveness property in such a way that the intersection c ontains precisely
the original property X. The histories that are in ∇Xbut not in Xwill thus be declared safe but
not alive. This is a special case of the geometric decomposit ion from prerequisites section 2 of
arbitrary sets into closed and dense sets.
Proposition 5.6. Any property can be expressed as an intersection of a safety p roperty and a
liveness property:
X=∇X∩(X∪¬∇ X).
Proof . Recalling that in the space Σ∗of traces the closure is ∇X={y⊑x∈X}, we have
X∪¬∇ X
X∇X∩(X∪¬∇ X)Σ∗
∇XDLive
F
Safe
/boxempty
Example 1: sheep life. If Alice wants to shear her sheep at most once every year, and o nly
to use meat in the end, the requirement might be
SheepLife=MilkWoolMeat∩MilkMeatWoolAnnual — where
MilkWoolMeat={milk,wool}∗:: meat
MilkMeatWoolAnnual ={milk,meat}∗∪/bracketleftbigmilk milk...milk/bracehtipupleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehtipdownright/bracehtipdownleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehtipupright
365 timeswool/bracketrightbig:: MilkWoolAnnual
The normal decomposition is then
SheepLife=∇SheepLife∩/parenleftbigSheepLife∪∆NoSheepLife/parenrightbig
where
•∇SheepLife consists of all histories where sheep’s wool is on ly ever used after 365 con-
secutive milkings, and where sheep’s meat is only ever used a t the end, after which there
are no further events; and
•∆NoSheepLife=¬∇SheepLife consists of all histories where the rules of ∇SheepLife
are broken.
59
Remark. Note that the liveness part of the SheepLife decomposition i s thus unsafe, whereas
the safety part of the decomposition is not alive. It is, of co urse, reasonable to expect this.
Yet, when the liveness part and the safety part are tested ind ependently, this sometimes causes
problems.
Authority-driven property decompositions. The cylinder closure /dblbracketleft∇X/dblbracketrightin Corollary 5.5 is
the smallest authorization property containing all histor ies from a given X⊆Σ∗. If a security
designer wants to make sure that all histories contained in Xare authorized, he will thus use
/dblbracketleft∇X/dblbracketright. Some of the histories authorized in /dblbracketleft∇X/dblbracketrightmay not be in X; but they can be eliminated as
unavailable, by intersecting /dblbracketleft∇X/dblbracketrightwith an availability property, as spelled out in Prop. 5.7. T he
desired property Xnow remains as the set of precisely those histories that are b oth authorized
and available, just like it remained as the set of histories t hat are both safe and alive in the
safety-driven decomposition above.
Proposition 5.7. Any property can be expressed as an intersection of authoriz ation and avail-
ability:
X=/dblbracketleft∇X/dblbracketright∩(X∪¬/dblbracketleft∇X/dblbracketright).
Proof . Recalling that the cylinder closure is /dblbracketleft∇X/dblbracketright={y∈Σ∗|∃x∈X∀u∈S.y↾u⊑x↾u}, and
expanding the diagram from Corollary 5.5, we have
X∪¬/dblbracketleft∇X/dblbracketright
X/dblbracketleft∇X/dblbracketright∩(X∪¬/dblbracketleft∇X/dblbracketright)Σ∗
/dblbracketleft∇X/dblbracketright.Avail
Auth
/boxempty
Availability-driven decomposition. The decomposition in Prop. 5.7 is authority-driven be-
cause it is obtained by embedding the desired property Xinto the smallest authorization prop-
erty/dblbracketleft∇X/dblbracketrightthat contains it. One of the equivalent logical forms of auth ority derived in Sec. 4.2.2
was
∀xz∈Σ∗.x/nelementF∧x⊑z=⇒ ∃ u∈S.z↾u/nelementFu (5.6)
It says that any unauthorized event will be observed by someo ne, independently on others. No
60
cooperation is needed. On the other hand, the corresponding availability property
∀x∈Σ∗∀u∈S∃z∈Σ∗./parenleftig
x↾u⊑z↾u∧z∈D/parenrightig
(5.7)
says that in any situation there is a future for all together . More precisely, it says that Alice on
her own can ﬁnd yAsuch that x↾A::yAis in DA; and Bob can ﬁnd yBon his own, such that
x↾B::yBis in DB; but they must come together to ﬁnd z∈Dsuch that z↾A=x↾A::yAand
z↾B=x↾B::yB. Finding such zrequires scheduling local histories. This requires cooperation.
Astrong availability requirement, strengthening (5.7) so that the required acti ons can be real-
ized by individual subjects with no need for cooperation, is
∀x∈Σ∗∀z∈Σ∗∃u∈S./parenleftig
x↾u⊑z↾u=⇒ z∈D/parenrightig
(5.8)
saying there is someone for whom a future is available independently of others .
Proposition 5.8. Any property can be expressed as a union of a strong availabil ity and an
authority breach:
X=/llparenthesis∆X/rrparenthesis∪/parenleftig
X∩¬/llparenthesis∆X/rrparenthesis/parenrightig
(5.9)
Proof . The largest strong availability property contained in Xis
/llparenthesis∆X/rrparenthesis={y|∃u∈S.y↾u∈Xu⇒y∈∆X}=/uniondisplay
u∈S{y|y↾u∈Xu⇒y∈∆X}=
/uniondisplay
u∈Su∗u∗(∆X)=¬/dblbracketleft∇¬X/dblbracketright
Since Prop. 5.7 gives ¬X=/dblbracketleft∇¬X/dblbracketright∩(¬X∪¬/dblbracketleft∇¬X/dblbracketright), the claim follows by taking comple-
ments on both sides. /boxempty
5.8 What did we learn about resource security?
Security properties of histories and interactions are like syntactic properties of sen-
tences. Security properties as families of trace properties are the “syntactic types” of the
“language” of network interactions. A correctly typed secu rity policy is like a syntactically
well-formed sentence in this language. A syntactically inc orrect sentence surely does not con-
vey the intended meaning. But syntactic correctness does no t guarantee that it does. An au-
thorization policy that is not expressed in terms of an autho rization property surely does not
implement the desired authority; but an authorization poli cy expressed in a sound authority
property does not guarantee the desired security e ﬀects. The meaning of a sentence, a program,
61
or a protocol is up to the designer. The science of security on ly provides the tools to achieve
the desired goals, just like the syntax of a language facilit ates communication — but does not
create the content to be communicated.
The geometric view allows us to express any set as an intersec tion of a closed and a dense set.
Since safety and liveness properties of computations turn o ut to be past-closed and dense sets,
any property of computations Xcan thus be approximated by a safety property, and precisely
recovered by intersecting that safety property with a liven ess property.
In cyberspace, the authority properties turn out to be past- closed cylindric sets, whereas the
availability properties are the sets whose cylindriﬁcatio ns are dense. Therefore, any resource
security property can be approximated by an authority prope rty, and precisely recovered by
intersecting it with an availability property.
In this chapter, we formalized arbitrary, often complex, re source security requirements and
decomposed them into normal forms. Verifying whether secur ity requirements are satisﬁed
can thus be simpliﬁed to standardized tests. In the next chap ter, we shall see how non-local
properties, that cannot be tested locally at all, can be esta blished and assured through non-local
interactions.
62
6 Relational channels and
noninterference
6.1 Networks and channels . . . . . . . . . . . . . . . . . . . . . . . . . . 63
6.1.1 Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
6.1.2 Channels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
6.1.3 Possibilistic channels . . . . . . . . . . . . . . . . . . . . . . . . 6 6
6.1.4 Possibilistic channel types and structure . . . . . . . . . . . . . . 67
6.2 Channel safety and inference . . . . . . . . . . . . . . . . . . . . . . . 70
6.2.1 Observation and inference in overt channels . . . . . . . . . . . . 70
6.2.2 Inverse channels . . . . . . . . . . . . . . . . . . . . . . . . . . 70
6.2.3 Unsafe channels . . . . . . . . . . . . . . . . . . . . . . . . . . 71
6.3 Channel security and interference . . . . . . . . . . . . . . . . . . . . . 72
6.3.1 Example: Elevator as a shared channel . . . . . . . . . . . . . . 73
6.3.2 Sharing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
6.4 Relational noninterference . . . . . . . . . . . . . . . . . . . . . . . . . 77
6.4.1 Ideas of covert channels and interference . . . . . . . . . . . . . 77
6.4.2 Characterizing noninterference . . . . . . . . . . . . . . . . . . . 79
6.4.3 Noninterference as invariance . . . . . . . . . . . . . . . . . . . 81
6.1 Networks and channels
6.1.1 Networks
We live in networks. Our computers and devices are connected to electronic networks, our
friends and families to social networks, our species to biol ogical networks, and so on. The
resources studied so far are the nodes of the various network s that we live in. The channels
studied in this chapter are the network links.
A network is an abstraction of space. It abstracts away the ir relevant features of space and
displays some features of interest as network nodes. The net work links present the connections
or relations between the nodes. Fig. 6.1.1 shows a physical s pace (the City of London) on the
top, a network of interest (the London Underground) at the bo ttom, and the two projected over
each other in the middle. The network nodes are the tube stati ons, whereas the network links are
the railway connections between them. The tra ﬃc channels are realized by the trains traveling
over the network links. They input the passengers who enter a nd output the passengers who
exit. The passengers may enter through many entrances, but a t each entrance, they enter one
after the other. For simplicity, we assume that they all ente r in a sequence (since that will be the
case in most examples of interest here). But di ﬀerent passengers travel to di ﬀerent destinations,
63
Figure 6.1: The London Underground network is an abstractio n of London
64
which are usually unknown. The tra ﬃc models, therefore, take into account possibilities or
probabilities. Since each passenger’s trip depends on the t raﬃc load, corresponding to where
the other passengers are going, the tra ﬃc channels take the sequences of entering passengers
at the input but single exits at the output, taking into accou nt all passengers that may possibly
exit there, and even assigning the probabilities to each of t hem, if known. The fact that all
passengers eventually exit the network is recovered by accu mulating the sequences of single-
exit outputs.
We ﬁrst focus on modeling a single channel and then expand the view to networks with many
channels and the protocols regulating the network tra ﬃc. The individual model is formalized
in Sec. 6.1.2, and the cumulative view in Sec. 6.1.4.1.
6.1.2 Channels
Idea. Channels transmit what we know, what we have, or what we are. M ost organisms are
equipped with senses to recognize each other and to display t heir traits to each other. The
senses are the basic channels. Tra ﬃc channels transmit what we have, and communication
channels transmit what we know. Our ﬁrst communication chan nels were the body gestures
and the speech. To be communicable, data must be encoded in a l anguage. Communication
channels transmit codes and languages. They are the content that the channels transmit.
Channels are the media that transmit content. An electronic channel can be a copper wire that
conducts electronic signals between two nodes in a circuit. The copper wire is the medium that
carries the communication channel. The radio waves are the m edium that carries the content
of radio channels. Radio channels carry the radio communica tions. Physicists used to think
that the radio waves were carried by a speciﬁc medium, the ether , but the theory of relativity
established that the only carrier of radio waves and light wa s the space itself. The theory of
relativity says that space is a channel and that information cannot travel through it faster than
light. All channels transmit information and can be used for communication. A communication
channel can be implemented on top of a radio channel, or a copp er wire, or even the English
Channel as the medium. A social channel can be implemented on top of a communication
channel. There are layers and layers of channels, implement ed on top of each other, but the
basic picture of each of them is always the same.
Channel as a history-sensitive function. A channel is a function that inputs histories.
While an ordinary function is in the form X→Y , a channel is in the form X+→Y , whereX+
is the type of strings of events from X, representing histories. (See Prerequisite 1.) A channel
is thus a history-sensitive function. The other way around, a function is a memoryless channel,
meaning that its output only depends on the most recent event , and not on the memory of earlier
events.
65
6.1.3 Possibilistic channels
Subsets and relations. A relation is a function in the form X→℘Y, where℘Y={Y⊆Y}
is the set of subsets of Y, its powerset. A relation is thus a function that may output m ultiple
values, or nothing. A function is a single-valued, total rel ation. The bijection℘Y/simequal{0,1}Y,
i.e., the correspondence
Y∈℘Y
==================/bracketleftbig−/bracketrightbig
Y:Y→{ 0,1}where/bracketleftbigy/bracketrightbig
Y=1⇐⇒ y∈Y (6.1)
induces the bijective correspondence
r:X→℘Y
==========================/bracketleftbig−⊢ r−/bracketrightbig:X×Y→{ 0,1}where/bracketleftbigx⊢ry/bracketrightbig=1⇐⇒ y∈rx (6.2)
A subset YofYcan thus be viewed as a 01-vector/bracketleftbig−/bracketrightbig
Y∈{0,1}Y, whereas a relation rfromX
toYcan be viewed as a 01-matrix/bracketleftbig−⊢ r−/bracketrightbig∈{0,1}X×Y.
Function view. A possibilistic (or relational) channel is a binary relatio n between strings of
inputs fromXand outputs inY. They can be viewed as functions f:X+→℘Y, mapping the
input histories x=/parenleftbigx0x1...xn/parenrightbigto the sets of possible outputs fx⊆Y.
Matrix view. According to (6.1), the output subsets can be equivalently v iewed as functions
to the set{0,1}. According to (6.2), the functions f:X+→℘Ycan be equivalently viewed as
the matrices/bracketleftbig− ⊢ f−/bracketrightbig:X+×Y→{ 0,1}whose entries are the sequents/bracketleftbigx0x1...xn⊢fy/bracketrightbig. The
bijective correspondence in (6.2) is thus instantiated to
f:X+→℘Y
===========================/bracketleftbig−⊢ f−/bracketrightbig:X+×Y→{ 0,1}where/bracketleftbigx0x1...xn⊢fy/bracketrightbig=fx0x1...xn(y) (6.3)
When the channel name fis clear from the context, we elide it and write/bracketleftbigx0x1...xn⊢y/bracketrightbig, or
more succinctly as/bracketleftbigx⊢y/bracketrightbig=fx(y) for x=/parenleftbigx0x1...xn/parenrightbig.
Special case: deterministic channels. A deterministic channel is a relational channel f
where the inputs determine unique outputs, which means that fx⊆Y is either a singleton {y}or
the empty set∅. A deterministic channel is, therefore, in the form f:X+→Y⊎ 1, where 1={∅}
and⊎is the disjoint union. It can be viewed as the partial function f:X+⇀Y, considered
undeﬁned when the output is empty. The empty output must be al lowed if computable channels
are to be taken into account, since computations may search i nﬁnitely long and not return any
output.
Special case: memoryless channel. While a channel is a history-sensitive function, a
function is a memoryless channel, whose output only depends on the most recent input, i.e.
fx0x1...xn=fxn. (6.4)
66
Examples. The English Channel links the North Sea and the Atlantic Ocea n. It is a determinis-
tic channel, because the same ships that enter eventually ex it, except those that sink. A channel
for land traﬃc can be a freeway between two cities or a railway line between two stations. The
sequence of cars/parenleftbigx0x1...xn/parenrightbigthat enters the freeway in the city Xis generally diﬀerent from
the set of cars Y={y0,y1,..., ym}that may exit in the city Y, since some cars that entered go
to other cities, and some that exit come from other cities. Th e set Yis not ordered because
its elements are the cars that may exit at a given moment. Belo w we introduce an equivalent
relational channel formalism, which also displays the sequ ence of outputs on a given sequence
of inputs. The relation/bracketleftbigx0x1...xn⊢y/bracketrightbigmeans that the car ymay exit the freeway after the
sequence of inputs/parenleftbigx0x1...xn/parenrightbig.
6.1.4 Possibilistic channel types and structure
6.1.4.1 Cumulative channels
Chatbots as channels. A chatbot inputs prompts as sequences of words1and outputs the
corresponding responses, which are also sequences of words . But the crucial point is that it
does not generate the response all at once. It ﬁrst guesses th e most likely ﬁrst word of the
response, which it then adds to the context and guesses the mo st likely next word, and so
on. So the chatbot, in principle, inputs sequences of words a nd outputs words, one at a time.
Although some words are more likely than others, taking into account just which words y∈Y
arepossible as continuations of the contexts x∈X+gives a channelX+→℘Y. Later, in
Ch. 8, we will spell out tools that will allow us to take into ac count the diﬀerent probabilities
of diﬀerent words in the same context. In any case, a chatbot is a channel . But taking just a
single word of the chatbot output into account provides a poo r picture of its performance. The
miracle of language is that the single words that we say form sentences; and that s entences
form stories, and conversations. The chatbot miracle boils down to accumulating the single
word outputs of the channel X+→℘Yand presenting it in the form X∗→℘Y∗, mapping the
prompts as contexts to the sequences of words that are its res ponses. This is the cumulative
form of the channel.
Accumulating and projecting strings. The string constructors given in Prerequisites 1 in-
duce the bijection
X+×X X+(::)
/simequal
∝a\}b∇ack⌉tl⌉{tpast,last∝a\}b∇ack⌉t∇i}ht(6.5)
1They actually partition words into tokens , and also take punctuation into account, also as tokens. Her e, it only
matters that the contexts are sequences. It does not matter w hether they are sequences of words or sequences
of tokens. For simplicity, we ignore the di ﬀerence.
67
where
past/parenleftbigx1x2...xn−1xn/parenrightbig=/parenleftbigx1x2...xn−1/parenrightbig,
last/parenleftbigx1x2...xn−1xn/parenrightbig=xn.
The condition in (6.4) can now be written in the form
fx=flast(x).
A channel fis thus memoryless if and only if the following diagram commu tes
X+
X ℘Y
X+lastf
(−)
f(6.6)
with the embedding ( −):X֌X+from Prerequisites 1(2).
Accumulating and projecting channels. The string constructors given in Prerequisites 1
induce the maps
/braceleftig
X+→℘Y/bracerightig /braceleftig
X∗→℘Y∗/bracerightig(−)∗
(−)∗(6.7)
where f:X+→℘Yandg:X∗→℘Y∗are mapped to
f∗()=/braceleftbig()/bracerightbig
g∗(x)=∅ ifg(x)⊆/braceleftbig()/bracerightbig
last(g(x))otherwisef∗(x::a)=f∗(x) ::f(x::a)
where the concatenation (::) and the projection lastare extended from elements to subsets
Y∗×Y(::)−−→Y∗
℘Y∗×℘Y(::)−−→℘Y∗Y+last−−→Y
℘Y+last−−→℘Y
along the direct images so that
f∗(x) ::f(x::a)={y::b|y∈f∗(x),b∈f(x::a)} last(g(x))={last(y)|y∈g(x)}
It is easy to verify that (f∗)∗=falways holds, whereas (g∗)∗=gholds if and only if gpreserves
the list lengths.
68
Sequent notation for cumulative channels. To capture correspondence (6.7), the notation
from Sec. 6.1.2 extends to
/bracketleftbigx0...xn⊢y0...yn/bracketrightbig=/bracketleftbigx0⊢y0/bracketrightbig·/bracketleftbigx0x1⊢y1/bracketrightbig·/bracketleftbigx0x1x2⊢y2/bracketrightbig···/bracketleftbigx0...xn⊢yn/bracketrightbig(6.8)
6.1.4.2 Continuous channels
Deﬁnition 6.1. Acontinuous (possibilistic) channel is a functionγ:℘X∗∪− →℘Y∗which pre-
serves all unions and maps ﬁnite sets to ﬁnite sets:
γ/parenleftig/uniondisplay
U/parenrightig
=/uniondisplay
U∈Uγ(U) andγ/parenleftig
{x}/parenrightig
=/braceleftig
y1,y2,..., ym/bracerightig
(6.9)
Channel continuation and restriction. The bijection between the cumulative channels
g:X∗→℘Y∗and the continuous channels γ:℘X∗∪− →℘Y∗is in the form
/braceleftig
X∗→℘Y∗/bracerightig /braceleftig℘X∗∪− →℘Y∗/bracerightig(−)#
/simequal
(−)#(6.10)
where the transformations
X∗g− →℘Y∗
℘X∗g#
−→℘Y∗℘X∗γ− →℘Y∗
X∗γ#−→℘Y∗(6.11)
deﬁne
g#(U)=/uniondisplay
x∈Ug(x) γ#(x)=γ({x})
Sequent notation for continuous channels. To capture correspondence (6.10), the sequent
notation is further extended from (6.8) to U∈℘X∗andV∈℘Y∗
/bracketleftbigU⊢V/bracketrightbig=/logicalordisplay
x∈U
y∈V/bracketleftbigx⊢y/bracketrightbig(6.12)
6.1.4.3 Overt and covert channels
Anovert channel is provided to serve an overtly speciﬁed intent. For example, a phone is an
overt channel for conversations. The security checkpoint a t the airport is an overt channel for
passengers.
69
Acovert channel is, on the other hand, used covertly and against the s peciﬁed intent. For
example, the phone can be used as a covert channel if Alice and Bob establish a secret code
to transmit a conﬁdential message. Say, if Alice calls at mid night and hangs up as soon as
Bob picks up, the message means that Bob should leave the fron t door open. While the overt
constraint for security checkpoints is that no more than 3.4 oz of liquid should be permitted into
the secure area, Alice and Bob can establish a covert channel by pooling their 3.4 oz containers
together, to transmit 6.8 oz liquid into the secure area.
While overt channels can be unsafe, the purpose of channel se curity is to prevent covert chan-
nels.
6.2 Channel safety and inference
6.2.1 Observation and inference in overt channels
Channels are often used to model causation . A channel of type X+→℘Yis then thought of as
a process of observing the e ﬀects of typeYcaused by sequences of events of type X. Typically,
the input causes in Xare unobservable, and the task is to infer information about them from
the output eﬀects inY. For example, a microscope and a telescope are such channels , making
the invisible visible. Galileo’s telescope is in Fig. 6.2, t ogether with Newton’s cradle, which is
a channel displaying the invisible transmission of force al ong a sequence of adjacent stationary
metal balls. Most scientiﬁc instruments are channels that t ransform unobservable inputs into
observable outputs. They are black boxes enclosing causati ons. The question: “What can be
inferred about the unobservable causes of this observed e ﬀect?” — becomes the problem of
channel decoding:
What can be inferred about the channel inputs from the channe l outputs?
This makes a channel into the principal tool of inductive inference . and the cumulative chan-
nels from Sec. 6.1.4.1 into a rudimentary model of empiric in duction [40].
6.2.2 Inverse channels
If a relational channel g:X∗→℘Y∗maps causal contexts x∈X∗to sets of their possible
eﬀects fx⊆ Y∗, then the task of deriving causes from e ﬀects, i.e., channel inputs from its
outputs requires that we construct the inverse channel ˜ g:Y∗→℘X∗deﬁned as
˜gy={x∈X∗|y∈gx}.
Given in the matrix form, the inverse of the relational chann el/bracketleftbig− ⊢ g−/bracketrightbig:X∗×Y∗→{0,1}is
just
/bracketleftbigy⊢˜gx/bracketrightbig=/bracketleftbigx⊢gy/bracketrightbig.
70
Figure 6.2: A channel transmits (Unobservable causes )+→(Observable eﬀects)
6.2.3 Unsafe channels
Based on the black boxes of causation, inductive inference i s always hypothetical. The claim
that a sequence of events x0x1...xncauses the event ymeans that the correlation/bracketleftbigx0x1...xn⊢y/bracketrightbig
has been frequently observed on the input and the output of a c hannel black box. Such a claim
can never be deﬁnitely proven, since further observations m ay always disprove it. It is accepted
as inductively validated only until it is disproven.
History of science is the history of disproven hypotheses, s tated as channel correlations
/bracketleftbigx0x1...xn⊢y/bracketrightbig.
Fig. 6.3 shows the diagram illustrating the hypothesis that the properties yof a person’s mind are
correlated with the shapes x0,x1,..., xnof their skull. The claim was that the scull provides an
overt channel into the human mind. This hypothesis, which we nt under the name phrenology ,
was conveniently viewed as true by many people through the XI X century. Although disproved
early on, it continued to be attractive for the same reason fo r which many people still follow
predictions of their horoscopes. The human mind is an ongoin g quest for unobservable causes
of everything we observe. When we cannot ﬁnd some likely caus al explanations supported by
experience, we contrive them. We are primed to think in terms of causal channels/bracketleftbigx⊢y/bracketrightbig, and
we construct them, no matter what.
71
Figure 6.3: Phrenological map of human mind
Chatbots are primed to answer questions, and when they canno t ﬁnd some likely answers sup-
ported by their training datasets, they contrive them. Thei r artiﬁcial mind is an ongoing process
of generation for which the underlying language models are t rained. They are required to ex-
trapolate the given context and generate a response to every prompt no matter what. Hence the
phenomenon of chatbots’ hallucinations . A hallucinating chatbot is unsafe because a contrived
correlation/bracketleftbigx⊢y/bracketrightbigof your prompt xand the chatbot’s response ymay cause bad stuﬀto happen.
In general, a channel is unsafe when it outputs observables ythat are correlated with the inputs
xby ﬂawed inferences or superstitions [50, Ch. 4].
6.3 Channel security and interference
Security problems arise from sharing goals, views, or resou rces; or rather from not sharing.
Alice wants to do this, and Bob wants to do that, and they canno t do both. Or they both
need the same thing, but only one can have it. A channel can be v iewed as a resource, a tool
for acquiring or transmitting information. Alice may need t o protect her private information
transmitted by the shared channel. To circumvent the protec tions, Bob may construct a covert
channel within the overt channel shared with Alice. Alice’s private information then covertly
leaks to Bob through a channel within a channel.
Idea of interference. If Alice is using a channel on her own and no one else needs it, t here
are no security problems with it. Channel security problems arise when Alice and Bob share a
channel. Each of them enters their inputs and observes their outputs. The channel f:X+→Y
receives the inputs as they are entered, and its input histor ies are the shuﬄes of Alice’s inputs
72
with Bob’s inputs, say
x=/parenleftbigxA
0xA
1xB
2xB
3xB
4xA
5xB
6.../parenrightbig
After each of Alice’s inputs, the channel produces an output for Alice; after each of Bob’s
inputs, it produces an output for Bob. Although each of them o nly observes their own outputs,
Bob’s output yB=f(xA
0xA
1xB
2) depends not only on his input xB
2but also on Alice’s inputs xA
0and
xA
1. Therefore, Bob’s input xB
2may cause the output yBin Alice’s context xA
0xA
1, but it may cause
a diﬀerent output yBif Alice enters a di ﬀerent context xA
0xA
1. When that happens, we say that
the private inputs interfere in the shared channel. Interference is the main problem of ch annel
security. Bob cannot directly derive Alice’s inputs xA
0xA
1orxA
0xA
1from his outputs yBandyB,
but he can derive that Alice has entered di ﬀerent inputs. That is one bit of information about
Alice’s inputs. If the inputs are 0 or 1, then there are just 4 p ossible inputs xA
0xA
1. If Bob can get
2 bits of information about Alice’s inputs, he can guess them .
In this section, we formalize the security problem of interference . In the next section, we
characterize the security requirement of noninterference .
6.3.1 Example: Elevator as a shared channel
The elevator in Fig. 6.4 inputs the calls to the ﬂoors 0 ,1,up to nrepresented as the events of
type
X=/braceleftig
∝a\}b∇ack⌉tl⌉{ti∝a\}b∇ack⌉t∇i}ht|0≤i≤n/bracerightig
.
It outputs the services
Y={(/squigglerighti),(⊙i)|0≤i≤n}
where
•(/squigglerighti)means: "The elevator comes to ﬂoor i" , whereas
•(⊙i)means: "The elevator is already on ﬂoor i".
The elevator can be viewed as a deterministic channel X+ele−−→Y realized by the state machine
in Fig. 6.5. Note that it is assumed that the outputs are produ ced both at each transition and at
each state.2
Sharing the elevator. If Alice and Bob share the elevator, then each of them calls it separately.
For simplicity, we assume that they also receive separate se rvice. For example, Alice calls it
to the ground ﬂoor 0 by pushing the input ∝a\}b∇ack⌉tl⌉{t0∝a\}b∇ack⌉t∇i}htA. When the elevator arrives, Alice observes
(/squiggleright0)A. To go to ﬂoor 1, Alice then enters ∝a\}b∇ack⌉tl⌉{t1∝a\}b∇ack⌉t∇i}htA. Then Bob may call the elevator down again
2This mixture of Moore and Mealy machine outputs can be reduce d to either paradigm using the usual translation
between the two.
73
x:=[x0<···<xk]delete x0,x2...
x ∝a\}b∇ack⌉tl⌉{t2∝a\}b∇ack⌉t∇i}ht,∝a\}b∇ack⌉tl⌉{t0∝a\}b∇ack⌉t∇i}ht,∝a\}b∇ack⌉tl⌉{t0∝a\}b∇ack⌉t∇i}ht,∝a\}b∇ack⌉tl⌉{t3∝a\}b∇ack⌉t∇i}ht,∝a\}b∇ack⌉tl⌉{t1∝a\}b∇ack⌉t∇i}ht... (/squiggleright2),(/squiggleright0),(⊙0),(/squiggleright3),(/squiggleright1)...
01n
input output
call stack
Figure 6.4: Elevator channel
by∝a\}b∇ack⌉tl⌉{t0∝a\}b∇ack⌉t∇i}htB. We do not distinguish the calls from inside the cabin from th e calls to the cabin. To
capture sharing, we now need to distinguish Alice’s actions from Bob’s actions, which doubles
the input and output types:
XS=/braceleftig
∝a\}b∇ack⌉tl⌉{ti∝a\}b∇ack⌉t∇i}htA|0≤i≤n/bracerightig
+/braceleftig
∝a\}b∇ack⌉tl⌉{ti∝a\}b∇ack⌉t∇i}htB|0≤i≤n/bracerightig
=X×S
YS=/braceleftig
(/squigglerighti)A,(⊙i)A|0≤i≤n/bracerightig
+/braceleftig
(/squigglerighti)B,(⊙i)B|0≤i≤n/bracerightig
=Y×S
For a subject S∈S
•(/squigglerighti)Smeans: "Subject observes that the elevator comes to ﬂoor 0", whereas
•(⊙i)Smeans: "Subject observes that elevator is already on ﬂoor i".
Channel interference in the elevator. When a subject calls the elevator, they observe
whether the elevator is already there or not. If Alice and Bob are the only users then each
of them will know where the other has left the elevator. If Ali ce arrives home and leaves the
∝a\}b∇ack⌉tl⌉{t0∝a\}b∇ack⌉t∇i}ht/(⊙0) 0∝a\}b∇ack⌉tl⌉{t1∝a\}b∇ack⌉t∇i}ht/(/squiggleright1)∝a\}b∇ack⌉tl⌉{t0∝a\}b∇ack⌉t∇i}ht/(/squiggleright0)∝a\}b∇ack⌉tl⌉{t1∝a\}b∇ack⌉t∇i}ht/(⊙1)n
1∝a\}b∇ack⌉tl⌉{tn∝a\}b∇ack⌉t∇i}ht/(⊙n)
Figure 6.5: The state machine view of the elevator
74
elevator on ﬂoor 1, Bob will know that she is home or not by call ing the elevator. E.g. if he en-
ters a call to ﬂoor 1, he may observe two di ﬀerent outputs, depending on the history of Alice’s
earlier action:
X+→ Y
∝a\}b∇ack⌉tl⌉{t0∝a\}b∇ack⌉t∇i}htA∝a\}b∇ack⌉tl⌉{t1∝a\}b∇ack⌉t∇i}htB∝mapsto→(/squiggleright1)B
∝a\}b∇ack⌉tl⌉{t1∝a\}b∇ack⌉t∇i}htA∝a\}b∇ack⌉tl⌉{t1∝a\}b∇ack⌉t∇i}htB∝mapsto→(⊙1)B
Alice’s use of the elevator thus interferes with Bob’s use.
6.3.2 Sharing
6.3.2.1 Shared world
In the preceding chapters, the world has been presented in te rms of histories x∈Σ∗as sequences
of events from a set partitioned into disjoint unions
Σ =/coproductdisplay
u∈SΣu=/coproductdisplay
i∈JΣi=/coproductdisplay
a∈AΣa
where the eventsΣuwere performed by a subject u, the eventsΣiwere performed on the object
i, and the eventsΣawere all occurrences of the action a, performed by all subjects on all objects.
In each case, the parts Σvstrictly localize the events at the subject, object, action, or level v. The
strictness meant that Alice’s local events ΣAare disjoint from Bob’s local events ΣB. But if the
clearances need to be taken into account, then the subjects a t the same clearance level should
be able to observe and enact the same events. And even if they d on’t have the same clearance
level, there are often events for which multiple subjects ar e cleared, and their general localities
are not disjoint.
General localities. The world is presented as histories x∈X∗, the sequences of events x∈X.
But some events happen here, other events there. Alice and Bo b may be able to observe what is
happening here, but not there. The localities “here” and “th ere” may be presented as network
nodes. The events from Xmay be localized on a network. To formalize this, we consider a
latticeLof clearance levels, viewed as general localities, like in S ec. 3.3.1. In general, they
are partially ordered, in the sense that a house is contained in a city, the city is contained in a
country, so the lattice Lthen contains the chain house ≤city≤country. But the order is partial
because, say, a blue house and a red house , are not contained in each other. Alice may, however,
have access to both houses, and the lattice Lmust contain the clearance level blue house∨red
house to assign it to Alice. If Carol has access to the whole city and Bob only to the blue house,
then their clearance levels are cℓ(B)<cℓ(A)<cℓ(C). To capture channel sharing, the subjects
fromSand the events from Xare assigned clearances by the maps
pℓ:X→L cℓ:S→L
75
Clearance types. Alice is cleared to enter an input xif its locality pℓ(x) is below Alice’s
clearance level cℓ(A). The clearance relation (∝)⊆X×Sis deﬁned
/parenleftig
x∝A/parenrightig
⇐⇒/parenleftig
pℓ(x)≤cℓ(A)/parenrightig
. (6.13)
The events and actions for which Alice is cleared are collect ed in
XA={x∈X| x∝A}. (6.14)
This is Alice’s clearance type . The family of clearance types {Xu|u∈S}is generally not a
partition ofX, since its elements may have nonempty intersections, and ma y not coverX. This
is in contrast with the partitions {Σu|u∈S}of general eventsΣin Sec. 4.2.1.
Worldviews. Alice’s worldview is the set X∗
Aof all histories from her clearance type XA. The
general histories x∈X∗are purged of events outside Alice’s clearance type along th egeneral
purge channel↾A:X∗→X∗
Adeﬁned
/parenleftbig /parenrightbig↾A=/parenleftbig /parenrightbig(x::u)↾A=(x↾A)::uifu∝A
(x↾A) otherwise(6.15)
The local history x↾Ais Alice’s view of a global history x.
Local states of the world. If Alice observes a local history xA∈X∗
A, then she knows that
any of the global histories x∈X∗such that x↾A=xAcould have taken place. Alice’s state of
the world is the set of global histories consistent with her l ocal view. Such derivations from her
local views induce Alice’s state of the world channel St =˜↾:X∗
A→℘X∗deﬁned
StxA(x)=1 if x↾A=xA
0 otherwise(6.16)
It is easy to see that this is the inverse (in the sense of Sec. 6 .2.2) of Alice’s worldview channel,
i.e., St=˜↾. Equivalently, using correspondence (6.3), Alice’s local state of the world can also
be written in the matrix form/bracketleftbig−⊢−/bracketrightbig:X∗
A×X∗→{0,1}with the entries
/bracketleftbigxA⊢x/bracketrightbig=StxA(x).
6.3.2.2 Shared channels
Setting for sharing. A shared channel must be given with a set of subjects S, a locality lattice
L, a locality assignment pℓ:X→L, and a clearance assignment cℓ:S→L, determining the
observability relation ( ∝) deﬁned in (6.13).
Local channel views. Alice’s local view of the channel g:X∗→℘Y∗is the derived channel
gA:X∗→℘Y∗which displays the outputs induced by Alice’s inputs, and pu rges all outputs
76
induced outside Alice’s clearance set XA:
gA()={()} gA(x::u)=gA(x)::g∗(x::u) if u∝A
gA(x) otherwise(6.17)
where g∗:X+→℘Ycorresponds to gby (6.7).
Lemma 6.2. Alice’s local view gAof any relational channel g :X∗→℘Y∗includes all channel
outputs of her worldviews x↾Afor every global history x:
g(x↾A)⊆gA(x) (6.18)
The converse inclusion is not satisﬁed by all relational cha nnels. In some cases, Alice may
learn from her local view gA(x) of a channel gmore than that channel outputs in response to
her local inputs as g(x↾A). That is a consequence of the interferences within some channels.
The channels where the converse inclusion of (6.18) is also v alid, so that gA(x)=g(x↾A), are
characterized in Prop. 6.4.
6.3.2.3 Interference channels
Alice can collect information about the interferences with in a channel g:X∗→℘Y∗by re-
peatedly entering the same input xA:X∗
Aand recording the di ﬀerent outputs that arise in the
diﬀerent global contexts within her local state. In this way, sh e builds a relational channel/integraltextAg:X∗
A→℘Y∗deﬁned as
/integraltextAg(xA)=/uniondisplay
u↾A=xAgA(u). (6.19)
If the channel gAis viewed, using (6.1–6.3), as the matrix/bracketleftbig−⊢ gA−/bracketrightbig:X∗×Y∗→{0,1}where/bracketleftbigx⊢gAy/bracketrightbig=/bracketleftbigy/bracketrightbig
gA(x)=1 if and only if y∈gA(x), else 0, then the interference channel/integraltextAg
becomes the matrix/bracketleftbig−⊢/integraltextAg−/bracketrightbig:X∗
A×Y∗→{0,1}deﬁned as
/bracketleftbigxA⊢/integraltextAgy/bracketrightbig=/logicalordisplay
x∈X∗/bracketleftbigxA⊢x/bracketrightbig·/bracketleftbigx⊢gAy/bracketrightbig.
6.4 Relational noninterference
6.4.1 Ideas of covert channels and interference
Theno-read-up andno-write-down requirements, that we studied in Chap. 3, assure that data
and objects can not ﬂow through a given channel downwards, i. e., that they cannot be either
pulled down by reading, or pushed down by writing.
77
But those requirements are imposed and can be tested only on t heexplicitly given , i.e., overt
channels. The ancient idea of the Trojan horse shows that an attacker can hide a covert channel
inside an overt channel, and transfer through it some prohib ited data or objects. The profession
of smugglers mainly consists of constructing covert channe ls inside overt channels. The story
Figure 6.6: Trojan horse covertly channels soldiers throug h overt gates
of the Trojan Horse is a legend of Odysseus’ brilliant smuggl ing idea. The city of Troy was
a walled fortiﬁcation, and the only overt channel into the ci ty was the city gate, manned by
the guards who made sure that no arms enter the city. Odysseus built a wooden horse, large
enough to conceal a group of armed soldiers and left it in fron t of the gate. To the citizens,
the wooden horse looked like a work of art. An overt channel fr om the gods. It was actually a
covert channel for armed soldiers, but it appeared to comply with the security requirement of
the city gate because they were covert.
Noninterference assures that overt channels do not contain covert channels. When a channel
g:X∗→/parenleftbig℘Y/parenrightbig∗is shared between Alice, Bob, Carol, and others, the overt ch annels are the
local views gA,gB,gC,... :X∗→/parenleftbig℘Y/parenrightbig∗deﬁned in (6.17). The security requirement is that
Alice’s local overt channel gAmust not provide any information about Bob’s channel gBunless
she has a clearance cℓ(A)≥cℓ(B) to see Bob’s private data. But how could gAever provide
information about gBif (6.17) purges Bob’s outputs from Alice’s view? We saw an in stance of
the answer (with the roles reverted) in Example 6.3.1. While Bob’s outputs are purged from
Alice’s local view, Alice’s outputs gA(x) do not depend only on her inputs, but on the entire
global histories x, which contain everyone’s inputs. So Alice can derive infor mation about
the global history x, including Bob’s inputs x↾B, from her local observations gA(x). Such
derivations are possible when there is channel interferenc e between Alice’s outputs and Bob’s
inputs.
How do we gain information about the input of a function from i ts outputs? For example, if I
know that the sum of two positive integers is x+y=4, then I know that xmust be 1, 2, or 3.
If an unknown function fover positive integers outputs the values f(x) and f(y), I may not be
78
able to ﬁnd what xandyare, but if f(x)/nequalf(y), then I do know that x/nequaly. This is 1 bit of
information about xandy. If I calculate the value of fat 0, and it turns out to be f(0)=f(x),
then I have one more bit of information about xandy. But if the function fis constant, and
f(x)=f(y) for all x,y∈A, then the outputs of fprovide no information about its inputs.
Suppose Alice observes a worldview xA. She wonders what is the actual state of the world
x, of which she only sees the restriction xA=x↾A. From the channel g:X∗→/parenleftbig℘Y/parenrightbig∗, she
also sees some outputs y,y′,...∈gA(x)⊆ Y∗. What does y∈gA(x) tell about x? Alice
can extract one bit of information about xif she can ﬁnd another global extension uof her
worldview x↾A=u↾Asuch that y/nelementgA(u), i.e., gA(x)/nequalgA(u). Another such extension
provides another bit of information and restricts xeven more. Starting from a local view xA
and testing the observable outputs gA(u) for unobservable global inputs usuch that u↾A=xA,
Alice gathers information about xthat caused her observation gA(x) by distinguishing it from
uthat cause diﬀerent eﬀectsgA(u), although they are for Alice locally indistinguishable, s ince
u↾A=x↾A=xA.
This information gathering process, where Alice keeps re-e ntering her local input xAto trigger
diﬀerent global inputs uwith u↾A=xAand record the possibly di ﬀerent channel outputs gA(u),
is modeled by Alice’s interference channel/integraltextAg. There is no interference or covert channels in
gif the interference channels/integraltextAgdo not gather any more information than the local channels
gAdisplay directly.
Deﬁnition 6.3. A channel g:X∗→/parenleftbig℘Y/parenrightbig∗satisﬁes the noninterference requirement if for all
subjects Athe interference channels output just the local views that t hey get overtly, i.e.
/integraltextAg(xA)=g(xA) (6.20)
A channel f:X+→℘Yis said to satisfy noninterference if f∗:X∗→/parenleftbig℘Y/parenrightbig∗satisﬁes (6.20).
6.4.2 Characterizing noninterference
The following proposition provides several equivalent cha racterizations of relational noninter-
ference. A useful equivalent condition is stated in terms of thecomplementary purge channel
↾¬A:X∗→Y∗
Adeﬁned
/parenleftbig /parenrightbig↾¬A=/parenleftbig /parenrightbig(x::u)↾¬A=(x↾¬A) ifu∝A
(x↾¬A)::uotherwise(6.21)
Comparing this deﬁnition with (6.15) shows that this complementary purge channel keeps in
↾¬Axprecisely those events from xwhich Alice purge operation eliminates from ↾Ax; and vice
versa,↾¬Apurges precisely those events which ↾Akeeps.
Proposition 6.4. For every shared channel g :X∗→℘Y∗and every subject A, the following
conditions are equivalent:
79
(a) g satisﬁes the noninterference requirement:
/integraltextAg(xA)=g(xA)
(b) for all x,y∈X∗
x↾A=y↾A=⇒gA(x)=gA(y)
(c) for all x∈X∗
gA(x)=g(x↾A)
(d) for all x,z∈X∗there is y∈X∗such that
x↾A=y↾A∧y↾¬A=z↾¬A∧gA(x)=gA(y)
Proof .(d)=⇒(c): Forz=/parenleftbig /parenrightbig, the second conjunct of (d)isy↾¬A=(), which by (6.21) and
(6.15) implies y↾A=y. The ﬁrst conjunct of (d)then gives x↾A=y↾A=y. Using this, the third
conjunct of (d)gives gA(x)=gA(y)=gA(x↾A). Hence (c).
(c)=⇒(b): Towards the implication in (b), suppose
x↾A=y↾A (•b)
Using (c), it follows that
gA(x)(c)=gA(x↾A)(•b)=gA(y↾A)(c)=gA(y) ( b•)
So we have proven the implication (•b(c)=⇒b•), which is (b).
(b)=⇒(d): For arbitrary x,z∈X∗, set
y=(x↾A)::(z↾¬A) (6.22)
Hence x↾A=y↾Aandy↾¬A=z¬A. But x↾A=y↾Aimplies gA(x)=gA(y) by (b). The three
conjuncts of (d)are thus satisﬁed by ydeﬁned as in (6.22).
(c)=⇒(a)holds because gA(u)=g(u↾A)implies
/integraltextAg(xA)(6.19)=/uniondisplay
u↾A=xAgA(u)(c)=/uniondisplay
u↾A=xAg(u↾A)=g(xA)
80
(a)=⇒(c):The inclusion one way follows from (a)by
g(x↾A)(a)=/integraltextAg(x↾A)(6.19)=/uniondisplay
u↾A=x↾AgA(u)⊇gA(x)
The inclusion g(x↾A)⊆gA(x) is valid by Lemma 6.2, independently on the interference. /boxempty
6.4.3 Noninterference as invariance
Deﬁnition 6.5. A continuous channel γ:℘X∗→℘Y∗is said to be invariant under the opera-
tion p:℘X∗→℘X∗ifγ◦p=γ, i.e., if the following diagram commutes
℘X∗
℘Y∗
℘X∗pγ
γ(6.23)
Aprojector is a continuous channel p:℘X∗→℘X∗that is invariant under itself, i.e., satisﬁes
p◦p=p.
Cumulative and single-output versions. Mapped along the bijections in (6.10) and (6.7),
diagram (6.23) becomes
X∗
℘Y∗
X∗qg
gX+
℘Y
X+rf
f(6.24)
Examples. Diagram (6.6) thus says that a channel f:X+→℘Yis memoryless if and only if
it is invariant under the operation r=/parenleftbigg
X+last։X(−)֌X+/parenrightbigg
. It is easy to check that r=r◦ris a
projector.
Another example is noninterference. Condition (c)from Prop. 6.4 can be summarized by piping
the outputs of the purges (6.15) from X∗
Ato all ofX∗and then bundling them into a projector:
/braceleftig
X∗↾A− →X∗
A֒→X∗/bracerightig
A∈S
X∗×S↾•− →X∗
↾=/parenleftbigg
X∗∝a\}b∇ack⌉tl⌉{tid,last∝a\}b∇ack⌉t∇i}ht−−−−−→X∗×Xid×sbjt−−−−−−→X∗×S↾•−−−−−→X∗/parenrightbigg
81
where↾•(x,A)=x↾A. Hence,
↾:X∗→ X∗(6.25)/parenleftbigx0x1...xn/parenrightbig∝mapsto→/parenleftbigx0x1...xn/parenrightbig↾sbjt(xn).
In words, the global history xis projected to the local view x↾Aof the subject A=sbjt(last(x))
who entered the last channel input last(x). It is easy to see that this is a projector, since
(x↾A)↾A=x↾Aand hence,
X∗
X∗
X∗↾↾
↾(6.26)
Proposition 6.6. A channel g :X∗→℘Y∗satisﬁes the noninterference requirement if it is
invariant under the purge:
X∗
℘Y∗
X∗↾g
g(6.27)
Proof . On one hand, deﬁnition (6.17) of the local view gAgives
g(x)=gΓx(x) (6.28)
whereΓ=/parenleftbigg
X+last−−→Xsbjt−−→S/parenrightbigg
tells who entered the last input into the channel. On the othe r
hand, Prop.6.4 (c)gives
gΓx(x)=g(x↾) (6.29)
It follows that gsatisﬁes the noninterference if and only if for all x
g(x)(6.28)= gΓx(x)(6.29)= g(x↾)
i.e., if and only if g=g◦↾, which is (6.27). /boxempty
It will turn out that all resource and channel security prope rties, that we studied in the previous
chapters, can be characterized in terms of invariants — as we ll as the data security properties
that we will study in the next chapters.
82
7 Communication channels, protocols,
and authentication
7.1 Communication channels . . . . . . . . . . . . . . . . . . . . . . . . . 83
7.1.1 What is communication? . . . . . . . . . . . . . . . . . . . . . . 83
7.1.2 Communicating vs sharing . . . . . . . . . . . . . . . . . . . . . 84
7.1.3 Example: the wren authentication . . . . . . . . . . . . . . . . . 85
7.1.4 Channeling what you know, have, are . . . . . . . . . . . . . . . 8 6
7.2 Protocols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
7.2.1 Composing networks from channels . . . . . . . . . . . . . . . . 8 9
7.2.2 Programming network computation . . . . . . . . . . . . . . . . 9 0
7.2.3 What is a protocol? . . . . . . . . . . . . . . . . . . . . . . . . . 91
7.3 Authentication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
7.3.1 Problem of communication . . . . . . . . . . . . . . . . . . . . . 92
7.3.2 Network security . . . . . . . . . . . . . . . . . . . . . . . . . . 93
7.3.3 Idea of authentication . . . . . . . . . . . . . . . . . . . . . . . 94
7.3.4 Example: the Needham-Schroeder Public Key (NSPK) pro tocol . 95
7.3.5 Protocol modeling and analysis . . . . . . . . . . . . . . . . . . 1 00
7.3.6 When is an authentication protocol secure? . . . . . . . . . . . . 100
7.4 Authenticity as a hyperproperty . . . . . . . . . . . . . . . . . . . . . . 102
7.4.1 Hyperproperties . . . . . . . . . . . . . . . . . . . . . . . . . . 102
7.4.2 Formalizing authenticity . . . . . . . . . . . . . . . . . . . . . . 1 03
7.5 Network computations are protocol runs . . . . . . . . . . . . . . . . . 105
7.1 Communication channels
7.1.1 What is communication?
Alice and Bob communicate by sending messages, exchanging o bjects, and displaying features.
Hence the three types of communication channels, transmitt ing the three types of security en-
tities: encoded data, physical things, and individual trai ts. Diﬀerent channels enable di ﬀerent
network computations. They are programmed as protocols. Th ey give rise to network security
problems and solutions. We need to understand communicatio n channels in order to be able to
understand network security problems and design protocols that solve them.
Any form of traﬃc can be viewed as communication. If we think of London and Par is as
subjects, then the air, land, and sea between them are commun ication channels. The air tra ﬃc,
the shipping across the English Channel, and the trains unde r it, are forms of communication.
83
The main feature of communication channels is that they conn ect a subject that enters the
inputs on one side with a subject that extracts the outputs on the other side. The two sides,
the input interface and the output interface are also the cha racteristics of functions. Channels
are a special kind of functions. That is how they were deﬁned i n Sec. 6.1: as functions that
take histories as the inputs. A possibilistic (relational) channel relates each input history to a
set of possible outputs, possibly empty. A probabilistic ch annel assigns a probability to each
input. A communication channel can be possibilistic, or pro babilistic, or even quantum.
What makes it into a communication channel is that the input and the output are under control
of communicating parties, usually Alice and Bob.
7.1.2 Communicating vs sharing
If Alice and Bob are merchants using the English Channel, the n they may interact in two ways:
•they may share the Channel:
–each of them uses both the inputs and the outputs: loads the Ch annel on one side
and unloads it on the other side, independently of the other m erchant; or
•they may communicate through the Channel:
–Alice enters the inputs on one side of the Channel, Bob extrac ts the outputs on the
other side of the Channel; and then Bob may enter the inputs fo r Alice into another
communication channel in the opposite direction.
The structural diﬀerence between sharing and communication is summarized in t he following
table:
shared channel communication channels
(XA+XB)+→℘YX+
A→℘YB
℘YA← X+
B
The displayed channels are the simplest possible: either of them is only shared by two subjects,
or only used for communication between two subjects. In prac tice, most channels combine
sharing and communication. Alice and Bob often talk at the sa me time, and share the ether
while trying to communicate. The practice of everyone talki ng at the same time is scaled up
to the extreme in packet-switching networks, where large cr owds of subjects enter their input
packets, the network as a shared channel mixes them all, and t hen unmixes them as a communi-
cation channel, to deliver each packet to the desired recipi ent. Modern networks, starting from
the Internet, are mostly just implementations of such chann els, hiding the network structure
from the users in a black box, and delivering the shared commu nication channel functionality.
In this section, we ignore sharing and focus on communicatio n.
84
7.1.3 Example: the wren authentication
The wren on the left in Fig. 7.1 receives its chicks’ chirps as messages saying “Feed me!”,
transmitted by the data channel carried by the sound. The wre n responds by delivering the
Figure 7.1: Wren’s chick-feeding protocol gets attacked by cuckoo
food on the channel carried by the physical space. Since the c hicks chirp together, the chirping
channel is shared on the input side, whereas the food channel is shared on the output side. The
competition for the food at the output provides an incentive for attacks on the chick feeding
protocol. A big one is shown in Fig. 7.1 on the right. The ﬁrst s tep is that a cuckoo lays an egg
in wrens’ nest. Since the wrens cannot tell apart their own ch icks from the cuckoo’s, they feed
them all. The second step of cuckoo’s attack is that the cucko o chick murders the wren chicks
by pushing them out of the nest. It keeps its adoptive wren par ents all for itself, and outgrows
them before they know.
Figure 7.2: Wren’s chick authentication
To defend against this attack, the Fairy-wrens evolved the a uthentication protocol [17], sketched
in Fig. 7.2. As they lay on eggs, the wrens keep chirping a ﬁxed but randomly chosen, and
therefore unique, pattern. The chicks learn it from inside t he eggs. This chirp is used as a
shibboleth1. When they hatch, the chicks chirp back their parents’ uniqu e chirping pattern. The
1If you don’t remember what is a shibboleth, a quick web search will do.
85
cuckoo chicks have so far not managed to evolve a capability t o learn chirping from inside the
egg. Without the shibboleth chirp, the cuckoo chicks do not g et fed and starve without killing
anyone. Authentication saves lives.
Figure 7.3: Authentication framework
Fig. 7.3 shows the wren chick communication channel within t he general authentication frame-
work. The wren parents on one hand and the wren chicks on the ot her are viewed as network
nodes (“places”). Their interactions through the data chan nel, transmitting chick’s shibboleth
chirps, cause state transitions at each node. Each of the par ents was ready to receive the mes-
sage (as marked by “?”). After a parent receives the chirp mes sage (denoted by “♪”), they know
(on the level of Logic) that the chick was successfully authe nticated.
7.1.4 Channeling what you know, have, are
Leaving the logic aside for the moment, Fig. 7.4 displays jus t the space and the time: this time
the network channel is at the bottom and the channel interact ion is above it. The network nodes
communicating through the channel are called Alice and Bob a gain, and the two possible inter-
actions through a data channel are pushing a message (on the l eft) and pulling an observation
(on the right). Alice pushes data to Bob by sending a message, whereas Bob pulls data from
Alice by observing an observation. But Alice’s message can o nly be transmitted if Bob is ready
to receive it. And Bob’s observation can only be made if Alice displays the data to be observed.
Alice as the sender initiates the message transmission, and Bob accepts it. Bob as the observer
initiates the observation of an event from Alice’s side, and Alice makes the observation pos-
86
Figure 7.4: Data transmissions: send /receive and observe /display
sible. In both cases, Fig. 7.4 denotes Bob’s state as recipie nt of the communication channel
output by the variable x. The channel interaction is modeled as substitution t/x, whereby the
transmitted data tgets stored in xand Bob’s state changes from xtot(just like wren’s state was
changed from the listening “?” to the transmitted chirp “ ♪”). Note that Alice knows the data t
both before she sends a copy and after she sent it.
Figure 7.5: Thing transmissions: give /have and have/take
Fig. 7.5 shows a channel transmitting things. The di ﬀerence is that Alice this time does not
have the thing tafter she transmits it to Bob. This is marked by the green ball oons denoting
her state after the channel interactions. This time the two p ossible interactions are that Alice
may give Bob the thing ton her own initiative, or that Bob may take it on his initiative. The
precondition for Bob’s taking tis that Alice had it; the postcondition of Alice’s giving tis that
Bob has it. Bob’s state changes like in the data channel, from xtot, but Alice’s state now
changes diﬀerently, from tto nothing.
In addition to the data channels and the thing channels, ther e are also trait channels, usually
biometric. On the input side, someone has a trait t. On the output side, someone compares that
trait with a trait pattern ? t, and if both match, conﬁrms the pattern as ! t. Table 7.6 displays all
three types of channels. The initiator actions are written i nbold . The classiﬁcation of channels
into the three types is tidy and simple in general but often su btle in concrete cases. Fig. 7.7
shows Alice asking Bob if he has a hammer, Bob observing a hamm er in the toolbox, Bob
observing Alice asking about the hammer, Bob taking a hammer from the toolbox. Fig. 7.8,
87
Figure 7.6: Channel types and the supported interactions
Figure 7.7: A “hammer” mentioned and observed. Alice observ ed. A hammer taken.
on the other hand, says that actions themselves may be observ ed, irrespective of the subjects
performing them. On the left in this ﬁgure, Bob observes the a ction of asking about the hammer.
This is diﬀerent from the diagram in Fig. 7.7 bottom left, where Bob obse rves Alice performing
88
Figure 7.8: Observing actions: sending vs observing
that action. Lastly, in Fig. 7.8 on the right, Bob observes th e action of observing.
7.2 Protocols
7.2.1 Composing networks from channels
Networks are built by composing channels. For example, give n a channelA+f− →℘Bfrom
Alice to Bob and a channel B+t− →℘Cfrom Bob to Carol, we compose a simple2network with
the node set{A,B,C}and the link set{f,t}. The simplest way to pipe the outputs of A+f− →℘B
as inputs intoB+t− →℘Cis to use (6.7) and (6.10) to present both channels as continu ous
A+→℘B
℘A∗→℘B∗B+→℘C
℘B∗→℘C∗
The network can now be viewed as a pair of composable function s:
℘A∗f− →℘B∗t− →℘C∗
Fig. 7.9 shows a concrete instance of such a composite. Bob ob serves that there is a hammer in
the toolbox and tells Carol.
Remark. Note that the subjects interacting in networks are not neces sarily people, or comput-
ers. E.g. in this case, Alice is a toolbox. Any actor playing a role in a network computation is
assigned a network node.
2A channelA+f− →℘Bon its own is also a network, with the node set {A,B}and the link set{f}. The pair of
channelsA+f− →℘BandC+h− →℘Dis another one, with 4 nodes and 2 links. But these networks ar e trivial
because the channels are isolated, and the transmissions ca nnot be composed.
89
Figure 7.9: Composing channels
7.2.2 Programming network computation
The central feature of network models is that every event is l ocalized at a node. A local event
is only observable by local subjects. There is no global obse rver who sees everything and there
are no events outside nodes.
In a computer, there is a global observer who sees everything because everything happens in
one place. You just look at that place and you are the global ob server. For instance, in a Turing
machine, all actions are performed by the head, reading and w riting on the tape. Nothing ever
happens anywhere else. You watch the head, and you are the glo bal observer.
In a network, every subject inhabits a network node and only o bserves the events at that node.
But there are also communication channels between the nodes . The local computations at the
nodes are coordinated through non-local communications th rough channels. A network is just
a speciﬁcation of nodes and links: the localities and the cha nnels. Network computation is
comprised of local computations coordinated through chann el communications:
network computation =computation+communication
Protocols prescribe the computations and the communications distrib uted over networks, just
like programs prescribe the computations centralized in co mputers:
protocol
program=network
computer
This indicates what protocols do. But how do they do it?
90
7.2.3 What is a protocol?
Protocols regulate network interactions, not just between computers, but also between people,
between people and computers, between anyone and anything t hat plays a role in a network.
Protocols prescribe the roles that network actors should ag ree to play.
Social life is a protocol suite. A supermarket is a network of products waiting to be sold,
shoppers coming to buy them, cashiers executing payment tra nsactions. Fig. 7.10 gives a high-
level view of the payment protocol there. You go with the shop ping cart to the cash register
Figure 7.10: Bird’s eye view of the shopping protocol
and present what you want to buy, say a rubber duck. The cashie r establishes the price. You
give the money and take the rubber duck. Every step of this abs tract protocol reﬁnes to a much
more complex protocol. The payment at the third step may be th e most involved. It is also a
multi-step protocol where every step reﬁnes to a protocol. I f the shopper pays by contacting a
bank, an entire suite of banking protocols is run. Widening t he perspective, layer upon layer of
social protocols come into sight, from regulating how we wai t in line before the cash register,
through traﬃc protocols we obey as we drive home, to household protocols a s we arrive home.
We follow protocols as instinctively as we follow grammars w hen we use languages. We enact
complex social interactions with the same ease with which we generate complex sentences.
Most of the time, we don’t even become aware of the underlying structures. And the references
that bind words into a sentence and messages into a protocol t urn out to be of the same kind.
— The referential structures underlying sentences and prot ocols are the syntactic patterns .
Protocols are the syntactic patterns of network interactions. But what is a syntactic pattern?
Our communication channels are coded using languages. The c rucial feature of a language,
whether it is a natural language like English, or a programmi ng language like Python, is that
it has a syntax and a semantics . Semantics conveys the meaning of words, syntax the form of
sentences. But the word meanings depend on the sentence cont exts, and the sentence structures
depend on the syntactic typing of words. Syntactically well -formed sentences are recognized
through type-checking and type-matching. The subject of a s entence should be of type Noun
Phrase, and it should be matched by a Verb Phrase.
In programming, we classify data into data types to make sure that the operations are correctly
applied: e.g., that we only ever try to add or multiply number s, and not people or tomatoes. If
you try to multiply tomatoes, a type-checking error is raised.
91
In natural language, a similar process is based on assigning thesyntactic types to words and
phrases. To interpret this sentence that you are reading now , your mind assigns each word a
type (e.g., the word “mind” is of type Noun) and it expects tha t, to make a sentence, the word of
type Noun is matched by a word of type Verb (e.g., the word “ass igns” is of type Verb). Hence,
the syntactically well-formed phrase “mind assigns”. But t he verb “assign” is transitive, so
your mind expects that the action of the verb “assigns” trans itions to another noun. The word
“type” is a Noun, and hence the phrase “mind assigns type”. A c ouple of further reﬁnements
required by the English syntax yield the sentence “your mind assigns a type”. By constraining
the candidate words in a sentence, syntax streamlines and si mpliﬁes the processes of generating
and understanding language. The underlying computations a re the syntactic processes .
Protocols work in a similar way. In a conversational protoco l extending Example 3 in Sec. 4.2.2,
the statements may be of type Question or of type Answer. If yo u interpret a statement as a
question, you expect it to be matched by an answer. In an authe ntication protocol, the messages
may be of type Challenge or of type Response. A challenge (e.g ., a wren challenges the chicks
by oﬀering the food) needs to be matched by a response (e.g., the ch icks’ respond by chirping).
Security protocols have been studied, designed, and analyz ed since the early days of network
computation. A variety of models and theories has been devel oped and used [9, 11, 19, 52,
to mention just a few]. Most are beyond our scope and besides o ur goals. We will just take a
quick look at a simple but central family: the challenge-res ponse authentication protocols.
7.3 Authentication
7.3.1 Problem of communication
The general problem of communication between Alice and Bob i s that Alice only sees her side
of a communication channel, whereas Bob only sees his side.
Communication as sharing meaning. If Alice says “I love you”, she would like to know
that Bob has heard and understood what she said. Bob, on the ot her side, would like to know
that she really said what he heard and that she really meant wh at she said. Alice and Bob
arecommunicating if they achieve a common interpretation of the messages between them.
If an eavesdropper Eve intercepts Alice’s message and chang es “I love you” to “I hate you”,
then the communication between Alice and Bob has been disrup ted. Eve can achieve the same
miscommunication e ﬀect if she prepares Bob to misinterpret Alice’s message by te lling him
misleading gossip.
The goal of communication from Alice to Bob is to transmit a me ssage and assure a common
meaning, accepted by the subject on both ends of the channel.
However, communication is a process. Channels transmit str ings, transmitting strings takes
time, and the interpretations evolve as the transmission pr ogresses. What Alice means and
how Bob interprets it is not ﬁxed and often remains ambiguous . The interpretations can be
92
narrowed down through feedback, e.g., Bob questioning the m eaning and Alice answering.
But the problem of interpretation applies to the questions a nd the answers. To clarify what they
mean, Alice and Bob may refer to other nodes on the network (e. g., a hammer as the meaning
of the word “hammer”, or the contexts where the word “hammer” occurs). But the evolution of
meaning never ends. Communication remains a network proces s.
7.3.2 Network security
Data, things, and traits that are communicated successfull y, in the sense that the meaning at the
source is transmitted to the target of communication, are sa id to be authentic .
Examples of data, things, and traits that need to be authenti cated include:
•a command claiming to be from the commander and not from pirat es;
•a passport claiming to be original and not forged;
•a voice claiming to be human and not artiﬁcial.
They are authentic if they are what they claim to be.
Authenticity vs integrity. Integrity is a property closely related to authenticity. It is some-
times viewed as the guarantee that the contents of a channel a re not only unchanged, but that
they were not even accessed. Integrity of data, things, or tr aits in that sense could be breached
even if they are unchanged but were accessed by the adversary . In other contexts, integrity is
identiﬁed with authenticity. Yet other times, authenticit y is narrowed to mean integrity of the
origination claims.
In network security, the good-stuﬀ-should-happen requirements are authenticity and integrity;
thebad-stuﬀ-should-not-happen requirements are secrecy and conﬁdentiality. In this chapt er
we study the good stu ﬀ. The bad stuﬀis covered in the next chapter.
Remark. In Sec. 2.3, we deﬁned authenticity and integrity as propert ies of data transmissions
alone. Here we widen the angle and view them as properties of n etwork transmissions, which
include data, things, or traits. This does not change but rat her reﬁnes the deﬁnition. Data have
carriers. The primordial data carrier was our voice. Then we wrote on clay tablets and paper.
Nowadays, data are carried by electronic signals. In any cas e, all channels have a physical
aspect. The other way around, every thing means something. R ecognizing a thing means
giving it a name and a meaning. You recognize a hammer by recog nizing the word “hammer”
as its name and its use for hammering as its meaning. A person’ s trait similarly carries a social
meaning. The distinctions between data, things, and traits , are convenient for classifying the
families of security tools and channels; but at a closer look , the demarcation lines between data,
things, and traits often shift and blur. The same security pr operties apply to all types of channel
contents.
Network security problem. If Bob interacts with Alice in one part of the network and with
93
Figure 7.11: Channel compositions lead to interference and attacks
Carol in another, then Alice and Carol cannot prevent Bob fro m implementing a covert channel
between them even if there are no overt or shared channels bet ween them. Suppose that Bob
is an operating system in a smart device, serving Alice as a ba nking application and Carol as
a game, as illustrated in Fig. 7.11. Then Carol may be able to i nitiate banking transactions
through a covert channel with Alice, although there is no ove rt channel between them.
This kind of security problem arises even in a single-channe l as a networkA+→℘Bon its
own. If Eve hijacks Bob’s role, she may steal Alice’s data or t hings. This happened in the wren
feeding protocol in Sec. 7.1.3, where the cuckoo as Eve stole food and killed the wren chicks.
If Eve hijacks Alice’s role, then she may impersonate Alice t o Bob. If Bob is a bank, Eve may
take control of Alice’s account.
Authentication protocols provide solutions for such problems. The wren protocol in Se c. 7.1.3
was an example of authentication. What is authentication?
7.3.3 Idea of authentication
Authentication is a proof that someone is who they claim to be . If a computer engaging in an
online chat claims to be human, proving that claim is authent ication. The Turing Test is an
authentication task. If a chick in wren’s nest claims to be a w ren chick, a proof of that claim
is an authentication. If an owner of a painting claims that th e painting was made by Picasso,
a proof of that claim is an authentication. After the claim is proven, the painting is said to be
anauthentic Picasso. If a visitor of an online banking website claims tha t they are Alice, then
Bob, the bank, asks that they authenticate themselves. If Al ice is supposed to prove who she is
by providing to Bob her credentials, then she may also need to authenticate Bob if a malicious
actor impersonating Bob could copy her credentials and impe rsonate her to the actual bank. So
Alice and Bob need to authenticate each other. In the next sec tion, we will see an example of a
mutual authentication protocol.
Logically speaking, authentication is an instance of hypothesis testing . The claims that a chick
is a wren, that a painting is a Picasso, that a visitor of an onl ine banking service is Alice
94
are hypotheses that require indirect veriﬁcation, because the direct evidence is unobservable:
the wren chick’s distinguishing properties are unobservab le for its parents, the act of Picasso
painting his painting is remote in time, the identity of a web site visitor is remote in space.
Just like scientists seek ways to indirectly verify hypothe ses about unobservable subatomic
particles, authenticators seek ways to verify hypotheses a bout unobservable network nodes.
7.3.4 Example: the Needham-Schroeder Public Key (NSPK) protoc ol
The NSPK protocol is a mutual authentication protocol based onconﬁdential channels , imple-
mented using public-key cryptography. It goes back to the ea rly days of computer security [44]
and is almost as simple as the wren authentication protocol f rom Sec. 7.1.3. Yet, analyzing
it took many years and even more publications. Its security f eatures are independent of the
cryptographic implementation of its conﬁdential channels3, so we sketch a high-level view of
that aspect.
7.3.4.1 NSPK channels
The NSPK protocol is run over conﬁdential channels realized through public-key cryptography.
Idea of conﬁdential channels. If Alice wants to send to Bob the conﬁdential data t, she
applies Bob’s encryption function {−} Band forms the message {t}B. The encryption scrambles
t, so that it cannot be recognized or extracted from {t}Bby anyone except Bob. The message
{t}Bcan therefore be sent to Bob through the channels of a public n etwork, while maintaining
the conﬁdentiality of t. You can think of {−} Bas a lockbox that anyone can close, but only Bob
can open. Alice puts tin the box{−} B, locks it, and sends {t}Bto Bob. Lots of posthandlers
Figure 7.12: A conﬁdential channel
may handle the closed box {t}Bon its way to Bob, but no one can learn anything about tuntil
Bob receives the box and opens it. That is the idea of a conﬁdential channel , displayed in
Fig. 7.12. Bob’s capability to open the box {t}Band extract tis presented by the pattern-
3Cryptographic protocols are usually analyzed assuming tha t the underlying cryptographic functions are per-
fectly secure. There are lots of cryptography textbooks tha t study what that means and how it is achieved.
95
matching operation{t}B/{x}Bwhich results in storing the data tinto the variable xthat Bob had
ready for the data that he may receive through the conﬁdentia l channel.
The next paragraph provides a closer view of the last paragra ph. You may skip it, but don’t
skip the protocol that follows.
Implementation of conﬁdential channels: public-key crypt osystems.⋆The conﬁdential
channel schema in Fig. 7.12 can be implemented using public- key cryptography. A public-key
cryptosystem is a triple ∝a\}b∇ack⌉tl⌉{tG,E,D∝a\}b∇ack⌉t∇i}htof functions4where
•Gis a random generator of public /private key pairs∝a\}b∇ack⌉tl⌉{tk,k∝a\}b∇ack⌉t∇i}ht,
•Ekis ak-indexed family of encryption functions, and
•Dkis an k-indexed family of decryption functions,
such that
(fun) all tsatisfy Dk/parenleftig
Ek(t)/parenrightig
=t, and
(sec) ifA/parenleftig
Ek(t)/parenrightig
=t, then A=Dk, i.e., any decryption algorithm depends on the private key k.
Cryptographers use more precise deﬁnitions, but this gives you an idea of what they say.
The idea of how cryptosystems are used is that Bob ﬁrst obtain s from Ga key pair∝a\}b∇ack⌉tl⌉{tk,k∝a\}b∇ack⌉t∇i}ht, of
which he announces kas his public key and keeps kas his private key. For a ﬁxed key pair, the
function pair∝a\}b∇ack⌉tl⌉{tEk,Dk∝a\}b∇ack⌉t∇i}htis called a cipher . An encrypted message Ek(t) is called the ciphertext
andtis the corresponding plaintext . When Alice wants to transmit in conﬁdence a plaintext t
to Bob who announced the public key k, she generates the ciphertext {t}B=Ek(t), and sends it
through a public channel (such as the Internet or a cellular n etwork). By the functional property
(fun), Bob can extract t=Dk(Ek(t)). By the security property ( sec), anyone who can extract
t=A/parenleftig
Ek(t)/parenrightig
using some algorithm Amust know Bob’s private key k.
High-level view of conﬁdential channels: pattern-matchin g.All that matters for the
NSPK protocol is that Bob is the only one who can extract tfrom{t}B, because this is the
capability used to identify him (like the chirps were used to identify baby wrens). Therefore,
we reduce the whole story of public-key cryptography to the pattern-matching operation
{t}B/{x}B⊢t/x (7.1)
which compares the patterns {−} Bof the terms on the left, and since they match, it stores the
content tof the ﬁrst pattern into the variable xof the second pattern. For a general pattern p
and an arbitrary term T, the pattern-matching operation T/p(x) is deﬁned by the reduction rule
p(t)/p(x)⊢t/x (7.2)
4More precisely, GandEare randomized algorithms.
96
In words, the operation T/p(x) substitutes tforxifT=p(t).
7.3.4.2 NSPK interactions
Fig. 7.13 shows two views of the NSPK protocol. On the top of ea ch of the pictures is the
Figure 7.13: The legendary Needham-Schroeder Public Key (N SPK) protocol
network on which the protocol runs: just two nodes, Alice and Bob, and two conﬁdential
channels between them, one in each direction. The diagram on the left displays the protocol
as a composition of three conﬁdential channel interactions , instances of Fig. 7.12. Each of the
interactions is a message send /receive pair along a conﬁdential channel. The diagram on the
right omits the senders’ state updates after the send action s and the receivers’ ready states before
their receive actions. This leaves on the right a protocol pi cture of the type that is usually found
on protocol designers’ whiteboards and in their papers, all owing you to follow the protocol
actions as a linear sequence. This view abstracts away the fa ct that receivers cannot receive
messages unless they are ready to receive them, and that send ers need to remember that they
sent a challenge to be able to receive and verify the response . But taking this into account
requires looking at state changes in two places at the same ti me, like in the diagram on the left.
We are primed to think in linear time and understand diagrams like the one on the right much
97
easier. It is often a good idea to draw both views, to avoid con fusion.
What happens in this protocol? Alice and Bob go through 4 stat e changes each. Initially, Alice
is in the state ( A,B,0), which means that she knows that she is Alice, that she want s to talk to
Bob, and that her counter is at 0. Bob is in the state ( B,X,x,0), which means that he knows
that he is Bob, that he is ready to talk to someone whose name he will store in the variable X,
their session identiﬁer into the variable x, and his counter is also at 0.
Alice begins a protocol session by performing the action νm, which stores a new value into the
variable mand changes Alice’s state ( A,B,0)∝mapsto→(A,B,m,1). The value in mis Alice’s nonce , a
value that she will not use but once . It will allow her to distinguish the messages related to thi s
protocol session from other messages that she may receive in the meantime.
Alice then begins the conversation by sending the message {A,m}B, which here means: “Hi,
this is Alice, my nonce is m. Please prove that you are Bob by decrypting this message.” —
This is Alice’s challenge to Bob , requesting that he decrypts from inside {−} B. She records that
she has sent it by changing the state ( A,B,m,1)∝mapsto→(A,B,m,y,2), remembering all her previous
values, but now ready to receive Bob’s response into y.
Bob, on the other hand, was ready to receive and decrypt messa ges in the form{X,x}Band store
the content into Xandx. Upon receiving and decrypting {A,m}B, he stores Ainto X,minto
x, and changes his state ( B,X,x,0)∝mapsto→(B,A,m,1). Then Bob also generates a nonce nbyνn,
changes his state ( B,A,m,1)∝mapsto→(B,A,m,n,2), and sends{m,n}A, meaning: “Hi, in response to
m, here is my nonce n. Please prove that you are Alice by decrypting this message. ” — This
message is Bob’s response to Alice (because he proves by sending mthat he is able to extract
data from{−} B) and moreover, it is also Bob’s challenge to Alice to decrypt from inside {−} A.
Alice was ready to receive messages in the form {m,y}A. Upon decrypting Bob’s {m,n}A, she
veriﬁes that the ﬁrst component is her nonce mand that her challenge has been responded to.
She then stores niny, and changes her state ( A,B,m,y,2)∝mapsto→(A,B,m,n,3). Alice’s remaining
task is to respond to Bob’s challenge. She sends {n}B. This is Alice’s response to Bob (be-
cause she proves by sending nthat she is able to extract data from {−} A) and she transitions
(A,B,m,n,3)∝mapsto→(A,B,m,n,4) to settle in her ﬁnal state.
Bob, in the meantime, is ready to receive a message in the form {n}B, which veriﬁes that his
challenge has been responded to. Upon receiving such a messa ge, Bob transitions ( B,A,m,n,3)∝mapsto→
(B,A,m,n,4) and settles in his ﬁnal state.
What has been achieved? Bob and Alice have not only responded to each other’s challen ges,
and authenticated each other; they have also responded to th e challenges m,nthat were bound
together in a single message. The protocol thus does not just provide t wo authentications: of
Bob by Alice and of Alice by Bob. It provides a mutual authentication, with both of them
requesting authentication and accepting to be authenticat ed, within the same session. Last but
not least, since the nonces mandnwere only ever sent under Alice’s or Bob’s public keys, they
can be used to generate a shared key that only Alice and Bob kno w. NSPK is therefore not only
amutual authentication protocol, but also a key distribution protocol.
98
7.3.4.3 Attack!!!
The problem with the NSPK protocol is that it does not require that Bob explicitly identiﬁes
himself, like Alice did with “Hi, this is Alice”, at the begin ning. The protocol binds Alice’s
authentication of Bob by the challenge and the response
c(m)
AB={A,m}B r(m)
AB={m}A (7.3)
and Bob’s authentication of Alice by the challenge and the re sponse
¢(n)
BA={n}A r(n)
BA={n}B (7.4)
Bob’s response r(m)
AB={m}Aand his challenge ¢(n)
BA={n}Aare fused into{m,n}Ain the second
message, which binds the Alice-Bob authentication and the B ob-Alice authentication into a
mutual authentication between Alice and Bob. However, the symmetr y is broken because Al-
ice’s challenge to Bob c(m)
AB={A,m}Bsays who is the challenger, whereas Bob’s challenge to
Alice ¢(n)
BA={n}Adoes not. Why is this a problem?
If Bob is dishonest, he may decrypt Alice’s challenge c(m)
AB={A,m}B, re-encrypt it by Carol’s
public key and send cAC={A,m}C— as Alice’s challenge to Carol . Since Carol as the re-
sponder in the NSPK protocol is not required to identify hers elf, Bob can forward her response
{m,n}Ato Alice as his own. The originator of the challenge nfor Alice is not mentioned,
and Alice will think that this is Bob, because the challenge nis bound with the response to
her challenge m, which she issued under Bob’s public key, and Bob must have de crypted it.
But Bob is dishonest, and he transformed the challenge for hi m into a challenge for Carol.
Alice will decrypt {m,n}Aand respond with {n}B, which Bob will decrypt, re-encrypt as {n}C
— and thus transform into a response to Carol. In the end, Caro l thinks she has a mutually
authenticated conﬁdential channel with Alice, Alice think s she has a mutually authenticated
conﬁdential channel with Bob, and Bob is sitting in the middl e, relaying the messages between
the two of them. If Carol is a bank where Alice has an account, t hen Bob can withdraw Alice’s
money.
The Needham-Schroeder Public-Key (NSPK) and Symmetric-Ke y (NSSK) protocols were pro-
posed in 1978 [44]. The above attack on NSPK was discovered 17 years later [38], by a
computer, in an automated analysis. In the meantime, the pro tocol was widely used to secure
shared computers and studied in 100s (if not 1000s) of excell ent publications, without anyone
noticing the problem. It is true that Needham and Schroeder n ever said that the protocol was
secure against a dishonest responder. But it is also true tha t neither they nor anyone else said
that it was not secure against a dishonest responder — until t he attack surfaced in [38], from
exhaustive search of protocol runs in a formal model.
The attack is easily prevented by making sure that Bob’s auth entication of Alice is not (7.4),
but symmetric to Alice’s authentication of Bob in (7.3), wit h
c(n)
BA={B,n}A r(n)
BA={n}B (7.5)
99
The second message {m,n}Anow becomes{m,B,n}A. The former was the fusion of r(m)
AB={m}
and ¢(n)
BA={n}A. The latter is the fusion of r(m)
AB={m}andc(n)
BA={B,n}A. Bob’s impersonation
of Alice now fails, because Carol’s second message is {m,C,n}A, and if Bob sends it through
to Alice, she will know that her challenge for Bob was respond ed to by Carol, and she will not
respond to complete the session. The attack has been preempt ed by adding Bob’s identity in
his challenge and the protocol now looks secure.
But if it took us so long to detect and eliminate this attack, w hat reasons do we have to believe
that there are no other attacks? How can we know whether a prot ocol is secure? What exactly
does it mean for a protocol to be secure?
7.3.5 Protocol modeling and analysis
No matter how communicative we are, each of us observes the wo rld from a diﬀerent standpoint
and their standpoint is the center of their world. Understan ding the diﬀerent worldviews from
diﬀerent standpoints at di ﬀerent network nodes is hard. Reasoning about the mazes and kn ots
of network links spanned by communication channels is even h arder. Each of us participates in
many protocols involving many timelines, looping and inter twining; yet we view protocol runs
as linear time intervals, progressing from a beginning to an end. I send a message, then you
receive the message, then you send a message and I receive it. In reality, only a half of these
events is directly observable for either of us. We imagine th e other half. Distributed processes
in general, and communications in particular, are hard to un derstand and easy to misunderstand.
Protocols are hard to secure. Yet they are everywhere and eve ry aspect of life depends on their
security. We need precise models, deﬁnitions, security pro ofs, and empiric testing to get better
models. We need Security Science (SecSci).
Towards the deﬁnition. An authentication protocol has at least two roles: someone a uthenti-
cates someone. We call the authenticator Alice and the subje ct she authenticates is Bob. Since
the authentication requires that Alice and Bob communicate , the protocol requires channels
both ways between them. The protocol interactions through t he channels should test the claim
(hypothesis) that the subject that Alice is communicating w ith is Bob. The basic test of such
claims is the challenge-response protocol schema .
7.3.6 When is an authentication protocol secure?
Deﬁnition 7.1. Achallenge-response protocol between Alice and Bob consists of
•communication channels A+→℘BandB+→℘A
•families of
–challenges c(m)
AB∈A+and
–responses r(m)
AB∈B+
100
indexed by numbers m∈N.
This authentication requirement from a challenge-response protocol is the implication
(A)=⇒ (B) (7.6)
where (A) and (B) are the following sequencers of events:
(A) Alice sends a challenge c(m)
ABand subsequently receives a response r(m)
AB,
(B) Bob receives c(m)
ABafter Alice sends it and sends r(m)
ABbefore she receives it.
Explanation. The intuitive intent of the authentication requirement (A) =⇒(B) is that
(A) whenever it seems to Alice that her challenge to Bob was re sponded to,
(B) then Bob really responded to Alice’s challenge.
In other words, the requirement is that, upon the completion of a protocol run, Alice’s state of
mind (A) should coincide with the state of the world (B). The protocol test is thus required
to convey the truth. Note that this is a requirement on the challenge function cABand the
response function rAB. In data-based authentication, their cryptographic prope rties assure that
the requirement is satisﬁed. In thing-based and trait-base d authentications, security is assured
by the tamper-resistance of authentication tokens and feat ures.
Examples. One example are the challenge-response functions (7.3). An other one is (7.4). But
this is a protocol where Bob authenticates Alice; i.e., the r oles in Def. 7.1 are swapped. It is
the simplest and the weakest form of challenge-response aut hentication, usually called the ping
authentication. If nis freshly generated by Bob, and no one can guess it, then he ca n only be
certain that Alice must have been active between the time whe n he sent c(n)
BA={n}Aand the time
when he received r(n)
BA={n}B. This he knows because only Alice could decrypt nfrom{n}A. He
cannot be sure, though, that Alice was the one who encrypted nby his public key, since anyone
could have done that if Alice sent nunencrypted; or Carol could have encrypted it if Alice sent
it encrypted by Carol’s public key. So all Bob knows from auth enticating Alice by (7.4) is that
she has been alive between his send-challenge and receive-r esponse actions. If he didn’t freshly
generate an unpredictable n, even that is not certain, since anyone could have replayed a known
nback to him. On the other hand, if Bob authenticates Alice by t he challenge-response pair
(7.5), then Alice knows that the challenge is from Bob, and if she is honest (meaning, if she
follows the protocol) then she will respond by r(n)
BA={n}B. Then Bob and Alice know that they
are participating the protocol, and Alice agrees to be authe nticated by Bob. This is a stronger
form of authentication, often called the agreement . Last but not least, composing (7.3) and (7.5)
by gluing r(m)
AB={m}Aandc(n)
BA={B,n}Ainto a single message {m,B,n}Awe get the NSPK like
in Fig. 7.13, but ﬁxed to be secure even if Bob is not honest [38 ]. To be sure that Bob and Alice
agree not only to be authenticated by each other, but that the y agree to a mutual authentication,
in a single session, it must be required that both of their sta tes of mind coincide with the state
of the world, and thus match . This is the strongest form of authentication, called the matching
101
conversation authentication in [22]. The general idea that the notions of authentication form a
hierarchy was discussed in [39].
Problem. The authenticity requirement is not a trace property in the s ense of Def. 4.1. The
reason is that Alice cannot see the global state of the world, but only her local worldview. More
precisely, she cannot see the global histories but only her l ocal projections. Taking into account
the possible non-local events that she does not see, her worl dviews are not histories, but sets of
possible histories. For example, when Alice observes the ev ents c,r∈ΣA, then her worldview
is the set of histories
c≺r=/braceleftbigx::c::y::r::z|x,y,z∈Σ∗
¬A/bracerightbig
whereΣ¬A=Σ\ΣA. The authenticity requirement is thus
•not a property P∈℘Σ∗
•but a hyper propertyP∈℘℘Σ∗.
7.4 Authenticity as a hyperproperty
7.4.1 Hyperproperties
A hyperproperty is a property of properties. While properti es (deﬁned in Sec. 4.1.1) are sets
of histories P∈℘Σ∗, hyperproperties are sets of sets of historiesP∈℘℘Σ∗. Their role in
channel and protocol security was recognized and studied by Clarkson and Schneider [16].
Examples. We have seen many hyperproperties in Chapters 4 and 5. While t he elevator safety
requirement SafElev ∈℘Σ∗deﬁned in (4.14) is a Σ-trace property, the set Safe∈℘℘Σ∗of all
Σ-trace safety properties is a hyperproperty. Safety is a pro perty of properties. The particular
elevator requirements studied in Ch. 4 are the properties Li vElev,AuthElev,AvailElev∈℘Σ∗,
but the kinds of requirements that we studied, i.e., livenes s, authority, and availability, are
properties of properties: Live,Auth,Avail∈℘℘Σ∗.
Programs (executed on computers) and processes (executed on state ma chines) generate his-
tories , recorded asΣ-traces or contexts. Their security was studied in Chapters 4–5 in terms of
trace properties .
Protocols (executed on networks) generate worldviews , recorded sets ofΣ-traces or contexts.
Their security will here be formalized in terms of trace hyperproperties .
102
7.4.2 Formalizing authenticity
Protocol events. To formalize security hyperproperties and align them with s ecurity proper-
ties from Ch. 4, we use the event space similar to Example 2 in C h. 4. Given the types
•J=/braceleftbigc(m),r(m),.../bracerightbigof protocol messages as objects;
•A=/braceleftbig∝a\}b∇ack⌉tl⌉{t−∝a\}b∇ack⌉t∇i}ht,(−)/bracerightbigof “sends” and “receives” as protocol actions;
•S=/braceleftbigA,B,C,.../bracerightbigas a type of protocol roles as subjects,
a protocol event is a set of triples representing subjects’ a ctions of sending and receiving ob-
jects:
Σ =J×A×S=/braceleftigg/angbracketleftig
p(m)/angbracketrightig
A,/parenleftig
p(m)/parenrightig
A/vextendsingle/vextendsingle/vextendsinglep∈J,A∈S,m∈N/bracerightigg
where an event/angbracketleftig
p(m)/angbracketrightig
Ais that “Alice sends the message p(m)∈J” whereas/parenleftig
p(m)/parenrightig
Ais “Alice
receives the message p(m)∈J”. The objects acted upon are thus the messages sent or receiv ed.
In practice, messages are usually the well-formed expressi ons of a language, suitably encoded.
We model them as terms of a sequent algebra5.
Term operations. While the challenge-response protocols are modeled using t he message
templates presented as families of terms c(m)
ABandr(n)
AB, parametrized by indeterminates mand
n, like we did in Sec. 7.3.4, in reality a message can only be sen t if all indeterminates in its
template are determined6. The message space Jis therefore the set of closed termsJ[] of
a term algebraJ. The full term algebra Jcontains variables x,y,z...∈ V , andJ[x,y]
denotes the subalgebra of terms with the free variables x,y. Any pair of values m,ninduces the
substitution operations m/xandn/y, which commute and can be performed concurrently:
J[y]
J[x,y] J[]
J[x]n/y m/x
n/ym/x,n/y
m/x
This means that the variables are independent on each other. The message space J=J[] is
5Sequent algebra has been studied from di ﬀerent angles in the work of Axel Thue, Andrey Markov, Emil Pos t,
Gerhard Gentzen since the early XX century. Noam Chomsky dev eloped them as formal grammars; computer
scientists as rewrite systems. See [49] for references and a general overview close to the current use.
6Careless applicants often forget to complete the indetermi nate parts of their cover letter templates, and submit
letters beginning with “Dear ”, leaving the space for recipient’s name indeterminate. The email mes-
sage format and the network packet header also have such inde terminate spaces for the recipient. They cannot
be sent empty, since they are used as addresses.
103
thus the subalgebra consisting of the terms where all free va riables have been instantiated to
values.
Where do the values come from? In the challenge-response pro tocols, there are two main
operations that generate and substitute values for variabl es:
•new value generation νm, which results in the substitution m/xof a fresh value mfor the
variable x, i.e.,
νx.p(x)⊢p(m/x) for a fresh m (7.7)
•pattern-matching p (m)/p(x), which results in the substitution m/xof the value mfor the
variable xwhenever p(m) is matched with p(x), i.e.
p(m)/p(x)⊢m/x (7.8)
For examples of these operations in action, see the NSPK prot ocol in Sec. 7.3.4.
Abbreviations. The examples that we saw, as well as most other protocol model s, suggest the
following natural assumptions:
•Whenever a message p(m) to be sent contains an indeterminate mnot in sender’s prior
state, then mmust be freshly generated before the send action, which is th us/angbracketleftig
νm.p(m)/angbracketrightig
.
•Whenever upon the receipt of a message p(m), a variable xtakes a value mthat was not
a part of recipient’s prior state, then the recipient must ha ve extracted m/xby pattern-
matching from the pattern p(x), so that their receive action must have been/parenleftig
p(m)/p(x)/parenrightig
.
These assumptions allow simplifying the notations by writi ng
•/angbracketleftig
p(m)/angbracketrightig
to abbreviate/angbracketleftig
νm.p(m)/angbracketrightig
, and
•/parenleftig
p(m)/parenrightig
to abbreviate/parenleftig
p(m)/p(x)/parenrightig
.
Deﬁnition 7.2. The set of authenticity hyperproperties of the challenge-r esponse protocols
between Alice and Bob form the set
ChRe AB=/braceleftigg
P∈℘℘Σ∗|∀P∈P∀ t∈P.
t=x::/angbracketleftig
c(m)
AB/angbracketrightig
A::y::/parenleftig
r(m)
AB/parenrightig
A::z=⇒y=u::/parenleftig
c(m)
AB/parenrightig
B::v::/angbracketleftig
r(m)
AB/angbracketrightig
B::w/bracerightigg
(7.9)
A challenge-response protocol is said to be secure if Alice’ s worldviews (the sets of possible
states of the world, consistent with her observations) are i nChRe AB.
104
Explanation. The security requirement in Def. 7.2 is the hyperproperty fo rmalization of the
security requirement in Def. 7.1. More precisely, the eleme nts of ChRe are just those hyper-
properties that satisfy (7.6), because
•t=x::/angbracketleftig
c(m)
AB/angbracketrightig
A::y::/parenleftig
r(m)
AB/parenrightig
A::zin (7.9) formalizes (A) in (7.6),
•y=u::/parenleftig
c(m)
AB/parenrightig
B::v::/angbracketleftig
r(m)
AB/angbracketrightig
B::win (7.9) formalizes (B) in (7.6).
The implication (A) =⇒(B) thus means that whenever Alice observes
x::/angbracketleftig
c(m)
AB/angbracketrightig
A::y::/parenleftig
r(m)
AB/parenrightig
A::z,
the actual state of the world is
x::/angbracketleftig
c(m)
AB/angbracketrightig
A::y/bracehtipdownleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext /bracehtipupright/bracehtipupleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext /bracehtipdownright
u::/parenleftig
c(m)
AB/parenrightig
B::v::/angbracketleftig
r(m)
AB/angbracketrightig
B::w::/parenleftig
r(m)
AB/parenrightig
A::z.
7.5 Network computations are protocol runs
Syntactic processes streamline communications. Many sema ntically diﬀerent sentences share
the same syntactic pattern. An algorithm is a syntactic patt ern underlying a family of com-
putations. A single algorithm can be instantiated to many ap plications, implemented by many
programs and executed on data from many sources. Di ﬀerent computations instantiate the same
algorithm just like di ﬀerent sentences instantiate the same grammatical structur e.
A protocol is an algorithm for network computations. Each protocol role is assigned a net-
work node7. Each protocol interaction is supported by a network channe l. While a centralized
computation is implemented by a program executed on a centra lized computer8, a network
computation is implemented by a troupe of communicating pro grams, usually cast over a net-
work of computers, interacting as prescribed by the protoco l. A protocol run consists of the
runs of the protocol programs, each at their own node, coordi nating with each other through
communications prescribed by the protocol.
Just like an algorithm is implemented by programs and instan tiated to computations, a dis-
tributed algorithm is implemented by protocols and instant iated to protocol runs. An example
of a protocol run from the animal world is shown in Fig. 7.14 sh owing a ﬂock of starlings. The
control of each individual ﬂight path is distributed to the i ndividual starlings, and their com-
munication and interaction protocols establish a protocol run that enables the intricate ﬂight
7Sometimes the same actor plays several roles, which require s controlling several network nodes. Centralized
control of multiple localities requires another network, w ith a hub and the channels to the multiple nodes. This
lifts to network computation the kind of tricks played when t he same actor plays several roles in a movie or in
a theater production.
8As explained in Sec. 2.5, computation is centralized when th ere is a global observer, who sees all computational
steps. In standard models of computation, this just means th at all computational steps are executed in one
place, e.g., by the writing and /or reading head of a state machine, by the rule set of a grammar , etc.
105
Figure 7.14: Starling murmurations are protocol runs
patterns of the ﬂock.
106
8 Information channels and secrecy
8.1 Channeling information and uncertainty . . . . . . . . . . . . . . . . . 107
8.1.1 Predictions are channels . . . . . . . . . . . . . . . . . . . . . . 10 7
8.1.2 Probabilistic channel types and structure . . . . . . . . . . . . . 109
8.1.3 Quantifying information . . . . . . . . . . . . . . . . . . . . . . 11 1
8.2 Predictive inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 14
8.2.1 Example: language models . . . . . . . . . . . . . . . . . . . . . 114
8.2.2 Generative channels and sources . . . . . . . . . . . . . . . . . . 115
8.3 Inverse channels and Bayesian inference . . . . . . . . . . . . . . . . . 117
8.4 Probabilistic noninterference . . . . . . . . . . . . . . . . . . . . . . . 119
8.4.1 Sharing information . . . . . . . . . . . . . . . . . . . . . . . . 119
8.4.2 Probabilistic worldviews . . . . . . . . . . . . . . . . . . . . . . 1 19
8.4.3 Probabilistic interference channels . . . . . . . . . . . . . . . . . 120
8.4.4 Deﬁnition and characterization of probabilistic non interference . 120
8.4.5 Example: Car rental . . . . . . . . . . . . . . . . . . . . . . . . 121
8.5 Secrecy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
8.5.1 Perfect secrecy . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
8.5.2 Computational secrecy . . . . . . . . . . . . . . . . . . . . . . . 125
8.1 Channeling information and uncertainty
8.1.1 Predictions are channels
Predictions. Life persists by resisting environmental changes. The envi ronmental changes
can be resisted when they are predictable. The predictions a re based on the assumption that the
future is like the past: if heavy rains caused ﬂoods last year and landslides a couple of years
before, then the heavy rain today may cause a ﬂood or a landsli de tomorrow. We remember the
past to predict the future from the present. If there were twi ce as many ﬂoods as landslides in the
past, then the chance of ﬂoods in the future is twice greater t han the chance of landslides in the
future. Probabilistic channels track such chances. If in th e past the word “you” occurred four
times as frequently after the phrase “I love” than the word “w atermelon”, then its probability
is assumed to be four times higher in the future. That is why la nguage and other information
channels are probabilistic.
From possibility to probability. Predictions are channels from the input type X=Causes to
the output typeY=Eﬀects. The possible future eﬀects of the present causes are captured by
107
thepossibilistic channels
Causes+f− →℘(Eﬀects). (8.1)
The output fx⊆Eﬀects is the set of possible e ﬀects of the causal context x∈Causes+at
the channel input. In Sec. 6.1.3(6.3), it was convenient to p resent the possibilistic channel
f: Causes+→℘(Eﬀects) in the form/bracketleftbig−⊢ f−/bracketrightbig: Causes+×Eﬀects→{0,1}, i.e., as relational
matrix whose entries are the sequents
/bracketleftbigx⊢fy/bracketrightbig=1 if y∈fx,
0 otherwise.(8.2)
A more informative view of the probable future eﬀects of the present causes is captured by the
probabilistic channels:
Causes+Pr− →∆(Eﬀects) (8.3)
where∆Y={µ:Y→ [0,1]|/summationtext
y∈Yµ(y)=1}is the set of probability distributions over Y. The
probabilistic sequents induced by such probabilistic chan nels are the conditional probabilities
/bracketleftbigx⊢Pry/bracketrightbig=Prx(y). (8.4)
Arranged together, they form stochastic matrices/bracketleftbig− ⊢ Pr−/bracketrightbig: Causes+×Eﬀects→[0,1]. A
matrix M:X×Y→ [0,1] is called stochastic when/summationtext
y∈YMxy=1 for all x∈X. The entries
deﬁned in (8.4) induce the bijective correspondence of prob abilistic channels and stochastic
matrices
Pr: Causes+→∆(Eﬀects)
=====================================/bracketleftbig−⊢ Pr−/bracketrightbig: Causes+×Eﬀects→[0,1](8.5)
When no confusion is likely, we omit the subscript Pr from/bracketleftbigx⊢Pry/bracketrightbigand write the conditional
probability of the e ﬀectyin the causal context xsimply as/bracketleftbigx⊢y/bracketrightbig.
Uncertainty. A prediction is uncertain when a causal context xallows multiple possible or
probable eﬀectsy. This means that there are events y0,y1,..., ynsuch that the sequents/bracketleftbigx⊢yi/bracketrightbig
are true (i.e., equal 1) for all i=0,1,...nin the possibilistic case, or they are all greater than 0
in the probabilistic case. Each of them may happen, and it is u ncertain which one will actually
happen.
Sampling. The operation of sampling a channel consists of entering an input xand observing
which of eﬀectsyioutput, for i=0,1,...n. Repeated sampling may produce di ﬀerent outputs.
Recording all possibilities, we deﬁne the possibilistic ch annel. Seeing a possible e ﬀect mul-
tiple times makes no di ﬀerence, since the repeated occurrences of the e ﬀect are not counted.
Once all possibilities have been observed, a possibilistic channel yields no new information. A
probabilistic channel is deﬁned by counting the number of ti mes each of the diﬀerent eﬀects of
the same cause occurs and by then calculating their frequenc ies. Suppose we sample a channel
108
and collect a multiset Dof input-output pairs ∝a\}b∇ack⌉tl⌉{tx,y∝a\}b∇ack⌉t∇i}ht1. The multiset Dis partitioned into the
disjoint unions
D=/coproductdisplay
x∈X+xD xD=/coproductdisplay
y∈YxDy
where xDis the multiset input-output pairs where the input componen t isx, whereas xDyis the
multiset of the occurrences of the input-output pair ∝a\}b∇ack⌉tl⌉{tx,y∝a\}b∇ack⌉t∇i}ht. The probability that a context xwill
cause an eﬀectycan then be approximated by the frequency with which xwas followed by y
inD
/bracketleftbigx⊢y/bracketrightbig=#xDy
#xD(8.6)
where # Sdenotes the number of elements of set S. The probability that the channel will output
y∈Y in any context can then be approximated by sum of frequencies :
/bracketleftbigy/bracketrightbig=/summationtext
x∈X+/bracketleftbigx⊢y/bracketrightbig
/summationtext
x∈X+
v∈Y/bracketleftbigx⊢v/bracketrightbig=/summationtext
x∈X+#xDy
#D. (8.7)
8.1.2 Probabilistic channel types and structure
8.1.2.1 Cumulative probabilistic channels
To capture the dynamics of information transmission, the cu mulative view of a probabilistic
channel is deﬁned along the lines of Sec. 6.1.4.1. Correspon dence (6.7) now lifts to
/braceleftig
X+→∆Y/bracerightig /braceleftig
X∗→∆Y∗/bracerightig
(8.8)
The cumulative sequents are still derived from the single-o utput sequents by formula (6.8)
/bracketleftbigx0...xn⊢y0...yn/bracketrightbig=/bracketleftbigx0⊢y0/bracketrightbig·/bracketleftbigx0x1⊢y1/bracketrightbig·/bracketleftbigx0x1x2⊢y2/bracketrightbig···/bracketleftbigx0...xn⊢yn/bracketrightbig
This time, however, the sequents are probabilities and not m ere relations, which means that
they are evaluated in the interval [0 ,1], and not merely in the set {0,1}. The meaning of this
probabilistic view of formula (6.8) is discussed in Sec. 8.2 .2. To go in (8.8) the other way
around, from cumulative sequents to single-output sequent s, just project away all accumulated
outputs except the last one.
1It is not a set but a multiset because we want to count how many t imes each input-output pair occurs, and retain
the multiple occurrences of ∝a\}b∇ack⌉tl⌉{tx,y∝a\}b∇ack⌉t∇i}htas diﬀerent elements of D, distinguished by suitable counters or parameters.
109
8.1.2.2 Continuous probabilistic channels
A continuous possibilistic channel, deﬁned in Sec. 6.1.4.2 , was a functionγ:℘X∗∪− →℘Y∗, pre-
serving unions and ﬁniteness. A continuous probabilistic channel is a function ϕ:∆X∗Σ− →∆Y∗,
preserving the convex combinations and the ﬁnitely support ed distributions. The continuation
and the restriction
/braceleftig
X∗→∆Y∗/bracerightig /braceleftig
∆X∗Σ− →∆Y∗/bracerightig(−)
/simequal
(−)(8.9)
form a bijection again, deﬁned
Prµ(y)=/summationdisplay
x∈X∗µxPrx(y) and Prx(y)=Pr/hatwidex(y)
for the inputsµ∈∆X∗andx∈X∗, with/hatwidex∈∆X∗denoting the point distribution, putting all
weight on the point x:
/hatwidexu=1 if x=u
0 otherwise.
The stochastic matrix/bracketleftbig−⊢−/bracketrightbig:X∗×Y∗→[0,1] corresponding to the cumulative probabilistic
channel Pr:X∗→∆Y∗now extends to the matrix/bracketleftbig−⊢−/bracketrightbig:∆X∗×∆Y∗→[0,1] of sequents
/bracketleftbigµ⊢ϕν/bracketrightbig=/summationdisplay
x∈X∗
y∈Y∗µx/bracketleftbigx⊢y/bracketrightbigνy
induced by the continuous channel Pr:∆X∗Σ− →∆Y∗.
8.1.2.3 Example: ﬂipping a coin
If a coin is viewed as a probabilistic channel, then sampling the channel means ﬂipping the coin.
The channel inputs are the coin ﬂips. Assuming that the coin c annot be manipulated, there is
just one way to ﬂip it, which means that the input type X=Causes has a single element, denoted
. A context x∈Causes+is therefore just a number of coin ﬂips x=/parenleftbig
···
/parenrightbig. Flipping
the coin Dtimes produces a sample y=/parenleftbigy1y2...yD/parenrightbig, where each yi∈{H,T}says whether
Heads or Tails came up in the i-th ﬂip. Viewed as a possibilistic channel f:{
}+→℘{H,T},
the coin supports all cumulative sequents with equal number of inputs and outputs:
/bracketleftbigD/bracehtipdownleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehtipupright/bracehtipupleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehtipdownright
···
⊢fy1y2...yD/bracketrightbig=1
110
If the coin is biased, one side comes up less often than the oth er; yet as long as both are possible,
all we know is that all possibilistic sequents are true.
To determine whether the coin is biased (and unfair) or unbia sed (and fair), the coin must be
viewed as a probabilistic channel Pr: {
 q}+→∆{H,T}, where qnow denotes the bias. It
is assumed that ﬂipping does not change the coin and that all ﬂ ips obey the same probability
distribution:/bracketleftbig
q⊢PrH/bracketrightbig=q/bracketleftbig
q⊢PrT/bracketrightbig=1−q
The coin is said to be unbiased if/bracketleftbig
q⊢PrH/bracketrightbig=/bracketleftbig
q⊢PrT/bracketrightbig, which means that q=1
2. Otherwise,
the coin is said to be biased . The assumption that ﬂipping the coin does not change it also means
that the past outcomes do not impact the present and the futur e outcomes, which implies
/bracketleftbigD/bracehtipdownleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehtipupright/bracehtipupleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehtipdownright
q
 q···
 q⊢y1y2...yD/bracketrightbig=/bracketleftbig
q⊢y1/bracketrightbig·/bracketleftbig
q⊢y2/bracketrightbig···/bracketleftbig
q⊢yD/bracketrightbig=qh·(1−q)D−h
where his the number of Heads among the outcomes y1,y2...yD. If the coin is unbiased, i.e.,
q=1−q=1
2, then
/bracketleftbigD/bracehtipdownleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehtipupright/bracehtipupleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehtipdownright
1
2
1
2···
 1
2⊢y/bracketrightbig=/parenleftigg1
2/parenrightiggD
(8.10)
for all y∈{H,T}D. On the other hand, if the coin is biased, say q=3
4and 1−q=1
4, then
/bracketleftbigD/bracehtipdownleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehtipupright/bracehtipupleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehtipdownright
3
4
3
4···
 3
4⊢y/bracketrightbig=3h
4D(8.11)
where his again the number of Heads in y. In particular
/bracketleftbigD/bracehtipdownleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehtipupright/bracehtipupleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehtipdownright
3
4
3
4···
 3
4⊢D/bracehtipdownleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehtipupright/bracehtipupleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehtipdownright
HH···H/bracketrightbig=/parenleftigg3
4/parenrightiggD/bracketleftbigD/bracehtipdownleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehtipupright/bracehtipupleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehtipdownright
3
4
3
4···
 3
4⊢D/bracehtipdownleft/bracehext/bracehext/bracehext/bracehext/bracehtipupright/bracehtipupleft/bracehext/bracehext/bracehext/bracehext/bracehtipdownright
TT···T/bracketrightbig=/parenleftigg1
4/parenrightiggD
.
The bias of the coin can be established with arbitrary conﬁde nce if suﬃciently large sample
setsDare available.
8.1.3 Quantifying information
Before the coin is ﬂipped, it is uncertain whether the Heads o r the Tails will come up. After
the coin is ﬂipped and the outcome is observed, the uncertain ty is eliminated. Information is
the ﬂip side of uncertainty: the more information, the less u ncertainty. The other way around,
probabilistic channels transmit information by decreasin g the uncertainty . That is why proba-
bilistic channels are the information channels . They were the starting point of Shannon’s theory
of information [5, 18, 55].
Increase of information is decrease of uncertainty. Sampling a channel decreases the
111
uncertainty and increases the information. The amount of in formation transmitted through a
channel can be measured by establishing how much uncertaint y has been eliminated. Flipping
a biased coin eliminates less uncertainty than ﬂipping an un biased coin. If the biased coin with
/bracketleftbig
3
4⊢H/bracketrightbig=3
4/bracketleftbig
3
4⊢T/bracketrightbig=1
4
is ﬂipped in a game, betting on Heads is a winning strategy in t he long run. If an unbiased coin
is ﬂipped, there is no winning strategy and the outcome of the game is completely uncertain.
Therefore, ﬂipping a biased coin eliminates less uncertain ty and furnishes less information than
ﬂipping an unbiased coin. Flipping a totally biased coin, wi th indistinguishable Heads on both
sides, furnishes no information at all.
Entropy. The uncertainty of a channel, and the information that it tra nsmits, can be measured
as the average length of the bitstrings needed to describe th e probabilities of its outputs. For
example, for the unbiased coin, the probabilities are
/bracketleftbig
1
2⊢H/bracketrightbig=/bracketleftbig
1
2⊢T/bracketrightbig=1
2=(.1)2
where (.1)2is written in binary. So writing each of the probabilities re quires precisely one
binary digit, and the lengths are
ℓ/bracketleftbig
1
2⊢H/bracketrightbig=ℓ/bracketleftbig
1
2⊢H/bracketrightbig=1
Their average length is calculated by weighing the length of each probability by the probability
itself, and the amount of information obtained (i.e., the am ount of uncertainty eliminated) by
ﬂipping the unbiased coin is
H(
 1
2)=/bracketleftbig
1
2⊢H/bracketrightbig·ℓ/bracketleftbig
1
2⊢H/bracketrightbig+/bracketleftbig
1
2⊢T/bracketrightbig·ℓ/bracketleftbig
1
2⊢T/bracketrightbig=1
2·1+1
2·1=1
In words, this says that ﬂipping a coin gives 1 bit of informat ion. For 3 ﬂips, (8.10) gives the
probability of each outcome y∈{H,T}3is
/bracketleftbig
1
2
1
2
1
2⊢y/bracketrightbig=1
8=(.001) 2
Writing the probability1
8now requires 3 binary digits. The lengths of the probabiliti es that
need to be averaged are this time
ℓ/bracketleftbig
1
2
1
2
1
2⊢y/bracketrightbig=3
Averaging is easy again, since the lengths of the probabilit ies are weighed by the probabilities,
which are the same again, just smaller:1
8rather than1
2. The amount of information obtained
112
(i.e., the amount of uncertainty eliminated) by ﬂipping the unbiased coin 3 times is
H(
 1
2
1
2
1
2)=/summationdisplay
y∈{H,T}3/bracketleftbig
1
2
1
2
1
2⊢y/bracketrightbig·ℓ/bracketleftbig
1
2
1
2
1
2⊢y/bracketrightbig=8·1
8·3=3
Flipping a coin 3 times gives 3 bits of information. It sounds trivial all right, but the underlying
general idea is not. The idea is that the amount of informatio n (i.e., the decrease of uncertainty)
about the causes xconveyed by the eﬀectsytransmitted by a channel/bracketleftbigx⊢y/bracketrightbigis
H(x)=/summationdisplay
y/bracketleftbigx⊢y/bracketrightbigℓ/bracketleftbigx⊢y/bracketrightbig
But counting the expected lengths ℓ/bracketleftbigx⊢y/bracketrightbigof the binary representations of the probability of
every output yis impractical. To simplify it, ﬁrst note that the length of t he binary notation for
an integer r>1 is the length of the smallest power of 2 above it, i.e., ℓ(r)=⌈log2r⌉. If we go
beyond the integers and also allow fractional lengths, we ca n ignore the ceiling operation ⌈−⌉,
and writeℓ(r)=log2r. Since the logarithms of numbers p∈(0,1] are negative, the fractional
lengths becomeℓ(p)=−log2p, the general formula for quantifying the information about x
transmitted by the channel outputs yis thus
H(x)=−/summationdisplay
y/bracketleftbigx⊢y/bracketrightbiglog2/bracketleftbigx⊢y/bracketrightbig(8.12)
This is the most used and studied information measure: Shann on’s entropy [55]. Applied to
ﬂipping the biased coin
/bracketleftbig
3
4⊢H/bracketrightbig=3
4=(.11) 2/bracketleftbig
3
4⊢T/bracketrightbig=1
4=(.01) 2
formula (8.12) tells how many bits of information it furnish es:
H(
 3
4)=−/parenleftig/bracketleftbig
3
4⊢H/bracketrightbig·log/bracketleftbig
3
4⊢H/bracketrightbig+/bracketleftbig
3
4⊢T/bracketrightbig·log/bracketleftbig
3
4⊢T/bracketrightbig/parenrightig
=
−/parenleftig3
4·log3
4+1
4·log1
4/parenrightig
=2−3
4log23
Since log23>3
2, ﬂipping this biased coin yields less than 1 bit of informati on. Flipping
 1
2
gives3
4log23−1 more information than ﬂipping
 3
4. Information theory is a symphony of such
measurements of information ﬂows. The theories of communic ation, language, intelligence,
data security, all depend on such measurements. As a quantit ative model of information trans-
mission, probabilistic channels play a central role in all s ciences of communication [50, 55]. A
rich theory of information ﬂow security is built almost enti rely in terms of channels as stochas-
tic matrices [3]. In cryptography, simple memoryless chann els are composed into complex
secure constructs used in secure function evaluation and mu lti-party computation [25].
Examples: erasure channel, oblivious transfer. A memoryless channel that transmits an
input bit bwith probability q, and erases it otherwise, can be deﬁned by the stochastic mat rix
113
/bracketleftbig−⊢−/bracketrightbig:{0,1}×{0,1,e}→[0,1] comprised of the sequents
/bracketleftbigb⊢b/bracketrightbig=q/bracketleftbigb⊢e/bracketrightbig=1−q
This erasure channel is displayed in Fig. 8.1 on the left. On the right is th eoblivious transfer
Figure 8.1: The Erasure Channel and the Oblivious Transfer
channel, which transmits one of two bits from Alice to Bob, ke eping Alice oblivious which of
the two bits was transmitted. The stochastic matrix/bracketleftbig−⊢−/bracketrightbig:{0,1}2×{0,1}→[0,1] and the
sequents are/bracketleftbigb0b1⊢b0/bracketrightbig=q/bracketleftbigb0b1⊢b1/bracketrightbig=1−q
8.2 Predictive inference
8.2.1 Example: language models
As I speak, the context xof words that I have said induces a probability distribution over the
next word ythat I will say. These conditional probabilities are the values of the sequents/bracketleftbigx⊢y/bracketrightbigdescribing the channel that I communicate on. We speak (and w rite) by sampling that
distribution. The conditional probability that I sample as I speak may be something like
/bracketleftbigwhat is the next word that I am going to ⊢write/bracketrightbig=1
8/bracketleftbigwhat is the next word that I am going to ⊢say/bracketrightbig=1
2/bracketleftbigwhat is the next word that I am going to ⊢swallow/bracketrightbig=1
30
In the usual notation for conditional probabilities, the ﬁr st line would be written
Pr/parenleftbigwrite|what is the next word that I am going to/parenrightbig=1
8
and the other two similarly. Here we don’t write them like tha t, but as sequents. Changing
standard notations is seldom a good idea, but they are seldom this bad.
Speech is a probabilistic channel. As you try to understand what I am trying to say, you also
sample from a similar distribution and try to predict what I w ill say. If what I say is predictable,
it is easier to understand. If I mumble, predictability enab les error correction. When I deviate
114
from your predictions, you may receive new information. The uncertainty increases with the
unpredictability, and removing it yields more information .
We speak the same language if we sample words from the same dis tribution. That distribution
produces the language that we speak and comprehend. In reali ty, everyone’s distribution is
slightly diﬀerent, and we never speak a completely identical language. B ut if the distributions
are close enough, we can communicate. That is what makes the p robabilistic channels that
generate streams of words into communication channels. The y are the languages that we speak
and write. But how do we come to share these channels? How do th e probability distributions
over the words settle in the minds of all people who use a langu age?
As we use a language, we sample it, and retain the word frequen cies (8.6–8.7). The conditional
probability of the next word in a given context is the ratio in the form
/bracketleftbigwhat is the next word that I am going to ⊢say/bracketrightbig=/bracketleftbigwhat is the next word that I am going to say/bracketrightbig
/bracketleftbigwhat is the next word that I am going to/bracketrightbig
The numerator is the total frequency of the whole phrase. The denominator is the total fre-
quency of the preﬁx, without the last word. The total frequen cies are summed up over all
contexts, as in (8.7) |.
Language generation. The process of language generation proceeds by sampling the next
word, adding it to the context, then sampling the next word, a dding it to the context, and so on.
This is what the chatbots do when they speak to us [50]. Intuit ively, the probability of a phrase
that I (or a chatbot) might say should be the product of the pro babilities of each of the words
from its preceding context:
/bracketleftbigwhat is/bracketrightbig=/bracketleftbigwhat/bracketrightbig·/bracketleftbigwhat⊢is/bracketrightbig
/bracketleftbigwhat is the/bracketrightbig=/bracketleftbigwhat/bracketrightbig·/bracketleftbigwhat⊢is/bracketrightbig·/bracketleftbigwhat is⊢the/bracketrightbig
/bracketleftbigwhat is the next/bracketrightbig=/bracketleftbigwhat/bracketrightbig·/bracketleftbigwhat⊢is/bracketrightbig·/bracketleftbigwhat is⊢the/bracketrightbig·/bracketleftbigwhat is the⊢next/bracketrightbig
and so on. Justifying this intuition leads to a general law fo r reasoning about probabilistic
channels.
8.2.2 Generative channels and sources
Deﬁnition 8.1. A channel is called generative if it is in the form X+→∆X, i.e., its input and
output types are the same. A generative channel makes its und erlying typeXinto a source .
The stochastic matrix of a generative channel is in the form/bracketleftbig−⊢−/bracketrightbig:X∗×X∗→[0,1]. On
one hand, it is just a conditional probability distribution overX∗, written in a matrix form. On
the other hand, when viewed as a channel, the fact that it emit s outputs of the same type as its
inputs puts it in the generative mode of operation. Starting from a context x=/parenleftbigx1x2...xm/parenrightbig, it
115
generates a stream of outputs
/bracketleftbigx1...xm⊢xm+1/bracketrightbig,/bracketleftbigx1...xmxm+1⊢xm+2/bracketrightbig,...,/bracketleftbigx1...xmxm+1...xm+n−1⊢xm+n/bracketrightbig,...
by appending each new output to the old context and thus formi ng a new context, which it
consumes as the input to generate the next output. Then it app ends that output to the previous
input, and so on. At each point of the process, the probabilit y of generating a particular cumu-
lative sequence of outputs is completely determined by the s ingle-output generations, as their
product:
/bracketleftbigx1...xm⊢xm+1...xn/bracketrightbig=/bracketleftbigx1...xm⊢xm+1/bracketrightbig·/bracketleftbigx1...xmxm+1⊢xm+2/bracketrightbig·/bracketleftbigx1...xmxm+1xm+2⊢xm+3/bracketrightbig· (8.13)
.../bracketleftbigx1...xmxm+1...xm+n−1⊢xn/bracketrightbig
The probability that a language channel generates a particu lar phrase is thus a product of the
single step generations, starting from the empty context:
/bracketleftbigwhat is the next word that I am going to say/bracketrightbig=/bracketleftbigwhat/bracketrightbig·/bracketleftbigwhat⊢is/bracketrightbig·/bracketleftbigwhat is⊢the/bracketrightbig·/bracketleftbigwhat is the⊢next/bracketrightbig·
.../bracketleftbigwhat is the next word that I am ⊢going/bracketrightbig·/bracketleftbigwhat is the next word that I am going ⊢to/bracketrightbig·/bracketleftbigwhat is the next word that I am going to ⊢say/bracketrightbig
Generalizing (8.13) yields the general transitivity law of generative channels.
Proposition 8.2. Every generative channel/bracketleftbig−⊢−/bracketrightbig:X∗×X∗→[0,1]satisﬁes
/bracketleftbigx⊢yz/bracketrightbig=/bracketleftbigx⊢y/bracketrightbig·/bracketleftbigxy⊢z/bracketrightbig(8.14)
for all x,y,z∈X∗.
Remark. Viewing the matrix/bracketleftbig−⊢−/bracketrightbig:X∗×X∗→[0,1] as a listing of conditional probabilities
makes equation (8.16) into a familiar law, equivalent to the Bayesian law, which follows directly
from the deﬁnition of the conditional probability given by T homas Bayes:
/bracketleftbigx⊢y/bracketrightbig=/bracketleftbigxy/bracketrightbig
/bracketleftbigx/bracketrightbig
116
8.3 Inverse channels and Bayesian inference
How can we derive causes from e ﬀects? How do we invert a probabilistic channel? — Finding
the unobservable causes of observable e ﬀects is the central problem of science. Finding the
unknown inputs of a channel corresponding to its known outpu ts is the central problem of
cryptanalysis and one of the main problems of channel securi ty.
What does it mean that the channels Pr: X∗→∆Y∗and/tildewidePr:Y∗→∆X∗are each other’s
inverses? — Intuitively, it means that ∝a\}b∇ack⌉tl⌉{tx,y∝a\}b∇ack⌉t∇i}ht∈X∗×Y∗occurs as the input-output pair of Pr
with the same probability as its reverse ∝a\}b∇ack⌉tl⌉{ty,x∝a\}b∇ack⌉t∇i}ht∈Y∗×X∗occurs as the input-output pair of
/tildewidePr. But these probabilities are only determined if the proba bilities of x∈X∗andy∈Y∗are
determined. If we cannot tell how frequently x∈X∗occurs, how could we tell the frequency of
∝a\}b∇ack⌉tl⌉{tx,y∝a\}b∇ack⌉t∇i}ht∈X∗×Y∗. But to say that the probabilities of x∈X∗andy∈Y∗are determined means
thatXandYare sources, in the sense of Def. 8.1.
Let/bracketleftbig−⊢X−/bracketrightbig:X∗×X∗→[0,1]/bracketleftbig−⊢Y−/bracketrightbig:Y∗×Y∗→[0,1]
be the (stochastic matrices corresponding to the) generati ve channels making XandYinto
sources. (We usually omit the subscripts since the names sug gest the types.) The probabilities
that the sourceXmay generate xand thatYmay generate yare respectively
/bracketleftbigx/bracketrightbig=/summationtext
u∈X∗/bracketleftbigu⊢x/bracketrightbig
/summationtext
u,w∈X∗/bracketleftbigu⊢w/bracketrightbig and/bracketleftbigy/bracketrightbig=/summationtext
t∈Y∗/bracketleftbigt⊢y/bracketrightbig
/summationtext
t,v∈Y∗/bracketleftbigt⊢v/bracketrightbig (8.15)
Recalling that Pr x(y)=/bracketleftbigx⊢y/bracketrightbigand/tildewidePry(x)=/bracketleftbigy⊢x/bracketrightbig,
•the chance that∝a\}b∇ack⌉tl⌉{tx,y∝a\}b∇ack⌉t∇i}htwill occur as the input-output pair of Pr is/bracketleftbigx/bracketrightbig·/bracketleftbigx⊢y/bracketrightbig, whereas
•the chance that∝a\}b∇ack⌉tl⌉{ty,x∝a\}b∇ack⌉t∇i}htwill occur as the input-output pair of /tildewidePr is/bracketleftbigy/bracketrightbig·/bracketleftbigy⊢x/bracketrightbig.
The requirement that /tildewidePr is the inverse of Pr is thus
/bracketleftbigx/bracketrightbig·/bracketleftbigx⊢y/bracketrightbig=/bracketleftbigy/bracketrightbig·/bracketleftbigy⊢x/bracketrightbig(8.16)
The inverse of the channel Pr: X∗→∆Y∗can thus be deﬁned
/tildewidePr:Y∗→∆X∗(8.17)
y∝mapsto→/bracketleftbigy⊢x/bracketrightbig=/bracketleftbigx/bracketrightbig·/bracketleftbigx⊢y/bracketrightbig
/bracketleftbigy/bracketrightbig
Bayesian reasoning. The deﬁnition of inverse probability goes back to Thomas Bay es’ fa-
mous essay “on the doctrine of chances” [7], albeit in a convo luted form. Formulas (8.16)
and (8.17) are often referred to as the Bayesian law . Within a single source, without the con-
cept of channel, it was rediscovered in the subsequent centu ries by many others, until it was
inaugurated as one of the basic tools of statistical inferen ce by Fisher [27].
117
Example: the Monty Hall problem
Monty Hall was the host of a TV game show Let’s make a deal . In one of his games, the prize
was a car, hidden behind one of three doors. You will win the ca r if you select the correct door.
After you have picked one door but before the door is open, Mon ty Hall opens one of the other
doors, where he reveals a goat, and asks if you would like to sw itch from your current selection
to the remaining door. How will your chances change if you swi tch?
Let us analyze the situation. Suppose that the doors are enum erated by 0,1,2. Say the door
that you have initially chosen is assigned the number 0.
•The available channel inputs X={C0,C1,C2}correspond to the situations that the Car is
behind the door 0,1, or 2.
•The available channel outputs Y={G0,G1,G2}correspond to the situations where Monty
Hall shows a Goat behind the door 0 ,1, or 2.
•Let/bracketleftbigx⊢y/bracketrightbigdenote the probability that the car position input x∈{C0,C1,C2}causes Monty
Hall’s goat output y∈{G0,G1,G2}. The matrix of the induced channel is given in the ﬁrst
of the following three tables.
/bracketleftbigx⊢−/bracketrightbigG0G1G2/bracketleftbigC0⊢−/bracketrightbig01
21
2 /bracketleftbigC1⊢−/bracketrightbig0 0 1/bracketleftbigC2⊢−/bracketrightbig0 1 0/bracketleftbigx,y/bracketrightbigG0G1G2
C0 01
61
6
C1 0 01
3
C2 01
30/bracketleftbigy⊢−/bracketrightbigC0C1C2/bracketleftbigG0⊢−/bracketrightbig↑↑↑/bracketleftbigG1⊢−/bracketrightbig1
302
3 /bracketleftbigG2⊢−/bracketrightbig1
32
30
–If the car is behind the door 0, then Monty Hall can open the doo rs 1 and 2 with
equal probabilities/bracketleftbigC0⊢G1/bracketrightbig=/bracketleftbigC0⊢G2/bracketrightbig=1
2, and show the goat there. This is the
ﬁrst line of the table.
–If the car is behind the door 1, then Monty Hall can only show th e goat behind the
door 2, and thus/bracketleftbigC1⊢G2/bracketrightbig=1.
–If the car is behind the door 2, then Monty Hall can only show th e goat behind the
door 1, and thus/bracketleftbigC2⊢G1/bracketrightbig=1.
•The second table displays the joint probability distributi on/bracketleftbigx,y/bracketrightbig∈∆(X×Y ), which is
computed from the ﬁrst table using the formula/bracketleftbigx,y/bracketrightbig=/bracketleftbigx/bracketrightbig·/bracketleftbigx⊢y/bracketrightbig, where we assume that
the chances that the car is behind each of the door are equal, i .e.,/bracketleftbigC0/bracketrightbig=/bracketleftbigC1/bracketrightbig=/bracketleftbigC2/bracketrightbig=1
3.
•The third table displays the chance/bracketleftbigy⊢x/bracketrightbigthat Monty Hall’s goat output y∈{G0,G1,G2}
was caused by the car position x∈{C0,C1,C2}as the input. The computation of the
probability that this might be the cause of the observed stat e is computed by going back
along the inverse channel (8.17). The distribution of the ca r is/bracketleftbigx/bracketrightbig=1
3forx∈{C0,C1,C2},
whereas the distribution of the goat/bracketleftbigy/bracketrightbigis computed by summing up the columns of the
middle table. Hence,/bracketleftbigG0/bracketrightbig=0 and/bracketleftbigG1/bracketrightbig=/bracketleftbigG2/bracketrightbig=1
2. Since/bracketleftbigG0/bracketrightbig=0, the output G0can
never be observed, and the chances/bracketleftbigG0⊢−/bracketrightbigare undeﬁned, which is denoted by ↑. The
118
second row is
/bracketleftbigG1⊢C0/bracketrightbig=/bracketleftbigC0/bracketrightbig·/bracketleftbigC0⊢G1/bracketrightbig
/bracketleftbigG1/bracketrightbig=1
3·1
2
1
2=1
3
/bracketleftbigG1⊢C1/bracketrightbig=/bracketleftbigC1/bracketrightbig·/bracketleftbigC1⊢G1/bracketrightbig
/bracketleftbigG1/bracketrightbig=1
3·0
1
2=0
/bracketleftbigG1⊢C2/bracketrightbig=/bracketleftbigC2/bracketrightbig·/bracketleftbigC2⊢G1/bracketrightbig
/bracketleftbigG1/bracketrightbig=1
3·1
1
2=2
3
The third row is computed analogously.
For both outputs G1andG2, displaying the goat behind the door 1 or 2, the chance that th e car
is behind the remaining door 2, resp. 1, is twice bigger than t he chance that it is behind the
door 0 that you had chosen initially. You should switch.
8.4 Probabilistic noninterference
8.4.1 Sharing information
Suppose that a probabilistic channel/bracketleftbig−⊢−/bracketrightbig:X∗×Y∗→[0,1] is shared by Alice, Bob,
and other subjects of type S, like in Sec. 6.3.2.1. Alice can only enter the inputs from he r own
clearance typeXA(6.15) and only observes the corresponding outputs from her local channel
view/bracketleftbig−⊢−/bracketrightbigA:X∗×Y∗→[0,1]
/bracketleftbig()⊢()/bracketrightbigA=1/bracketleftbigx::u⊢y::v/bracketrightbigA=/bracketleftbigx⊢y/bracketrightbigA·/bracketleftbigx::u⊢v/bracketrightbig
∗ifu∝A/bracketleftbigx⊢y/bracketrightbigAotherwise(8.18)
where/bracketleftbig−⊢−/bracketrightbig
∗:X+×Y→ [0,1] is the matrix of the single-output version of the channel:
/bracketleftbigu⊢v/bracketrightbig
∗=/summationdisplay
y∈Y∗/bracketleftbigu⊢y::v/bracketrightbig(8.19)
Deﬁnition (8.18) lifts to probabilistic channels the local view from the possibilistic framework
in (6.17).
8.4.2 Probabilistic worldviews
Alice’s state of the world St xA:X∗→{0,1}was deﬁned in (6.16) as the characteristic function
of the set{x:X∗|x↾A=xA}. It is the inverse image of Alice’s view xAalong the purge projec-
tion↾A:X∗→X∗. The states of the world corresponding to the di ﬀerent projections that Alice
may observe were collected into the possibilistic channel S t:X∗
A→℘X∗, conveniently written
119
as the matrix/bracketleftbig−⊢−/bracketrightbig:X∗
A×X∗→{0,1}of entries/bracketleftbigxA⊢x/bracketrightbig=StxA(x). In the stochastic case,
assuming thatXis a source with the frequency distribution Pr =/bracketleftbig−/bracketrightbig:X∗→[0,1] deﬁned as
in (8.15), the local state of the world is the probabilistic c hannel St:X∗
A→∆X∗corresponding
to the stochastic matrix/bracketleftbig−⊢−/bracketrightbig:X∗
A×X∗→[0,1] with the entries
/bracketleftbigxA⊢x/bracketrightbig=StxA(x)=[x]
[xA]ifx↾A=xA
0 otherwise(8.20)
8.4.3 Probabilistic interference channels
By repeatedly entering the same input xA:X∗
Alike in Sec. 6.3.2.3 , but this time not just record-
ing the possible outputs, but counting their frequencies, A lice can derive the interference ver-
sion of a probabilistic channel, say in the matrix form
/bracketleftbig−⊢−/bracketrightbig:X∗×Y∗
/integraltextA/bracketleftbig−⊢−/bracketrightbig:X∗
A×Y∗
by deﬁning the matrix entries using (8.18) and (8.20) to be
/integraltextA/bracketleftbigxA⊢y/bracketrightbig=/summationdisplay
x∈X∗/bracketleftbigxA⊢x/bracketrightbig·/bracketleftbigx⊢y/bracketrightbigA(8.21)
8.4.4 Deﬁnition and characterization of probabilistic noni nterference
With the above liftings of notions and notations from possib ilistic channels to probabilistic
channels, the deﬁnition of probabilistic noninterference can be written in the same way. We
just rewrite Def. 6.3 from its relational form to stochastic matrices.
Deﬁnition 8.3. A channel/bracketleftbig−⊢−/bracketrightbig:X∗×Y∗→[0,1] satisﬁes the noninterference requirement
if for all subjects A, it is indistinguishable from the interference channels th at it induces
/integraltextA/bracketleftbigxA⊢y/bracketrightbig=/bracketleftbigxA⊢y/bracketrightbig. (8.22)
Proposition 8.4. For every shared channel/bracketleftbig−⊢−/bracketrightbig:X∗×Y∗→[0,1]and every subject A,
the following conditions are equivalent:
(a) the noninterference requirement:
/integraltextA/bracketleftbigxA⊢y/bracketrightbig=/bracketleftbigxA⊢y/bracketrightbig
120
(b) for all x,x′∈X∗andy∈Y∗,
x↾A=x′↾A=⇒/bracketleftbigx⊢y/bracketrightbigA=/bracketleftbigx′⊢y/bracketrightbigA
(c) for all x∈X∗andy∈Y∗
/bracketleftbigx⊢y/bracketrightbigA=/bracketleftbigx↾A⊢y/bracketrightbig
(d) for all x,x′∈X∗there is u∈X∗such that for all y∈Y∗holds
x↾A=u↾A∧/bracketleftbigx⊢y/bracketrightbigA=/bracketleftbigu⊢y/bracketrightbigA∧u↾¬A=x′↾¬A
where¬A=S\{A}.
The proof is left as an exercise, since it again lifts from the possibilistic cases in Prop. 6.4.
8.4.5 Example: Car rental
Alice and Bob used to visit Smallville together, and they alw ays rented a car. Alice loved to
rent a Porsche. They are not so close anymore, but Bob started missing Alice, and he is hoping
that their paths might cross again.
One day Bob visits Smallville again, and goes as always to the car rental oﬃce. He requests the
Porsche. If the Porsche is available, then Alice is probably not in town. Or maybe she is, but
the Porsche was not available when she tried to rent it. Or may be the Porsche is not available,
but someone else has rented it, and Alice is not in town. Or may be. . .
"Everything is possible. But how likely is it that Alice is in town if the Porsche is available?"
"Hmm. It is much easier to estimate how likely it is that Porsc he is available if Alice is in town.
No chance! Maybe a small chance. Say 1 in 5. On the other hand, n o one ever wants to pay
the pricey Porsche rental, so if Alice is notin town, then I think the chance that the Porsche is
available should be something like 90%."
Bob’s estimates express his beliefs about the extent to whic h Alice’s presence may be the cause
of Porsche’s unavailability. There are thus two possible ca uses, and two possible e ﬀects:
•The causes are the inputs from X={a,¬a}, where ameans that Alice is in town, and ¬a
that she is not in town.
•The eﬀects are the outputs from Y={p,¬p}, where pmeans that the Porsche is available
for rent, and¬pthat it is out, and unavailable.
•The channel/bracketleftbig−⊢−/bracketrightbig:X×Y→ [0,1] now expresses Bob’s beliefs. It is the leftmost
matrix. The ﬁrst row displays the probable e ﬀects of Alice’s presence; the second row
121
are the eﬀects of her absence.
/bracketleftbigx⊢−/bracketrightbigp¬p/bracketleftbiga⊢−/bracketrightbig1
54
5 /bracketleftbig¬a⊢−/bracketrightbig9
101
10/bracketleftbigx,y/bracketrightbigp¬p
a1
102
5
¬a9
201
20/bracketleftbigy⊢−/bracketrightbiga¬a/bracketleftbigp⊢−/bracketrightbig2
119
11 /bracketleftbig¬p⊢−/bracketrightbig8
91
9
•Bob has no idea how likely it is that Alice is in town, so he take s the chance to be ﬁfty-
ﬁfty, i.e., that the probabilities are/bracketleftbiga/bracketrightbig=/bracketleftbig¬a/bracketrightbig=1
2. Using the formula/bracketleftbigx,y/bracketrightbig=/bracketleftbigx/bracketrightbig·/bracketleftbigx⊢y/bracketrightbig
again, the second table is obtained from the ﬁrst table by mul tiplying all entries with1
2.
•The third table displays the chance/bracketleftbigy⊢x/bracketrightbigthat the availability and unavailability of the
Porsche, the output y∈{p,¬p}is caused by Alice’s presence or absence, which is the
input x∈{a,¬a}. This chance is derived from the second table using the Bayes ian law
in (8.16). The probabilities/bracketleftbigy/bracketrightbigare obtained by summing up the columns of the second
table. Hence,/bracketleftbigp/bracketrightbig=11
20and/bracketleftbig¬p/bracketrightbig=9
20.
So if the Porsche is available, then the chance that Alice is i n town is/bracketleftbigp⊢a/bracketrightbig=2
11; if the
Porsche is not available, then the chance is/bracketleftbig¬p⊢a/bracketrightbig=8
9.
8.5 Secrecy
The earliest notion of secrecy, the "One Ring to rule them all" of cryptography, is Shannon’s
notion of perfect secrecy [56]. We show how this classical concept is subsumed under ch annel
security, and then comment how its modern reﬁnements ﬁt into the same framework.
8.5.1 Perfect secrecy
Acipher is a family of functions {∝a\}b∇ack⌉tl⌉{tEk,Dk∝a\}b∇ack⌉t∇i}ht}k∈K, where
•Ek:M→C are the encryption functions,
•Dk:C→M are the decryption functions,
•Mis the type of messages , orplaintexts ,
•Cis the type of ciphertexts , and
•Kis the type of keys,
such that the equations
Dk(Ek(m))=m (8.23)
hold for every k∈K and every m∈M . These equations impose on ciphers the functional
requirement that every message menciphered as c=Ek(m) can be deciphered as m=Dk(c), for
122
every k∈K. The security requirement, on the other hand, is that this is the only way to recover
the plaintext, i.e., that it cannot be recovered without the keykwhich allows the user to pick the
correct Dkfor (8.23). This is the secrecy requirement. It can be formalized probabilistically, by
requiring that chance that mcan be guessed from c=Ek(m) is negligible, i.e., close to 0. Ever
since Shannon’s seminal paper on "Communication theory of Secrecy Systems" [56], where the
formalization was proposed, the assumption was that "the en emy knows the system", in the
sense that the attacker Alice is given the family {∝a\}b∇ack⌉tl⌉{tEk,Dk∝a\}b∇ack⌉t∇i}ht}k∈K. Although she is not given the key
k, and therefore does not know which particular encryption fu nction Ekis used, knowing the
whole family of encryption functions allows her to average o ut and derive a guessing channel
K×ME− →C
C/bracketleftbig
−⊢−/bracketrightbig
−−−−−−→∆M
where the distribution/bracketleftbigc⊢−/bracketrightbig∈∆Mmeasures the probability/bracketleftbigc⊢m/bracketrightbigthatc=Ek(m) by
averaging overKthem-cylinder of E−1
k(c), i.e., by counting which portion of Kenciphers m
asc. Hence, we have the memoryless probabilistic channel
/bracketleftbig−⊢−/bracketrightbig:C →∆M
c∝mapsto→/parenleftbig/bracketleftbigc⊢−/bracketrightbig:M → [0,1]/parenrightbig
m∝mapsto→/bracketleftbigc⊢m/bracketrightbig=#{k∈K | Ek(m)=c}
#K(8.24)
where # Xdenotes the number of elements of the set X. This channel captures attacker Alice’s
view of the cipher. Her goal is to guess mfrom c, and (8.24) gives the guessing odds. The
value/bracketleftbigm⊢c/bracketrightbigis the chance that the plaintext is mif the ciphertext is c. In other words, we
write the conditional probability Pr( m|c) in the form/bracketleftbigc⊢m/bracketrightbig. The guessing is thus driven by the
probability distribution/bracketleftbigc⊢−/bracketrightbig:M→ [0,1]. If Alice is given a single chance to guess the
plaintext, she should probably try an mwith the highest probability/bracketleftbigc⊢m/bracketrightbig; otherwise, if there
are more guessing chances, or if the cryptanalytic attack is an ongoing process, then the whole
area of betting strategies and information elicitation ope ns up.
Shannon’s perfect secrecy requirement was that the ciphertext tells nothing about the plaintext
that wouldn’t be known without the ciphertext. It is assumed that the messages are sourced
with a publicly known distribution/bracketleftbig−/bracketrightbig:M→ [0,1], which can be, e.g., the word frequency if
Mis the lexicon of a language. So/bracketleftbigm/bracketrightbigis the a priori chance that an unknown word is m. The
perfect secrecy requirement is that the ciphertext does not add any posterior information to this
prior knowledge.
Deﬁnition 8.5. A cipher satisﬁes the perfect secrecy requirement if the equation
/bracketleftbigc⊢m/bracketrightbig=/bracketleftbigm/bracketrightbig(8.25)
holds for all m∈M andc∈C.
The statistical independency of mandc, required by (8.25), can be equivalently expressed using
123
the constant projector
υ! :∆C!− →1υ− →∆C (8.26)
which projects all distributions over Cinto the uniform distribution υ:C → [0,1] where
υ(c)=1
#Cfor all c∈C. Note that 1=∆1, as there is a single probability distribution on the
singleton.
Proposition 8.6. A cipher satisﬁes the perfect secrecy requirement from Def. 8.25 if and only
if the memoryless channel
/bracketleftbig−⊢−/bracketrightbig:∆C →∆M (8.27)
γ∝mapsto→/summationdisplay
x∈Cγ(x)·/bracketleftbigx⊢−/bracketrightbig
satisﬁes the negative security requirement with respect to the projectorυ!from (8.26) , which
means that the following triangle commutes
∆C
∆1 ∆M
∆C!/bracketleftbig−⊢−/bracketrightbig
υ
/bracketleftbig−⊢−/bracketrightbig(8.28)
Proof . Consider the point distribution χc∈∆C
χc:C → [0,1]
x∝mapsto→1 if x=c
0 otherwise
for an arbitrary c∈C. The deﬁnition of/bracketleftbig−⊢−/bracketrightbigin (8.27) implies
/bracketleftbig−⊢−/bracketrightbig(χc)(m)=/summationdisplay
x∈Cχc(x)·/bracketleftbigx⊢m/bracketrightbig=/bracketleftbigc⊢m/bracketrightbig(8.29)
where we ﬁx an arbitrary m∈M , for convenience. On the other hand, going down and right
around the triangle in (8.28), we get
/bracketleftbig−⊢−/bracketrightbig(υ)(m)=/summationdisplay
x∈C1
n·/bracketleftbigx⊢m/bracketrightbig=1
n/summationdisplay
x∈C/bracketleftbigx,m/bracketrightbig(8.30)
where n=#C. The commutativity of the triangle in (8.28) for all χc, i.e., the equations
124
/bracketleftbig−⊢−/bracketrightbig(χc)(m)=/bracketleftbig−⊢−/bracketrightbig(υ)(m) thus give nequations in the form
(1−n)/bracketleftbigc⊢m/bracketrightbig+/summationdisplay
x∈Cx/nequalc/bracketleftbigx⊢m/bracketrightbig=0
one for each c∈C. This system of nequations thus completely determines the values of/bracketleftbigc⊢m/bracketrightbig
for each of the nciphertexts c∈C, and for the ﬁxed arbitrary m∈M . Since each coccurs in
exactly one equation with a coe ﬃcient 1−nand with the coeﬃcient 1 in the remaining n−1
equations, the system is invariant under the permutations o fc, which means that all solutions
must be equal, i.e./bracketleftbigx⊢m/bracketrightbig=/bracketleftbigc⊢m/bracketrightbigfor all x,c∈C
This means that the value of/bracketleftbigx⊢m/bracketrightbigdoes not depend on the choice of x, and hence
/bracketleftbigm/bracketrightbig=/summationdisplay
x∈C/bracketleftbigx/bracketrightbig·/bracketleftbigx⊢m/bracketrightbig=/bracketleftbigc⊢m/bracketrightbig·/summationdisplay
x∈C/bracketleftbigx/bracketrightbig=/bracketleftbigc⊢m/bracketrightbig
. /boxempty
8.5.2 Computational secrecy
Security of modern cryptosystems is based on computational hardness assumptions. Such a
system can satisfy the secrecy requirement even if the chanc e/bracketleftbigc⊢m/bracketrightbigto guess mfrom c=Ek(m)
is greater than the chance/bracketleftbigm/bracketrightbigto guess mwithout c— provided that computing the probabilities/bracketleftbigc⊢m/bracketrightbigfor the various plaintexts mis computationally hard. This means that, although "the
enemy [still] knows the system", and the attacker Bob is stil l given the cipher {∝a\}b∇ack⌉tl⌉{tEk,Dk∝a\}b∇ack⌉t∇i}ht}k∈K, it
is computationally unfeasible for him to derive the channel/bracketleftbig−⊢−/bracketrightbig:C→∆Mand actually
compute the guessing chances/bracketleftbigc⊢m/bracketrightbig. So the security requirements on modern crypto systems
cannot be stated in terms of this channel. But although they a re stated in diﬀerent terms, they
still ﬁt into the conceptual framework of channel security i n an interesting way. The technical
details are beyond the scope of this text, but we sketch the bi g picture.
For a cipher{∝a\}b∇ack⌉tl⌉{tEk,Dk∝a\}b∇ack⌉t∇i}ht}k∈Kderived from a modern crypto system, the functional require ment is
still that the plaintext is recovered from the ciphertext by
Dk(Ek(m))=m
but now the security requirement, that this should be the only way to recover mfrom c, is
expressed not by saying that the chance to guess mfrom cwithout kis negligible (i.e., not
greater than guessing mon its own), but by saying that the chance to ﬁnd a feasible alg orithm
Athat satisﬁes
A(Ek(m))=m (8.31)
is negligible (i.e., not greater than guessing kand using A=Dk). Although echoing the structure
of the functional requirement Dk(Ek(m))=m, this version of the secrecy requirement turns out
125
to be independent of it. If we omit the keys and forget about th e decryption functions for a
moment, we are left with a function E:M→C , and the security requirement is simply that
ﬁnding the inverse images along it is hard. This is the semina l concept of one-way functions,
the stepping stone into modern cryptography [21].
Deﬁnition 8.7. A function E:M→C is said to be one-way if
a) given m∈M , it is easy to compute E(m), but
b) given c∈C, it is hard to ﬁnd m∈M such that E(m)=c.
Formally,
•(a) means that there is a feasible algorithm that computes E:M→C ; but
•(b) means that there is no feasible algorithm that computes a function A:C→M such
thatA(E(m))∈E−1(E(m))for all m∈M .
Remark. In general, it is not required that one-way functions Eare injective, and the inverse
images may not be unique. In the special case of crypto system s, however, the functional
requirement Dk◦Ek(m)=mimplies that Ek(x)=Ek(y) only when x=y, and Ekis thus
injective. In the general case, the requirement that A(E(m))∈E−1(E(m))is easily seen to be
equivalent with the equation E◦A◦E(m)=E(m). But E◦A◦E=Emeans that A◦Eis a
projector.
Proposition 8.8. A function E:M→C is one-way if and only if it is feasible, and if for
any feasible function A :C→M where A◦E:M→M is a projector, the chance that the
following triangle commutes is negligible (which we expres s by⋆).
∆M
∆C⋆∆C
∆M∆E∆E
∆A∆E(8.32)
Comment. The requirement that the triangles in (8.32), indexed by all feasible functions
A:C→M ,almost never commute, is of course notone of the channel security requirements
from Def. 8.3. There, the triangles are required to commute t o be secure. The idea is that the
projectorsΛin Def. 8.3 ﬁlter out the undesired ﬂows, and let through the d esired ﬂows. A
function fthus behaves like f◦Λprecisely when it passes no undesired ﬂows. In triangle
(8.32), the undesired ﬂows are the composites A◦Ewhere Areverts the one-way-function E.
Here the security requirement is that such ﬂows are unlikely . This is expressed by minimizing
the incidence of the equation E◦A◦E=E. The requirement that equations are not satisﬁed
are unusual in algebra, but crucial in security.
126
9 Privacy
9.1 Idea of privacy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
9.1.1 History . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
9.1.2 Attacking and securing privacy . . . . . . . . . . . . . . . . . . 1 31
9.2 Surveillance and sousveillance . . . . . . . . . . . . . . . . . . . . . . 132
9.2.1 The paradox of data as a resource . . . . . . . . . . . . . . . . . 13 2
9.2.2 The two sides of data security . . . . . . . . . . . . . . . . . . . 13 2
9.3 Data privacy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
9.3.1 Deﬁning data privacy . . . . . . . . . . . . . . . . . . . . . . . . 134
9.3.2 K-Anonymity . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
9.3.3 Diﬀerential Privacy . . . . . . . . . . . . . . . . . . . . . . . . . 141
9.4 Inﬂuence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
9.4.1 Layered architectures . . . . . . . . . . . . . . . . . . . . . . . . 14 4
9.4.2 Level-above attacks . . . . . . . . . . . . . . . . . . . . . . . . 145
9.4.3 Upshot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146
9.5 What did we learn? . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146
9.1 Idea of privacy
What is privacy? Privacy is the right to be left alone.
Privacy is nota security property. It is the source of security requiremen ts. I claim that the
water well is my private property and that my health record co ntains my private data. These
privacy claims give rise to security requirements: to preve nt others from accessing my water
well and my health record.
Privacy is the owners’ right to enjoy their resources with no interference from others. T his
right protects the owner from access through law-abiding pa rties, but also declares access from
adversaries to be illegal. Security focuses on the adversar ialprocess of defending and attacking
some privately owned assets.
Privacy is the realm of ownership claims:
•“The data about me are owned by me.”
•“The sea is owned by the King.”
Security is the process of implementing privacy claims:
•“My data are secured by encryption.”
127
•“Our maritime borders are secured by King’s Navy.”
There is also a feedback loop from security to privacy. If I am unable to secure the water well,
then I cannot claim the right to be left alone with it. For exam ple, if Lion wants to prevent other
animals from using a water well, but the well is too big for Lio n to defend it, then Lion cannot
claim the right to be left alone with the well. He may try to cha se away Gazelle drinking on the
other side of the well. But by the time Lion gets there, Gazell e has had plenty of water. After
trying to secure the well by running around it for a while, Lio n will eventually have to give up
his privacy claim.
If an asset can be secured, then it can be made private in as man y ways as it can be
used. Back in Sec. 2.3, we distinguished the dependability requir ements that can be imposed
on a car from the security requirements. The dependability i s assured by the engine and the
brakes; the security by the locks and the keys. In addition, t he privacy requirements can be
imposed on the various aspects of the use of the car. Driver’s and passengers’ privacy can be
assured by tinted windows. The right to privately use the car can be secured by the ownership
documents. The right to use a car for public transport can be i ssued as a license to private
individuals, for taxi or ride-sharing. A range of privacy ar rangements for vehicles is illustrated
in Fig. 9.1. Fig. 9.2 shows a city as a partition of space into p ublic and private. The structure
Figure 9.1: Diﬀerent types of vehicles address di ﬀerent privacy requirements
of this partition is determined by the privacy claims and arr angements: “This is my private
apartment and I have the right to be alone”, or “This is public space and everyone can come
here”. The purpose of security tools and the task of security policies is to implement such
privacy claims and denials, as well as complex privacy polic ies.
128
Figure 9.2: The structure on living spaces is induced by the v ariety of privacy policies
9.1.1 History of privacy
The social history is, ﬁrst of all, the history of shifting de marcation lines between the public
sphere and the private sphere [6, 45, 54]. The communist revo lutions usually start by abolishing
not only private property, but also private rights. Tyranni es and oligarchies, on the other hand,
erode the public ownership and rights, by privatizing resou rces, services, and social life. The
distinction between the realms of
•the public: city, market, warfare. . . and
•the private: family, household, childbirth. . .
was established and discussed in antiquity [4, 13]. It was a f requent topic in Greek tragedies:
e.g., Sophocles’ Antigone is torn between her private commitment to her brothers and he r public
duty to the king. The English word politics comes from the Greek word πόλις , denoting the
public sphere; the English word economy comes from the Greek word οίκος , denoting the
private sphere.
Disambiguations. There are many aspects of privacy, conceptualized in di ﬀerent research
communities, and studied by di ﬀerent methods; and perhaps even more aspects that are not
conceptualized in research, but arise in practice, and in in formal discourse. We carve a small
part of the concept, and attempt to model it formally.
As an abstract requirement, privacy is a negative constrain t, in the form "bad stuﬀ(actions)
should not happen" . Note that secrecy and conﬁdentiality are also such negativ e constraints,
whereas authenticity and integrity are positive constrain ts, in the form "good stuﬀ(actions)
should happen" . More precisely, authenticity and integrity require that s ome desirable infor-
129
mation ﬂows happen. For example, a message "I am Alice" is authentic if it originates from
Alice. On the other hand, conﬁdentiality, secrecy and priva cy require that some undesirable
information ﬂows are prevented: Alice’s password should be secret, her address should be
conﬁdential, and her health record should be private.
But what is the diﬀerence between privacy, secrecy and conﬁdentiality? Let us move out of the
way the diﬀerence between the latter two ﬁrst. In the colloquial usage, they allow subtle dis-
tinctions. For example, when a report is conﬁdential, we don ’t know its contents; but when it is
secret, we don’t even know that it exists. For the moment, we i gnore such diﬀerences, and fo-
cus on distinguishing privacy as a right from secrecy and con ﬁdentiality as security properties.
The terms are used interchangeably.
As for the diﬀerence between privacy and secrecy (or conﬁdentiality), it comes in two ﬂavors:
1)while secrecy is a property of information , privacy is a property of any asset orresource
that can be secured1; and
2)while secrecy is a local requirement, usually imposed on information ﬂowing throug h a
given channel, privacy is a global requirement, usually imposed on all resource requests
and provisions, along any channel of a given network.
Let us take a look at some examples.
Ad (1) Alice’s password is secret, whereas her bank account is priv ate. Bob’s health record is
private, and it remains private after he shares some of it wit h Alice. It consists of his health
information, but may also contain some of his tissue samples for later analysis. On the other
hand, Bob’s criminal record is in principle not private, as c riminal records often need to be
shared, to protect the public. Bob may try to keep his crimina l record secret, but even if he
succeeds, it will not become private. Any resource can be mad e private if the access to it can
be secured. For example, we speak of a private water well, pri vate funds, private party if the
public access is restricted. On the other hand, when we speak of a secret water well, secret
funds, or a secret party, we mean that the public does not have any information about them. A
water well can be secret, and it can be private, but not secret .
Ad (2) To attack Alice’s secret password, Mallory eavesdrops at th e secure channel to her bank;
to attack her bank account, he can of course, also try to steal her credentials, or he can initiate
a request through any of the channels of the banking network, if he can access it. Or he can
coordinate an attack through many channels. To attack a secr et, a cryptanalyst analyzes a given
cipher. To attack privacy, a data analyst can gather and anal yze data from many surveillance
points. To protect secrecy, the cryptographer must assure t hat the plaintexts cannot be derived
from the ciphertexts without the key, for a given cipher. To p rotect privacy, the network operator
must assure that there are no covert channels anywhere in the network.
1Information is, of course, a resource, so it can be private.
130
9.1.2 Attacking and securing privacy
The general principle of Privacy Policy Enforcement is:
If a resource can be secured,
then it can be made private.
This extends the general principle of Security Policy Enforcement , which states that
A resource can be secured
if and only if its value is greater than the cost of securing it .
In practice, though, things get complicated due to the dynam ic interactions across the layers
of the policy enforcement stack , displayed in Fig. 9.3. The complications arise from the fac t
Privacy
Security
Cryptography
Figure 9.3: Privacy is built upon security, which is built up on cryptography
that the strength of the foundations does not in general guar antee the strength of the buildings
that they carry. The strong foundations are necessary, but n ot suﬃcient for a strong building.
In security, this leads to the well-known phenomenon that se curity protocols may be broken
without breaking the underlying crypto [57]. In privacy, th e analogous phenomenon is that
privacy protocols may be broken without breaking the underl ying security protocols [14]. The
idea is displayed in Figures 9.4 and 9.5. The privacy attacks that leave the underlying security
intact are implemented through deceit . The methods of deception allow attackers to
•pull private data and resources, or
•push private actions
Privacy
Security
CryptographyPrivacy
Security
Cryptography
Figure 9.4: Security protocols can be attacked, by breaking the underlying crypto, but also
directly
131
Privacy
Security
CryptographyPrivacy
Security
Cryptography
Figure 9.5: Privacy protocols can be attacked, by breaking t he underlying security, but also
directly
without breaking the security measures that make them priva te.
9.2 Surveillance and sousveillance
9.2.1 The paradox of data as a resource
Private data are a privately owned resource. How can this be, if data are easy to copy, whereas
a resource was deﬁned in Sec. 3.1.1 by being hard to come by? Th e reason is that some data
must be protected as private in order to protect some physica l or ﬁnancial resources as private.
Examples. A password or a cryptographic key are easily copied, but shou ld be kept private if
they are used to protect private funds in a bank account. The i ntrinsic value of Alice’s health
record is to provide the information needed by Alice’s physi cian Bob to treat her if she becomes
ill, but the extrinsic value of Alice’s health record is to he lp Alice’s insurers withdraw her health
coverage just before she may need it, and also perhaps to tell burglars when Alice might be in
a hospital.
Private data are thus kept private not because of their utili ty for the owner, but be-
cause of their potential value for others, as means for attac ks.If private data are e ﬀec-
tively secured, they become hard to come by for the attackers , but easy to use in attacks when
they are available. Paradoxically, the privately owned dat a are resources for the non-owners.
9.2.2 The two sides of data security
TheDigital Rights Management (DRM) technologies are tasked with controlling the dis-
tribution of speciﬁed digital data. While the individually owned data are also digitized and
private, the DRM technologies have been developed mainly to protect the digital assets owned
and marketed by organizations, and in some cases by governme nts. The marketed digital assets
include software and media, such as digital music, movies, a nd digital text, such as news, study
materials, and popular literature.
The surveillance technologies are tasked with enabling and facilitating data collection
132
about the behaviors of speciﬁed subjects. In market surveil lance, the subject behaviors usually
include the marketing habits and interests, which are used f or targeted advertising, inﬂuence
campaigns, sponsored search, and recommender systems in ge neral. In investigative surveil-
lance, the subject behaviors include a wide range of subject behaviors and contacts, often open
for inclusion of any correlatable information.
It is not hard to see that the surveillance tasks and the DRM tasks are two sides of the s ame
coin: the former strives to establish a data ﬂow, the latter to pre vent data from ﬂowing. A DRM
copy protection may be used to prevent surveillance; survei llance techniques may be used to
break or disable a DRM protection.
technologyprivacy
surveillancecopy protections
sousveillanceﬁle sharingmore privacy
for gov’t, industry
less privacy
for citizens, consumersanonymizers
trust services
Figure 9.6: Technology investments favor the proﬁtable sid e of data privacy
The tasks of securing data privacy on one hand and of the intel lectual property on the other
correspond to the same security problem : to control the data ﬂows in digital networks. How-
ever, the technologies developed for surveillance on the on e hand and for the DRM on the other
hand lead to the opposite solutions : they weaken privacy and strengthen the intellectual prop-
erty. Fig. 9.6 illustrates how the technical advances suppo rt privacy protections of intellectual
property and commercial digital assets, but help break the p rivacy protections of individual
behaviors and personal digital assets.
133
9.3 Data privacy and pull attacks
9.3.1 Deﬁning data privacy
Data privacy deﬁnes the boundaries between the private sphe re and the public sphere and pro-
vides the legal right to be left alone with your private data. This right intends to restrict the
access to the private data, just like secrecy intends to rest rict the access to secret information.
But while secrecy is concerned with access to data through a s peciﬁc channels, the right to
privacy is not speciﬁc to a particular channel, it legally pr otects from access to the private data
through any global channel.
Secrecy is formally deﬁned in cryptography. The earliest de ﬁnition, due to Shannon [56], says
that it is a property of a channel where the outputs are statis tically independent of the inputs.
It is tempting, and seems natural, to deﬁne privacy in a simil ar way. This was proposed by
Dalenius back in the 1970s [20]: A database is private if the p ublic data that it discloses publicly
say nothing about the private data that it does not disclose. This desideratum , as Dalenius
called it, persisted in research for a number of years, befor e it became clear that it was generally
impossible as a requirement when considering access throug h any global channel. For example,
if everybody knows that Alice eats a lot of chocolate, but the re is an anonymized database that
shows a statistical correlation between eating a lot of choc olate and heart attacks, then this
database discloses that Alice may be at a risk of heart attack , which should be Alice’s private
information, and thus breaches Dalenius’ desideratum. Not ably, this database breaches Alice’s
privacy even if Alice’s record does not come about in it. Indeed, it is not nec essary that Alice
occurs in the database either for establishing the correlat ion between chocolate and heart attack,
or for the public knowledge that Alice eats lots of chocolate ; the two pieces of information can
arise independently. Alice’s privacy can be breached by lin king two completely independent
pieces of information, one about Alice and chocolate, the ot her one about chocolate and heart
attack. But since Alice’s record does not come about in the da tabase, it cannot be removed
from it, or anonymized in it. The public knowledge of backgro und information can establish
covert channels that require the deﬁnition for data privacy to be diﬀerent from the deﬁnition of
secrecy.
Alice’s privacy is her global right. But the deﬁnition of pri vacy of her data in a database cannot
have global requirements and depend on the (non)-availabil ity of background information and
covert channels that are in no control of the database. It has to depend on the records in the
database that contain Alice’s information and how they are d isclosed. Limiting the focus to
what private information can be learned just from Alice’s re cord in the database leads to the
more practical measure of diﬀerential privacy . In diﬀerential privacy, it is required that all
sensitive data about Alice that can be learned from a databas eDwith Alice’s record, could also
be learned from the database D′where Alice’s record is replaced by an alternative record. T his
sounds very close to Dalenius’ deﬁnition, which says that al l sensitive data about Alice that
can be learned from a database Dwith Alice’s record, could also be learned without access to
the database D, but on a closer look it is very di ﬀerent. The deﬁnition of di ﬀerential privacy
addresses the impossibility of privacy according to Daleni us’ deﬁnition and removes the issue
134
of background information.
Data privacy in databases. The life of any data set consists of ﬁrst gathering the data (v eil-
lance), then storing and releasing the data, and ﬁnally proc essing the data. Databases are the
central tool for the management of large collections of data . Conceptually, databases organize
data as large matrices, called tables , storing values of attributes organized in records .
Deﬁnition 9.1. Given the setsRofrecords ,Aofattributes , and Vaofvalues for each a∈A,
a database is a matrix
D:R×A → V
where V=/uniontext
a∈AVaandD(r,a)∈Vafor all r∈Randa∈A.
The rows of the matrix represent the tuples of data in a record of the database, and the columns
of the matrix represent the attributes, i.e., the data for an attribute for each record.
If an attribute has a unique value for each record, this attri bute is considered to be an identiﬁer .
Deﬁnition 9.2. Anidentiﬁer (ID) is an attribute a∈A that uniquely determines all entities.
More precisely, there is a function f:Va→E such that for all e∈Eholds
f(Da
R(e))=e
where Da
R(e)is value for the attribute athat occurs in a record R(e) in the databaseD.
Attributes of private data, like health conditions, educat ional records, or ﬁnancial information,
that are particularly sensitive regarding the protection o f their access, are called sensitive at-
tributes .
Figure 9.7: Privacy can be achieved by being hard to see or har d to ﬁnd
135
We described some instances of data gathering in section 9.2 , in particular in the context of
public data. The fact that the gathered data in these cases ha s been public does not imply for
every stored datum, possibly containing sensitive attribu tes, to be publicly standing out. One
can be completely private and anonymous if lost in a crowd as s uggested in Fig. 9.7, even if
the data about the crowd is public and contains sensitive inf ormation. The surveillance of this
data alone does not kill the privacy of any individual’s data in the crowd. It is the search in the
collected data that can kill the privacy. The simple look at t he image does not highlight any
private data of any subject, but if there is su ﬃcient reason to spend any e ﬀort on processing the
image, private data can be identiﬁed and extracted. The cont rolled access to the records of the
database through its interface can restrict the searches an d help protect the privacy of the data.
ID QID S A
Name Zipcode Age S ex Disease
Alice 47677 29 F ovarian cancer
Betty 47602 22 F ovarian cancer
Charles 47678 27 M prostate cancer
David 47905 43 M ﬂu
Emily 47909 52 F heart disease
Fred 47906 47 M heart disease
Figure 9.8: Data contain identiﬁers (ID) and sensitive attr ibutes (SA).
Medical databases contain sensitive attributes, like e.g. , health conditions or medication records,
that are privately owned before they are brought into the dat abase. Data are assumed to be se-
cured while stored, their release makes them public, and the concern of privacy and anonymity
arises with the controlled access and the release of any reco rd. The database in Fig. 9.8 contains
the identiﬁer Name and the sensitive attribute Disease . Neither Alice nor Betty would like the
information that they have been diagnosed with cancer to be p ublicly known, they would want
to protect access to this private data.
As a simple solution to support the restriction of access to s ensitive private data, access could
simply be prohibited to anyone other than the owners. This si mple solution is not very practical,
because direct access to the private data in not just needed b y the owners, but is also important
for health care professionals to have to do their work.
Accumulative and anonymized access is important for the pub lic and for research to improve
general understanding of health conditions and the spread a nd causes of diseases. In many
cases, the sensitive attributes only need to be presented in accumulated form for statistical
analysis where they can be detached from the identiﬁers of th e records. Individuals would
not be willing to have their sensitive data released if their identity can be identiﬁed from the
released record. This is where the privacy protection of sta tistical databases come in.
Deﬁnition 9.3. Data are collected from a set of entitiesE.Data gathering is a map R:E→R ,
so that DR(e)is the tuple of the data corresponding to the entity e∈E.Data identiﬁcation is a
map E:R→E , such that E(R(e))=e.
136
Simply removing the identiﬁer from a record before release r emoves the data identiﬁcation by
the means of the identiﬁer, but it does not detach the record f rom the entity. The removal of the
identiﬁer of the name attribute in the example in Fig. 9.8 lea ves the attributes shown in Fig. 9.9.
In each record the sensitive attributes are only linked to no n-sensitive attributes that do not
reveal the identity of the record by themselves - the records seem to be anonymized.
QID SA
Zip code Age Sex Disease
47677 29 F ovarian cancer
47602 22 F ovarian cancer
47678 27 M prostate cancer
47905 43 M ﬂu
47909 52 F heart disease
47906 47 M heart disease
Figure 9.9: Attempt at anonymizing records by removing iden tiﬁers (ID).
Cross-referencing the records in the anonymized database a gainst records in public databases,
like a voter register shown in Fig. 9.10, allows for re-ident iﬁcation of the records and link the
sensitive attribute entries back with the individuals’ ide ntities.
QID SA
Zip code Age Sex Disease
47677 29 F ovarian cancer
47602 22 F ovarian cancer
47678 27 M prostate cancer
47905 43 M ﬂu
47909 52 F heart disease
47906 47 M heart diseaseName Zip code Age Sex
Alice 47677 29 F
Bob 47983 65 M
Carol 47677 22 F
Dan 47532 23 M
Ellen 46789 43 F
Figure 9.10: Medical database linked with V oter Register.
Such examples are real. In the 90s, a case involving the gover nor of Massachusetts got highly
publicized and was subsequently used to drive privacy polic y in the health care industry. Af-
ter the governor collapsed at a public event, a graduate stud ent demonstrated that linking the
anonymized data of health records that his health insurance generally made available for re-
search studies, with the anonymized data that could be bough t from the voter register, allowed
the re-identiﬁcation of the governor’s health record relat ed to the public collapse and exposed
the governor’s health condition. The anonymized records of the health insurance had the ob-
vious personal identiﬁers removed, but the published data i ncluded zip code, date of birth, and
gender. When matched with the voter records, there were only six possible patient records that
could belong to the governor. Only three matched the gender, and only one shared the zip code.
Those attributes were su ﬃcient to identify the governor uniquely. This case got later published
137
in [58]. Interestingly, the names and other details of the ca se from public media got changed in
this publication, as listed in Fig. 9.11, to protect the priv acy of the involved entities.
Medical Data Released as Anonymous
SSN Name Race Date of Birth Gender ZIP Martial Status Problem
asian 09/27/64 female 02139 divorced hypertension
asian 09/30/64 female 02139 divorced obesity
asian 04/18/64 male 02139 married chest pain
asian 04/15/64 male 02139 married obesity
black 03/13/63 male 02138 married hypertension
black 03/18/63 male 02138 married shortness of breath
black 09/13/64 female 02141 married shortness of breath
black 09/07/64 female 02141 married obesity
white 05/14/61 male 02138 single chest pain
white 05/08/61 male 02138 single obesity
white 09/15/61 female 02142 widow shortness of breath
V oter List
Name Address City ZIP DOB Gender Party......
................................................
................................................
Sue J. Carlson 1459 Main St. Cambridge 02142 09/15/61 female democrat......
................................................
Figure 9.11: Medical record of the Governor of Massachusett s identiﬁed.
There are several other similar cases that have been publici zed over the years, but this speciﬁc
case has been inﬂuential in determining the criteria for the 2003 HIPAA act and in deﬁning the
related criterion of k-anonymity, that has been used to advance privacy protectio n in statistical
databases. In the identiﬁcation of the governor, the attrib utes of gender, birthdate and ZIP code
were not sensitive by themselves. Their critical feature wa s that the speciﬁc value combination
of these attributes for the governor were unique in both data bases. This allowed them to act -
in combination - as an identiﬁer. We call such sets of attribu tes a quasi-identiﬁer .
Deﬁnition 9.4. Aquasi-identiﬁer (QID) is a set of attributesQ⊆A that uniquely determine
some entities.
More precisely, there is a partial function f:/producttext
i∈QVi⇀Esuch that for some e∈Eholds
f(DQ
R(e))=e
where DQ
R(e)is aQ-tuple of attributes in the database D.
In ﬁgure 9.10, the set of attributes Q={ZIP,Age,Sex}is a quasi-identiﬁer because
f(D{ZIP,Age,Sex}
R(Alice ))=Alice,
138
where D{ZIP,Age,Sex}
R(Alice )=(47677, 29, F ) and the function fis determined from the voter register
uniquely to be f(47677, 29, F )=Alice.In ﬁgure 9.11, the set of attributes Q={Gender,DOB,ZIP}
is a quasi-identiﬁer because
f(D{Gender,DOB,ZIP}
R(Sue J. Carlson ))=Sue J. Carlson,
where uniquely D{Gender,DOB,ZIP}
R(Sue J. Carlson )=(female, 09/15/61, 02142 ) and the function fis determined
from the voter register to be uniquely f(female, 09/15/61, 02142 )=Sue J. Carlson.
9.3.2 K-Anonymity
The measure of k-anonymity [60] has been introduced to make re-identiﬁcation more di ﬃcult.
With k-anonymity, a database ensures that the values of the quasi- identiﬁer attributes of any
record cannot be distinguished among at least krecords; the larger the k, the larger the set of
records among which one entity can be hiding. This measure he lps protect privacy when the
focus is one individual query.
Deﬁnition 9.5. A database Dsatisﬁes the k-anonymity requirement if for every quasi-identiﬁer
Qand everyQ-tuple of values x∈VQ=/producttext
i∈QVi, there are either at least krecords with the
same value x, or no such records exist in D. Formally, the requirements are ∀Q⊆A,∀x∈VQ:
#RQ
x=#{r∈R| DQ
r=x} ≥ k, or x/nelementDQ.
This measure needs to consider any Q-tuple of attributes as potential quasi-identiﬁers.
Techniques. The two techniques to achieve k-anonymity are generalization and suppres-
sion [59].
Ingeneralization some details of some attributes are removed to make their val ues more com-
mon, and less identiﬁable.
The choice which attribute values are used to generalize to l arger ranges has an impact on
the information that each query to the database can provide. In their use to provide averages
and statistical information about the attribute values of t he records, it is important to chose
generalizations that limit the bias that they have on the ave rages and cumulative information
that is processed for any queries.
Suppression removes records that are considered outliers that cannot be easily generalized with
other records.
Example. Figure 9.12 illustrates the records of a database that, befo re anonymization has
aQID of the attributes∝a\}b∇ack⌉tl⌉{tZIP,car,child∝a\}b∇ack⌉t∇i}ht, where the value ∝a\}b∇ack⌉tl⌉{t96822, Subaru Outback 1999, 8
year old∝a\}b∇ack⌉t∇i}htonly occurs in one record of the database as shown in the ﬁgure on the left. But
generalizing the values of the attribute ZIP to the ﬁrst two digits, the attribute carto just the
139
brand of the car, and the attribute of the age of the child to ju st the information whether the
child is a minor or not, the same anonymized values for the pre vious QID attributes for the
entity occur in krecords now, as shown in the ﬁgure on the right. The record of t he entity that
could previously be uniquely identiﬁed, now shares its valu e for these attributes with k−1 other
records. The database is k-anonymized if there are at least krecords not just for this speciﬁc Q-
tuple value for this speciﬁc Q, but if there are at least krecords in the database for any Q-tuple
value for any subset of attributes Q.
QID∝a\}b∇ack⌉tl⌉{tZIP,car,child∝a\}b∇ack⌉t∇i}ht
96822
Subaru Outback 1999
8 year oldQID∝a\}b∇ack⌉tl⌉{tZIP,car,child∝a\}b∇ack⌉t∇i}ht—k-anonymized
96***
Subaru **************
<18 years oldk
Figure 9.12: Database k-anonymization by generalization of QID-attributes.
Limitations of k-anonymity. The utility of using k-anonymity to protect privacy in databases
has its limits:
Lack of diversity: Even if a database provides k-anonymity by the appropriate application of
generalization and suppression, an entity could still be as sociated with their private information,
e.g., the sensitive attribute of their health condition if t he database lacks diversity and the same
SA value occurs in more than krecords. Then, k-anonymity does not conceal the value. It
reveals the SA not just for the entity e, but for all the more than kentities with the same SA
value xs=DS A
R(e), the same QID tuple qs=DQ
R(e)and with recordsRQ
qs. In this case the database
would be k-anonymous, but still discloses the SA of a group of more than kentities.
Background information: General anonymized data may also disclose an individual SA v alue
when combining the data from the database with any backgroun d information about an indi-
vidual.
The data relating smoking and cancer from a database Dcan be used together with the public
knowledge that Bob smokes, and, in e ﬀect, link Bob with the SA of having a cancer risk —
even if Bob does not occur in D. The background information that is used to reveal Bob’s
private information in this case is a false problem , because its release is not under the control
of the database. No anonymization of the database Dcan eliminate the information available
outside of D. Therefore, it must be acceptable that a database Dmay disclose some sensitive
information about me to those who know me — even if I do not occu r inD.
140
Example. Alice writes an exam at school. Her teacher grades the exams a nd returns them to
the students. He also gives them a summary of the results and t ells them, among others, that 3
students failed the exam. If the records for each student in a database Dcontain their grade for
this exam, the teacher’s summary does not release the grades for any student but just provides
the information:
#Rgrade
f ail=#{r∈R| Dgrade
r=fail}=3
Charlie likes to harass students, and asks all of his friends how they did in the exam. Alice
does not want to tell Charly about her exam, she wants to keep h er exam results private. After
Charly ﬁnds out the grades of all other students in the class, and that only two of them had
failed the exam, he concludes, that Alice must have failed th e exam, and harasses her about it.
Through his collection of information, Charly builds the da tabase D′that contains all records
ofDexcept for Alice’s. For this database D′, Charlie determines that
#{r∈R| D′grade
r=fail}=2.
and can conclude that D\D′=R(Alice ) and Dgrade
R(Alice )=f ail. The release of the summary
of the grades by the teacher did not preserve the privacy of Al ice’s grade on the exam, although
Alice’s result had been made 3-anonymous in the disclosure o f results from database Dby
the teacher. If the teacher had made her statement less preci se and told the students instead
that either 2 or 3 students had failed the exam, the privacy of Alice’s results would have been
preserved. The question remains how the release of aggregat e data that has been processed by
tools like mining or classiﬁcation a ﬀect the privacy and anonymity of sensitive data. This issue
occurs and has been studied in a reﬁned form in the context of s tatistical databases and has lead
to the notion of k-privacy.
Statistical databases. Statistical databases are collecting data for statistical analysis and
reporting. Their purpose is to provide cumulative and stati stical information of public interest
and for research while protecting the privacy of the individ ual records. Their focus is on the
classiﬁcation and analysis of the dataset as a whole, they ar e not interested in the details of a
speciﬁc datum. To be sharing sensitive data with the databas e, participants need to be convinced
that their record cannot be identiﬁed, and any part of their s ensitive data cannot be reconstructed
from the data provided in the disclosures of the database.
9.3.3 Differential Privacy
Diﬀerential privacy [23] is a requirement on the disclosure algorithm Fof the database D, and
not a requirement on the data in the database itself. It imple ments the indistinguishability of
databases DandD′, where database Ddiﬀers from database D′just in one individual records, in
terms of an equivalence kernel. Di ﬀerential privacy requires that the ﬂow leakage of individua l
information from any single record is negligible when relea sing a statistical result from the
database D. Intuitively, the risk to one’s privacy incurred by partici pating in a database is
expressed by the parameter of the privacy budget εin the model. The techniques that have
141
been developed to achieve di ﬀerential privacy can be controlled to achieve an arbitrary l evel of
privacy under this measure.
Deﬁnition 9.6. LetDbe a family of databases, P⊆/summationtext
a∈AVa, a family of properties (viewed
as sets of values in some attributes), and ε>0 a real number, called the privacy budget.
A disclosure algorithm F:D→P isε-diﬀerentially private if for every property Y∈Pholds
1≥|Pr(F(x)∈Y)
Pr(F(x′)∈Y)|≥e−ε
for any pair of databases x,x′∈D which diﬀer in at most one record, and where the normalized
ratio is denoted as
|x
y|=/braceleftiggx/yifx≤y,
y/xotherwise.
Implementation. Diﬀerential privacy is a property of a database disclosure algo rithm F. To
make a disclosure algorithm meet the di ﬀerent privacy requirement, the data in the database is
not getting aﬀected at all, so that no bias is added to the recorded data. Ins tead, diﬀerential pri-
vacy can be achieved by perturbing the disclosure algorithm in diﬀerent phases of the process.
In principle, one could perturb the disclosure algorithm
•at the inputs of the queries,
•at the calculation of intermediate values for the query, or
•at the output values of the disclosure.
In practice, the perturbation of the output values is the mos t common approach by adding noise
to the output values. In this way, the statistical propertie s of the added noise and the relation
of the perturbed values to the unperturbed values can be best controlled to meet the di ﬀerential
privacy requirements from deﬁnition 9.6 for a speciﬁc priva cy budgetε.
Output Perturbation Method. The standard method to achieve di ﬀerential privacy is the
controlled addition of Laplace noise to generate the output s of the disclosure algorithm [24].
The amount of noise is adjusted with respect to the parameter εto achieveε-diﬀerential privacy
and trade-oﬀthe accuracy of the disclosed results versus the risk of priv ate individual data to
be leaked. The parameter choices are guided based on the foll owing properties.
To implement diﬀerential privacy and guarantee a bound on the ratio of the pro babilities be-
tween F(x) and to F(x′), the density function of the added noise needs to have a boun d on its
ratios in shifted versions. This is why Laplace noise is chos en. The Laplace density function
for various parameters λis shown in Fig. 9.13. Two versions of the density functions a re shown
that are shifted by ∆fx,x′=∝ba∇⌈blf(x)−f(x′)∝ba∇⌈blto represent the densities of perturbed results dis-
closed by F(x) and F(x′) from databases x,x′∈ D - this is the situation we are in when adding
Laplace noise to the unperturbed results of a feasible discl osure algorithm f(x) and f(x′).
142
A bound on the ratio of the two densities for a disclosed value y∈Y∈P that could have
been generated from y=F(x) ory=F(x′) then allows to determine and control the Laplace
parameters to achieve ε-privacy for the disclosure algorithm F(x).
Lapd/parenleftig
y,GS f
ε/parenrightig∆fx,x′
y∗
Figure 9.13: Laplace density functions shifted by ∆fx,x′
Theorem 9.7. Let f :D→P be a feasible disclosure algorithm. Then
F(x)=f(x)+Lap/parenleftiggGS f
ε/parenrightigg
isε-diﬀerentially private, where GS f=/logicalandtext
x,x′∝ba∇⌈blf(x)−f(x′)∝ba∇⌈blis the global sensitivity , and Lap (λ)
describes randomly sampled noise with density function Lap d(y,λ)=1
2λexp(−|y|
λ), and where
databases x,x′∈ D diﬀer in at most one record.
Proof. To getε-diﬀerential privacy, we need to compare the disclosures of data bases xandx′,
where the two databases are di ﬀerent in just one record r. The global sensitivity GS fexpresses
a bound of how much the unperturbed disclosure f(x) can diﬀer from f(x′). With the diﬀerence
denoted by∆fx,x′=f(x)−f(x′), we can write
|∆fx,x′|=|f(x)−f(x′)| ≤ GS f.
When an arbitrary value y′∈Yis disclosed, this value could have been generated as y′=
F(x)=f(x)+Nx
Lapor as y′=F(x′)=f(x′)+Nx′
Lap, where Nx
LapandNx′
Lapare continuous
random variables representing the Laplace noise sampled fr omLap/parenleftigGS f
ε/parenrightig
. For F(x)=F(x′),
we have Nx
Lap=Nx′
Lap+∆fx,x′, and if we set y∗=F(x)−f(x), the ratio of the probabilities can
be expressed as
Pr/parenleftig
F(x)=y′/parenrightig
/Pr/parenleftig
F(x′)=y′/parenrightig
=Pr/parenleftig
Nx
Lap=y∗/parenrightig
/Pr/parenleftig
Nx′
Lap=(y∗+∆fx,x′)/parenrightig
=Lapd/parenleftig
y∗,GS f
ε/parenrightig
/Lapd/parenleftig
y∗+∆fx,x′,GS f
ε/parenrightig
.
143
Note that y∗is shown in Fig. 9.13, Lapd (y∗,GS f
ε) is depicted by the red bar and Lapd (y∗+
∆fx,x′,GS f
ε) is depicted by the blue bar in the ﬁgure. To get to the require ment for diﬀerential
privacy from Def. 9.6, we consider the normalized ratio:
|Pr(F(x)=y′)
Pr(F(x′)=y′)|=|Lapd (y∗,GS f
ε)
Lapd (y∗+∆fx,x′,GS f
ε)|
=|exp/parenleftig
−ε·y∗
GS f/parenrightig
exp/parenleftig
−/summationtext
a∈Aε|y∗+∆fx,x′|
GS f/parenrightig|
=exp/parenleftig
−/vextendsingle/vextendsingle/vextendsingle/vextendsingleε·|y∗|
GS f−ε|y∗−∆fx,x′|
GS f/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenrightig
≥exp/parenleftig
−ε/vextendsingle/vextendsingle/vextendsingle∆fx,x′/vextendsingle/vextendsingle/vextendsingle
GS f/parenrightig
≥exp/parenleftig
−ε·GS f
GS f/parenrightig
=e−ε.
The third line follows from the deﬁnition of the normalized r atio and its relation to the absolute
value of the exponent di ﬀerence:|exp(a)
exp(b)|=exp(|a−b|). The ﬁrst inequality after that applies
the triangle inequality, and the second inequality applies the global sensitivity GS fas the upper
bound on∝ba∇⌈bl∆fx,x′∝ba∇⌈bl. At the end we get the lower bound required by Def. 9.6. This bo und was
determined taking an arbitrary value of y′∈Y. Therefore, we generally have as required
1≥|Pr(F(x)∈Y)
Pr(F(x′)∈Y)|≥e−ε.
This shows that the choice of parameter λ=GS f
εfor the additive Laplace noise is successful in
making the disclosure algorithm F(x)ε-diﬀerentially private. /boxempty
9.4 Public inﬂuence and push attacks
9.4.1 Layered architectures
The internet stack, displayed in Fig. 9.14 on the right, is st udied in most network courses2. It is
less often noted that most codes and languages, both natural and artiﬁcial, are built according
to the same layered architecture, as displayed in Fig. 9.14 o n the left. The signals or messages
transferred at each layer provide the particles of which the signals or messages at the next layer
above are composed. In a natural language, the words are comp osed of letters, the sentences
2Many courses actually follow the OSI network stack, which ha s two additional layers. The additional layers do
not seem to arise from additional functionalities, but they are useful nevertheless, as a fascinating illustration
of design-by-committee and of accidents-of-evolution.
144
Figure 9.14: Languages and networks transfer messages acro ss layers
of words, texts of sentences, and so on. On the Internet, the n etwork packets are composed of
the frames transferred on the data link layer, the transport layer frames encapsulate sequences
of network packets, and so on. Just like the syntax of a natura l language determines which sen-
tences are well-formed, and thus facilitates parsing, the n etwork protocols determine formats
of the packets, to facilitate assembling the messages that m ay have been split into particles.
The familiar network protocol architecture of the Internet is displayed in Fig. 9.15. The appli-
HTTP, FTP, ...
TCP, UDP
IP (ICMP, IGMP)
Ethernet, WAP
code
Figure 9.15: Protocols are syntactic rules for communicati on across layers
cation layer protocols are designed by the application deve lopers, and follow their own layered
architecture, often several layers deep. Communications i n natural languages are also regulated
by protocols, which distinguish the sentence ﬂow of a newspa per article from the formal lan-
guages of legal documents, of engineering, and the style and language of literary narrative, of
poetry, and so on. These high-level rules tend to be more comp lex than the grammatical rules
that generate sentences, and they are only known for some lim ited cases and situations.
9.4.2 Level-above attacks
In general, a context can be viewed as a preﬁx of a well-formed message: it conveys some
information but leaves some uncertainty. A sequence of cont exts is a higher-level context, that
may require error correction or abstraction. For example, A lice’s email message to Bob is a
context, as is Bob’s response: they both narrow the possible conversations that may ensue, but
usually do not determine them completely. A completed conve rsation, or a protocol run, is also
a context at a still higher level, where a sequence of convers ations may constitute a transaction.
Deceit and outsmarting arise when such level-above channel s are established covertly. For
example, Bob sends a message in the context of one conversati on, but that message may shift
145
the conversation into a di ﬀerent context, which Alice may or may not notice, and may resp ond
to it at the higher level, or remain at the lower level. Throug h a sequence of transactions, with
or without the level shifts and covert messages, Alice and Bo b may establish a covert context
of trust, separate from the overt contexts of any of the conve rsations.
9.4.3 Upshot
Fig. 9.14 shows the computational versions of the language s tack, from the carriers at the
bottom to the contexts at the top. The natural language stack on the left is echoed by the
programming language stack in the middle, which is echoed by the network stack on the right3.
Figure 9.16: Attacks are easy, level-below attacks are easier, level-above attacks are the easiest
9.5 What did we learn?
The takeaway ideas of this chapter are:
•Privacy is the right to be left alone.
•Data privacy prevents the information pull.
•Inﬂuence implements the action push.
•Deceit defeats privacy through level-above attacks.
But where do these ideas leave the tasks of the privacy protec tions in reality? For example,
implementing the data privacy in a network requires at least the guarantees that
a) the private data cannot be derived from the publicly relea sed data;
b) the publicly released data cannot be compiled and expande d through cross-referencing.
But requirement (a) is a typical example of an intractable lo gical problem. What is derivable
from what is demonstrated by constructing derivations. Wha t is not derivable is even harder
to characterize in general. Requirement (b) requires contr olling the information ﬂows, which
3The OSI model, usually taught in networks courses, separate s asession layer above the transport layer, and a
presentation layer below the application layer. A quick look at protocols shows that this is just a legacy quirk.
146
is a typical example of an intractable problem in a network. S o it seems that the data privacy
problem has no general solution, as it involves intractable problems.4
Yet in real life, we do manage to maintain certain levels of pr ivacy. Some say less and less,
some say there are new forms of data that remain private. Eith er way, there is empiric evidence
that privacy is not completely impossible.
4Diﬀerential privacy is a general method to protects the statistical databases, where the problems of cross-
referencing and cross-derivations are ﬁltered out togethe r.
147

Epilogue
Aloha Students of Security Science,
The grades have been entered. You have all done well. Thanks f or taking this course.
Interestingly, you seem to have coordinated the votes to eve nly distribute the points for the pre-
sentations. (The chance that everyone might get the same num ber of votes with no coordination
seems to be less than 1 in 8000.) This is obviously in the inter est of those who have invested
less work and could expect an outcome below the average, and a gainst the interest of those who
have invested more work, and could expect an outcome above th e average. The advantages and
disadvantages of egalitarian social contracts are well-kn own.
On the other hand, the agreement may be signaling that the uti lity of points for those who gained
some was greater than for those who lost some, in which case th e collusion has increased the
total utility. This allows us to close the course by repeatin g the side remark that we never had
a chance to fully develop:
Security and Economy are two sides of the same coin:
•a resource is an economic asset only if it can be secured, whil e
•a security protection is e ﬀective only if it is cost-e ﬀective.
For instance, the lion cannot claim a water well as his asset i f it is so big that he cannot prevent
the gazelle from drinking on the other side. A $100 lock is not an eﬀective protection of a $50
bike.
But valuations can vary wildly. My bike can be priceless to me . To prevent it from being stolen,
I might be willing to steal a lock. In a market economy, my goal is to minimize my costs,
maximize my revenue, and secure my proﬁts. The less I give and the more I take, the better
oﬀI am. But there are also social processes, such as love, relig ion, art, and science, where our
goal is to give as much as we can and take out as little as possib le. Many security failures arise
from confusing diﬀerent security goals that arise in di ﬀerent economic environments. To be
good security engineers and scientists, you must remember d o not waste money on the market
and to not save eﬀorts in love and science.
In any case, we respected your preferences and assigned the m aximum of 25 points to everyone.
Happy holidays!
– Peter and Dusko
149

Bibliography
[1] W. Aiello and et al S.M. Bellovin. Just fast keying: Key ag reement in a hostile internet.
ACM Trans. Inf. Syst. Secur. , 7(2):242–273, May 2004.
[2] Bowen Alpern and Fred B. Schneider. Recognizing safety a nd liveness. Distributed
Computing , 2:117–126, 1987.
[3] M.S. Alvim, K. Chatzikokolakis, A. McIver, C. Morgan, C. Palamidessi, and G. Smith.
The Science of Quantitative Information Flow . Information Security and Cryptography.
Springer, 2020.
[4] A. Angela and G. Conti. A Day in the Life of Ancient Rome . Europa Editions, 2009.
[5] Robert B. Ash. Information Theory . Dover Publications, 1990.
[6] Joe Bailey. From public to private: The development of th e concept of "private". Social
Research , 69(1):15–31, 2002.
[7] Thomas Bayes. An essay towards solving a problem in the do ctrine of chances. Philo-
sophical Transactions of the Royal Soceity. of London , 53:370–418, 1763.
[8] David E. Bell and Leonard J. LaPadula. Secure computer sy stems: Mathematical foun-
dations. Technical Report ESD-TR-73-278, V ol. I - IV , Elect ronic Systems Division, Air
Force Systems Command, 1973.
[9] Giampolo Bella. Formal Correctness of Security Protocols . Information Security and
Cryptography. Springer Berlin Heidelberg, 2007.
[10] Ken Biba. Integrity considerations for secure compute r systems. Technical report, The
Mitre Corporation, 06 1975.
[11] Colin Boyd and Anish Mathuria. Protocols for Authentication and Key Establishment .
Information Security and Cryptography. Springer, 2013.
[12] D.F.C. Brewer and M.J. Nash. The chinese wall security p olicy. In Proceedings. 1989
IEEE Symposium on Security and Privacy , pages 206–214, 1989.
[13] S. Burke. Delos: Investigating the Notion of Privacy Within the Ancie nt Greek House .
PhD thesis, University of Leicester, 2000.
[14] Jason Castiglione, Dusko Pavlovic, and Peter-Michael Seidel. Privacy protocols. In
Joshua Guttman et al., editor, Foundations of Security, Protocols, and Equational Rea-
soning , volume 11565 of Lecture Notes in Computer Science , pages 167–192. Springer,
2019.
151
[15] David D. Clark and David R. Wilson. A Comparison of Comme rcial and Military Com-
puter Security Policies . In Proceedings IEEE Symposium on Security and Privacy , pages
184–184, Los Alamitos, CA, USA, April 1987. IEEE Computer So ciety.
[16] Michael R. Clarkson and Fred B. Schneider. Hyperproper ties. J. of Computer Security ,
18(6):1157–1210, 2010.
[17] Diane Colombelli-Négrel, Mark E. Hauber, Jeremy Rober tson, Frank J. Sulloway, Herbert
Hoi, Matteo Griggio, and Sonia Kleindorfer. Embryonic lear ning of vocal passwords
in superb fairy-wrens reveals intruder cuckoo nestlings. Current Biology , 22(22):2155–
2160, December 2012.
[18] Thomas M. Cover and Joy A. Thomas. Elements of information theory . Wiley-
Interscience, New York, NY , USA, 1991.
[19] Cas Cremers and Sjouke Mauw. Operational Semantics and Veriﬁcation of Security Pro-
tocols . Information Security and Cryptography. Springer, 2012.
[20] Tore Dalenius. Towards a methodology for statistical d isclosure control. Statistik Tidskrift ,
15:429–444, 1977.
[21] Whitﬁeld Diﬃe and Martin E. Hellman. New directions in cryptography. IEEE Transac-
tions on Information Theory , 22(6):644–654, 1976.
[22] Whitﬁeld Diﬃe, Paul C. van Oorschot, and Michael J. Wiener. Authenticati on and au-
thenticated key exchanges. Designs, Codes, and Cryptography , 2:107–125, 1992.
[23] Cynthia Dwork. Di ﬀerential privacy. In Michele Bugliesi, Bart Preneel, Vladi miro Sas-
sone, and Ingo Wegener, editors, Proceedings of ICALP 2006, Part II , volume 4052 of
Lecture Notes in Computer Science , pages 1–12. Springer, 2006.
[24] Cynthia Dwork and Aaron Roth. The algorithmic foundati ons of diﬀerential privacy.
Foundations and Trends in Theoretical Computer Science , 9(3-4):211–407, 2014.
[25] David Evans, Vladimir Kolesnikov, and Mike Rosulek. A Pragmatic Introduction to Se-
cure Multi-Party Computation . Foundations and Trends ®in Privacy and Security Series.
Now Publishers, 2018.
[26] Richard P. Feynman. The character of physical law , volume 66. MIT Press, Cambridge
MA, USA, 1965.
[27] Ronald A. Fisher. Inverse probability. Proceedings of the Cambridge Philosophical So-
ciety , 28:528–535, 1930.
[28] S. Frankel and S. Krishnan. IP Security (IPsec) and Inte rnet Key Exchange (IKE) Docu-
ment Roadmap. RFC 6071, February 2011.
[29] D. Harkins and D. Carrel. The Internet Key Exchange (IKE ). RFC 2409, November 1998.
152
[30] Michael A. Harrison, Walter L. Ruzzo, and Je ﬀrey D. Ullman. Protection in Operating
Systems. Communications of the ACM , 19(8):461–471, August 1976.
[31] Burton S. Kaliski. An unknown key-share attack on the MQ V key agreement protocol.
ACM Trans. Inf. Syst. Secur. , 4(3):275–288, August 2001.
[32] Neal Koblitz and Alfred Menezes. Critical perspective s on provable security: Fifteen
years of “another look” papers. Advances in Mathematics of Communications , 13(4):517–
558, 2019.
[33] Hugo Krawczyk. SKEME: A versatile secure key exchange m echanism for internet. In
Proceedings of Internet Society Symposium on Network and Di stributed Systems Security ,
pages 114–127. IEEE, 1996.
[34] Hugo Krawczyk. HMQV: A High-Performance Secure Di ﬃe-Hellman Protocol. In Victor
Shoup, editor, CRYPTO 2005 , pages 546–566, Berlin, Heidelberg, 2005. Springer.
[35] Thomas S. Kuhn. The Structure of Scientiﬁc Revolutions . University of Chicago Press,
2012.
[36] Leslie Lamport. Proving the correctness of multiproce ss programs. IEEE Transactions
on Software Engineering , SE-3(2):125–143, 1977.
[37] Laurie Law, Alfred Menezes, Minghua Qu, Jerome A. Solin as, and Scott A. Vanstone. An
eﬃcient protocol for authenticated key agreement. Designs, Codes and Cryptography ,
28:119–134, 2003.
[38] Gavin Lowe. An attack on the Needham-Schroeder public- key authentication protocol.
Information Processing Letters , 56:131–133, 1995.
[39] Gavin Lowe. A hierarchy of authentication speciﬁcatio ns. In Proceedings of the 10th
IEEE Computer Security Foundations Workshop , pages 31–43. IEEE, 1997.
[40] David J.C. MacKay. Information Theory, Inference and Learning Algorithms . Cambridge
University Press, 2003.
[41] Catherine Meadows and Dusko Pavlovic. Deriving, attac king and defending the GDOI
protocol. In Peter Ryan, Pierangela Samarati, Dieter Gollm ann, and Reﬁk Molva, edi-
tors, Proceedings of ESORICS 2004 , volume 3193 of Lecture Notes in Computer Science ,
pages 53–72. Springer Verlag, 2004.
[42] Catherine Meadows, Paul Syverson, and Iliano Cervesat o. Formalizing GDOI group key
management requirements in NPATRL. In Proceedings of the 8th ACM CCS , CCS ’01,
pages 235–244, New York, NY , USA, 2001. ACM.
[43] Alfred Menezes. Another look at hmqv. Journal of Mathematical Cryptology , 1(1):47–64,
2007.
153
[44] Michael D. Needham, Roger M.; Schroeder. Using encrypt ion for authentication in large
networks of computers. Communications of the ACM , 21:993–999, 1978.
[45] Lena C. Orlin. Locating Privacy in Tudor London . Oxford University Press, 2009.
[46] H. Orman. The OAKLEY Key Determination Protocol. RFC 24 12, November 1998.
[47] Vilfrido Pareto. Considerations on the Fundamental Principles of Pure Polit ical Econ-
omy. Routledge Studies in the History of Economics. Taylor & Fra ncis, 2007.
[48] Lawrence C. Paulson. Mechanized proofs for a recursive authentication protocol. In
Proceedings of CSFW ’97 , CSFW, page 84, USA, 1997. IEEE Computer Society.
[49] Dusko Pavlovic. Lambek pregroups are Frobenius spider s in preorders. Compositionality ,
4(1):1–21, 2022. arxiv.org /abs/2105.03038.
[50] Dusko Pavlovic. Language processing in humans and comp uters. CoRR ,
arxiv.org/abs/2405.14233, May 2024.
[51] Karl R. Popper. Conjectures and Refutations: The Growth of Scientiﬁc Knowl edge . Clas-
sics Series. Routledge, 2002.
[52] Peter Ryan and Steve Schneider. The Modelling and Analysis of Security Protocols: The
CSP Approach . Addison-Wesley, 2001.
[53] P.Y .A. Ryan and S. A. Schneider. An attack on a recursive authentication protocol. a
cautionary tale. Inf. Process. Lett. , 65(1):7–10, January 1998.
[54] Ferdinand D. Schoeman. Philosophical Dimensions of Privacy: An Anthology . Cam-
bridge University Press, 1984.
[55] Claude E. Shannon. A mathematical theory of communicat ion.The Bell System Technical
Journal , 27:379–423 and 623–656, 1948.
[56] Claude E. Shannon. Communication theory of secrecy sys tems. The Bell System Technical
Journal , 28(4):656–715, 1949.
[57] Gustavus J. Simmons. Cryptanalysis and protocol failu res.Communications of the ACM ,
37:56–65, 1994.
[58] Latanya Sweeney. Weaving technology and policy togeth er to maintain conﬁdentiality.
Journal of Law, Medicine and Ethics , 25:98–110, 1997.
[59] Latanya Sweeney. Achieving k-anonymity privacy prote ction using generalization and
suppression. International Journal of Uncertainty, Fuzziness and Knowl edge-Based Sys-
tems, 10(5):571–588, 2002.
[60] Latanya Sweeney. k-anonymity: A model for protecting p rivacy. International Journal of
154
Uncertainty, Fuzziness and Knowledge-Based Systems , 10(5):557–570, 2002.
[61] B. Weis, S. Rowles, and T. Hardjono. The Group Domain of I nterpretation. RFC 6407,
October 2011.
155

Appendix A. Prerequisites
1 List and string constructors and notations
•lists: X∗=/braceleftig/parenleftbigx1x2...xn/parenrightbig∈Xn|n=0,1,2,.../bracerightig
•strings: X+=/braceleftig/parenleftbigx1x2...xn/parenrightbig∈Xn|n=1,2,3.../bracerightig
The only diﬀerence between the type X∗of lists of elements of Xand the type X+of strings
of elements of Xis that X∗contains the empty list/parenleftbig /parenrightbig, whereas X+does not. Strings are the
nonempty lists, whereas a list is either a string or empty:
X∗=X+∪/braceleftig
( )/bracerightig
Lists are inductively generated by the list constructor (:: ) and the empty list ():
X∗×X(::)−−−→ X∗()←−−1 (1)/angbracketleftig/parenleftbigx0...xn/parenrightbig,xn+1/angbracketrightig
∝mapsto−→/parenleftbigx0...xnxn+1/parenrightbig
/parenleftbig /parenrightbig←−/mapsfromchar∅
whereas strings are generated by the string constructor (:: ) and the letter inclusion ( −) operation:
X+×X(::)−−−→ X+(−)←−−− X (2)/angbracketleftig/parenleftbigx0...xn/parenrightbig,xn+1/angbracketrightig
∝mapsto−→/parenleftbigx0...xnxn+1/parenrightbig
/parenleftbigx/parenrightbig←−/mapsfromcharx
Notation. We write the names of lists and strings in bold1
x=/parenleftbigx0x1x2...xn/parenrightbig
When convenient, the indices can also be written right to lef t, i.e., x=/parenleftbigxnxn−1...x1x0/parenrightbig.
Induction
Inductive constructors allow inductive deﬁnitions. Here a re a couple of basic examples.
1Functional programmers write xs=/parenleftbigx1x2...xn/parenrightbig
157
List concatenation. The list concatenation appends two lists
X∗×X∗@−→ X∗( )←−1/angbracketleftbigz,( )/angbracketrightbig∝mapsto−→ z/angbracketleftbigz,y::x/angbracketrightbig∝mapsto−→ (z@y)::x
Since concatenation is obviously associative, it makes X∗into a monoid, with the empty string
() as the unit. More precisely, X∗is the free monoid over X, whereas X+is the free semigroup.
The monoid operations easily extend from lists to the sets of lists
X∗×X∗@−→X∗( )←−1
℘(X∗)×℘(X∗)@−→℘(X∗){( )}←−− 1
by deﬁning the concatenation of C,D⊆X∗to be the set of concatenations of their elements
CD=C@D={c@d∈X∗|c∈C,d∈D}
List length. The simplest inductive deﬁnition assigns to each list the nu mber of its symbols:
X∗ℓ− →N
()∝mapsto→0
y::x∝mapsto→ℓ(y)+1
Note thatN/simequal{1}∗is the free monoid over one generator, with the operation of a ddition onN
corresponding to the concatenation of {1}∗, providing the arithmetic in base 1. The list length
operation can thus be viewed as the monoid homomorphism X∗→{1}∗induced by the unique
function X→1, identifying all symbols of the alphabet Xwith a single symbol.
Abbreviations. When no confusion is likely we abbreviate, not just C@DtoCDas above,
but also x@ytox::y, and even xy.
Preﬁx order. Thepreﬁx relation⊑deﬁned by
x⊑y⇐⇒ ∃ z.x::z=y (3)
is a partial order, both on lists and on strings. Writing x⊑yand saying that xis a preﬁx of y
means that there is z=/parenleftbigz1...zn−k/parenrightbigsuch that
/parenleftbigx1x2...xkz1...zn−k/parenrightbig
=/parenleftbigy1y2...ykyk+1...yn/parenrightbig
so that xi=yifori≤kandzi=yi+kfori≤n−k. This partial ordering makes any set of lists
X∗into a meet semilattice, with the meet x⊓yextracting the greatest common preﬁx of xand
y. The set of strings X+is not a semilattice just because the greatest common preﬁx o f strings
158
starting diﬀerently is the empty list, which is not a string.
Specifying properties. For any pair of events a,b∈Σ, and for arbitrary sets of events
C,D⊆Σ, some of the properties that can be deﬁned are:
a={x::a::y|x,y∈Σ∗}
a≺b={x::a::y::b::z|x,y,z∈Σ∗}
a≺∃b=/braceleftbigt∈Σ∗|∃xy.t=x::a::y=⇒ ∃ y′y′′.y=y′::b::y′′/bracerightbig
∃a≺b=/braceleftbigt∈Σ∗|∃xy.t=x::b::y=⇒ ∃ x′x′′.x=x′::a::x′′/bracerightbig
C={x::c::y|x,y∈Σ∗,c∈C}
C≺D={x::c::y::d::z|x,y,z∈Σ∗,c∈C,d∈D}
C≺∃D=/braceleftbigt∈Σ∗|∀c∈C.t=x::c::y=⇒ ∃ d∈D.y=y′::d::y′′/bracerightbig
∃C≺D=/braceleftbigt∈Σ∗|∀d∈D.t=x::d::y=⇒ ∃ c∈C.x=x′::c::x′′/bracerightbig
2 Neighborhoods
Neighborhoods. Topology is the most general theory of space. Spaces usually model the
realms of observations, and their structures therefore cor respond to what is observed: e.g., met-
ric spaces capture distances, vector spaces also capture an gles, etc. Topological spaces only
capture neighborhoods , viewed as sets that are inhabited by some objects together. Smaller
neighborhoods suggest that the objects are closer together , and the set inclusion of neighbor-
hoods thus tells which objects are closer together, and whic h ones are further apart. But these
distinctions are made without assigning any numeric values to the distances between objects,
as it is done in metric spaces. Metric spaces can thus be viewe d as a special case of topological
spaces.
For any set E, a (topological) space structure is deﬁned by either of the f ollowing:
•open neighborhoods are represented by a family OE⊆℘(E) closed under ﬁnite ∩and
arbitrary/uniontext; whereas
•closed neighborhoods are represented by a family FE⊆℘(E) closed under ﬁnite ∪and
arbitrary/intersectiontext
The two families determine each other, in the sense that a set is open if and only if its comple-
ment is closed, i.e.
OE=/braceleftbigU∈℘(E)|¬U∈F E/bracerightbigFE=/braceleftbigF∈℘(E)|¬F∈O E/bracerightbig
Any familyBE⊆℘(E) can be declared to be open (or closed) neighborhoods, and us ed to
generate the full space structure. If we want BEto be
159
•open, then set
B∩
E=n/intersectiondisplay
i=0Bi|B0,..., Bn∈B EandOE=/braceleftig/uniondisplay
V|V⊆B∩
E/bracerightig
•closed, then set
B∪
E=n/uniondisplay
i=0Bi|B0,..., Bn∈B EandFE=/braceleftig/intersectiondisplay
V|V⊆B∪
E/bracerightig
Closure operators. For any given topology on E, the familyFEcan be equivalently presented
in terms of the closure operator that it induces:
∇:℘(E)→℘(E) (4)
X∝mapsto−→/intersectiondisplay
{F∈F E|X⊆F}
It is easy to see that the general requirements from a closure operator are satisﬁed:
X⊆ ∇X=∇∇X
andFE={X∈℘(E)|∇X=X}.
Interior operators are dual to closure operators. This means that for any given t opology on E,
the familyOEof open sets (dual to the closed sets FE) can be equivalently presented in terms
of the interior operator that it induces:
∆:℘(E)→℘(E) (5)
X∝mapsto−→/uniondisplay
{U∈O E|U⊆X}
It is easy to see that the general requirements from an interi or operator are satisﬁed:
X⊇∆X= ∆∆ X
andOE={X∈℘(E)|∆X=X}.
Closures and interiors determine each other by
∇X=¬∆¬X ∆X=¬∇¬ X
Density can be deﬁned by saying for X,Y∈℘(E) that X is dense on Y if∇X⊇Y. This can
be equivalently written in the form Y∩∆¬X=∅. We sometimes describe such situations by
saying that¬Y is codense on¬X. In general, Uis codense on Vwhen/integraltext
V\U=∅. The family
160
of sets that are dense on Y, or on which Yis codense, is
DY=/braceleftbigD∈℘(Y)|∀F∈F E.D⊆F=⇒Y⊆F/bracerightbig
=/braceleftbigD∈℘(Y)|∀U∈O E.Y∩U/nequal∅=⇒D∩U/nequal∅/bracerightbig
Closed-dense∩-decomposition. Any inclusion X⊆Eclearly factors as X⊆∇X⊆E
where∇X∈ F Eis closed, and X∈ D∇Xis relatively dense. Therefore, any X⊆Eis an
intersection of a closed and a dense set
X=∇X∩(X∪¬∇ X)
where∇X∈F Eis closed, and X∪¬∇ X∈D Eis dense everywhere.
Open-codense∪-decomposition is induced by the closed-dense ∩-decomposition of the
complement¬X⊆Ethrough∇¬X=¬∆X⊆E. More precisely¬X=∇¬X∩(¬X∪¬∇¬ X)=
¬∆X∩¬(X∩¬∆X)becomes
X= ∆ X∪/parenleftbigX\∆X/parenrightbig
where∆X∈O Eis open, and X\∆Xis codense.
3 Images, cylinders and cylindriﬁcations
Direct and inverse images. Any function f:A→Binduces two direct image maps and one
inverse image map:
f!:℘A−→℘B
X∝mapsto−→ { y|∃x.f(x)=y∧x∈X}
f∗:℘B−→℘A
V∝mapsto−→ { x|f(x)∈V}
f∗:℘A−→℘B
X∝mapsto−→ { y|∀x.f(x)=y⇒x∈X}
It is easy to show that they satisfy
f!(X)⊆Y⇐⇒ X⊆f∗(Y) and
f∗(Y)⊆X⇐⇒ Y⊆f∗(X)
and hence for any X∈℘Aholds
f∗f∗(X)⊆X⊆f∗f!(X)
161
Cylinders. We call f∗f∗(X) the internal f -cylinder contained in X, whereas f∗f!(X) is the
external f -cylinder containing X. An external f-cylinder is thus the smallest inverse image of
a set in Balong fthat contains X, whereas an internal f-cylinder is the largest inverse image
of a set in Balong fthat is contained in X.
Cylinder closures and interiors. Suppose that we are given a family of functions V={v:
A→Bv|v∈V}, whereVis an arbitrary set of indices. We consider such families as cylin-
der localizations : we build v-cylinders, internal and external, for all v∈V, and approximate
arbitrary sets using cylinders.
B0B1
X
∇X
B0B1
XX
∆X
Figure 1: Cylinder closure and interior
Thecylinder closure andcylinder interior operators are deﬁned by
/dblbracketleft−/dblbracketright:℘A→℘A (6)
X∝mapsto−→/dblbracketleftX/dblbracketright=/intersectiondisplay
v∈Vv∗v!(X)
/llparenthesis−/rrparenthesis:℘A→℘A (7)
X∝mapsto−→/llparenthesisX/rrparenthesis=/uniondisplay
v∈Vv∗v∗(X)
where the sets
/dblbracketleftX/dblbracketrightv=v∗v!(X)={z∈A|∃x.v(z)=v(x)∧x∈X}
/llparenthesisX/rrparenthesisv=v∗v∗(X)={z∈A|∀x.v(z)=v(x)⇒x∈X}
are respectively the external v-cylinders around Xand the internal v-cylinders inside X. It is
easy to see that the /dblbracketleft−/dblbracketrightis a closure operator and that /llparenthesis−/rrparenthesisis an interior operator, which means
that they satisfy
/llparenthesis/llparenthesisX/rrparenthesis/rrparenthesis=/llparenthesisX/rrparenthesis/dblbracketleft/dblbracketleftX/dblbracketright/dblbracketright=/dblbracketleftX/dblbracketright
/llparenthesisX/rrparenthesis⊆X⊆/dblbracketleftX/dblbracketright
162
They are complementary in the sense
¬/dblbracketleftX/dblbracketright=/llparenthesis¬X/rrparenthesis¬/llparenthesisX/rrparenthesis=/dblbracketleft¬X/dblbracketright
which means that they determine each other:
/dblbracketleftX/dblbracketright=¬/llparenthesis¬X/rrparenthesis /llparenthesis X/rrparenthesis=¬/dblbracketleft¬X/dblbracketright
Deﬁnition A.1. X∈℘Aisexternal cylindric ifX=/dblbracketleftX/dblbracketrightandinternal cylindric ifX=/llparenthesisX/rrparenthesis.
Lemma A.2. Any history tof eventsΣ=/coproducttext
w∈WΣwis completely determined by
•the restrictions t↾wfor w∈W, and
•their schedule , which is function s :|t|→Wsuch that
s(k)=w⇐⇒ tk∈Σw (8)
where t=/parenleftbigt0t1...tn/parenrightbigand|t|=n+1.
Proof . The claim is that the function s:|t|→Wsatisfying (8) provides enough information to
reconstruct tfrom its restrictions t↾w, given for all w∈W. The restrictions can be scheduled
in many ways, but (8) says that k-th component tkoftcomes from the restriction t↾s(k). To
simplify notation, we write t↾s(k)asτ(k), so thatσ-th component of t↾s(k)becomesτ(k)
σ. So we
know that tk=τ(k)
σfor someσ. The question is: what isσ?
Towards the answer, we use s:|t|→Wto deﬁne the function ς:|t|×S→|t|+1 which counts
the number of w-actions in the k-length preﬁx of t. The deﬁnition is by recursion:
ς(0,w)=1 if s(0)=w
0 otherwise
ς(n+1,w)=ς(n)+1 if s(n+1)=w
ς(n) otherwise
It is easy to show by induction that ς(k,w) is the length of the w-restriction of the k-preﬁx tk⊑t,
i.e.,ς(k,w)=|tk↾w|. Hence, tk=τ(k)
ς(k,s(k))is thus theς(k,s(k))-th component of τ(k)=t↾s(k)./boxempty
Proposition A.3. A property P is localized if and only if together with every tit contains all s
such that t↾w=s↾wfor all w∈W.
Localization. In general, the smallest local property containing P⊆Σ∗with respect to the
163
partitionΣ=/coproducttext
i∈IΣiis
/hatwideP={t∈Σ∗|∀i∈I.t↾i∈Pi}
=/intersectiondisplay
i∈I/hatwidePiwhere /hatwidePi={t∈Σ∗|t↾i∈Pi}
The property /hatwidePis called the localization ofP. It is easy to see that P⊆/hatwideP=/hatwide/hatwideP, i.e., that
localization is a closure operator. The property Pis thus localized when P=/hatwideP.
164
Index
access control, 21, 23
capability-based, 25
lists, 25
matrix, 23
model, 23
requirements, 24
types, 23
UNIX, 25
access matrix, 24
access policy, 26
access policy authority, 24
actions, 24
asset, 22
authentication, 85
agreement, 101
by matching conversation, 101
general framework, 86
mutual, 94, 99
of wren chicks, 86
ping, 101
authenticity, 15, 17, 18, 93, 94
authority, 15, 18
authorization, 15, 18
availability, 15, 18
bad stuﬀ, 11, 13, 15
Bayesian law, 117
biometric feature, 14
causation, 70
channel, 65, 84
bundled purge, 82
communication, 84
composition, 89
conﬁdential, 95
continuous, 69, 81, 110
sequent notation, 69
continuous possibilistic, 69
continuous probabilistic, 110covert, 69, 78
cumulative, 67
sequent notation, 69
deterministic, 66
generative, 115
inference, 70
information, 111
interference, 73, 77, 120
inverse, 70
local view, 76
memoryless, 66, 68
overt, 69, 71
point-to-point, 84
possibilistic, 108
see relational, 66
probabilistic, 84, 108
purge, 76, 82
relational, 66
sequent notation, 66
shared, 76, 119
unsafe, 72
chatbot, 67
hallucination, 72
CIA-triad, 15
cipher, 96, 122
classiﬁcation, 26
clearance, 26, 75
relation, 76
scope, 76
type, 76
closure, 160
cylinder, 162
communication, 83
complementary purge, 79
concatenation, 158
conﬁdentiality, 15, 18
cryptosystem, 96
cylinder, 162
165
data, 13
data security, 15
datatype, 91
dependability, 17, 39
detection, 11
deterrence, 11
entropy, 111
event, 35
forensics, 11
freedom, 15
good stuﬀ, 11, 13, 15
health, 15
history, 35
global, 76
local, 76
strictly local, 42
image, 161
direct, 161
inverse, 161
inductive inference, 70
integrity, 15, 18, 93
interference, 72, 75
channel, 77, 120
interior, 160
cylinder, 162
intrusion detection, 11
invariance, 81
item, 24
k-anonymity, 139
know-have-be tripod, 14
label, 24
list, 157
constructor, 157
diﬀerence from string, 157
empty, 157
liveness, 11, 17, 39, 40
locality, 26, 75
strict, 42
localization
strict, 43matrix
relational, 108
stochastic, 108
MLS, 21, 27
Monty Hall problem, 118
network, 63
computation, 90
non-repudiation, 17
nonce, 98
noninterference, 79, 120
objects, 23
observer
global, 90
permission matrix, 24
phrenology, 71
physical security, 15
preﬁx order, 158
prevention, 11
probability
conditional, 114
projector, 81, 82
properties
dependability, 11
security, 15
property, 35
localized, 57
strictly localized, 44
trivial, 35
protocol, 83, 90
and grammar, 91
authentication, 85, 94
challenge-response, 92
key distribution, 98
mutual authentication, 98
Needham-Schroeder Public Key (NSPK),
97
Needham-Schroeder Public-Key (NSPK),
99
Needham-Schroeder Symmetric-Key (NSSK),
99
run, 105
security, 95
supermarket, 91
166
purge, 57, 82
complementary, 79
general, 76
strict, 43
relation, 66
resource, 21
resource security, 15
safety, 11–13, 17, 39
sampling, 108
secrecy, 15, 18, 122
perfect, 123
security, 12
classiﬁcation, 26
clearance, 26, 27
levels, 27
location, 27
multi-level, 21, 27
static, 21
semantics, 91
sequent, 66
source, 115
state, 23
string, 157
constructor, 67, 157
diﬀerence from list, 157
letter inclusion, 157
subjects, 23
syntax, 91, 105
system monitor, 24
thing, 14
tokens vs words, 67
traﬃc and communication, 83
trait, 14
Trojan horse, 78
Turing Test, 94
type
checking, 91
data, 91
matching, 91
syntactic, 92
uncertainty, 108
unsafety, 39of a channel, 72
user, 24
view
strictly local, 43
words vs tokens, 67
world, 75
state of the world, 76
worldview, 76, 119
167