MOSAIC : A Skill-Centric Algorithmic Framework
for Long-Horizon Manipulation Planning
Itamar Mishani, Yorai Shaoul, and Maxim Likhachev
Robotics Institute, School of Computer Science, Carnegie Mellon University
{imishani, yshaoul, maxim }@cs.cmu.edu
Fig. 1: M OSAIC solves long-horizon manipulation tasks by generating local skill trajectories (circles) and connecting those
with connector skills (squares). M OSAIC capitalizes on the skills themselves to guide the exploration process toward regions
where skills are likely to succeed – enabling effective composition of generic local skills to solve complex tasks.
Abstract —Planning long-horizon motions using a set of pre-
defined skills is a key challenge in robotics and AI. Address-
ing this challenge requires methods that systematically explore
skill combinations to uncover task-solving sequences, harness
generic, easy-to-learn skills (e.g., pushing, grasping) to gen-
eralize across unseen tasks, and bypass reliance on symbolic
world representations that demand extensive domain and task-
specific knowledge. Despite significant progress, these elements
remain largely disjoint in existing approaches, leaving a critical
gap in achieving robust, scalable solutions for complex, long-
horizon problems. In this work, we present M OSAIC , a skill-
centric framework that unifies these elements by using the skills
themselves to guide the planning process. M OSAIC uses two
families of skills: Generators compute executable trajectories and
world configurations, and Connectors link these independently
generated skill trajectories by solving boundary value prob-
lems, enabling progress toward completing the overall task. By
breaking away from the conventional paradigm of incrementally
discovering skills from predefined start or goal states—a limi-
tation that significantly restricts exploration—M OSAIC focuses
planning efforts on regions where skills are inherently effective.
We demonstrate the efficacy of M OSAIC in both simulated and
real-world robotic manipulation tasks, showcasing its ability to
solve complex long-horizon planning problems using a diverse
set of skills incorporating generative diffusion models, motion
planning algorithms, and manipulation-specific models. Visit
skill-mosaic.github.io for demonstrations and examples.
I.INTRODUCTION
Recent progress in robotics has made large strides toward
enabling robots to learn and execute complex manipulationskills. Tasks that were widely regarded as extremely difficult
to perform autonomously only a few years ago, like non-
prehensile reorientation of irregularly shaped objects, can now
be taught to a robot in a matter of hours and be executed
with high confidence under some conditions. However, as we
move towards incorporating robots into unstructured domains
such as homes, offices, and dynamic environments, we seek
to solve longer-horizon manipulation tasks and need to utilize
skills that were not necessarily trained for the given condi-
tions. For instance, imagine a robot tasked with tidying up
a cluttered dining table. We want the robot to autonomously
discover how to reorient and reposition plates for stable grasps,
avoid wet regions that may affect performance, and execute
a coordinated sequence of sweeping motions to complete the
cleanup. Scenarios like this may require solutions capable of
reasoning over long horizons and composing a diverse set of
imperfect skills – reasoning not only about the composition
of skills but also about the conditions that allow them to
be executed with high success rates. To date, we believe
that no framework comprehensively addresses this problem,
leaving robots primarily confined to controlled environments
like warehouses or factories, where their motions are typically
prescribed, despite significant advancements in skill learning.
Existing approaches typically address this challenge through
one of two extremes: Task and Motion Planning (TAMP)
or full-horizon policy learning. Policy learning techniques,arXiv:2504.16738v1  [cs.RO]  23 Apr 2025
which often use data-driven methods such as imitation learning
and reinforcement learning [15, 6], have shown success in
solving short-horizon tasks but struggle with long-horizon
reasoning and skill composition. Potential reasons for this
are that, when attempting to use policy learning methods
for long-horizon tasks, their task-specific data requirements
grow significantly [12] and trained policies typically gener-
alize poorly to new scenarios without substantial retraining.
These limitations hinder their applicability to solving tasks
that demand reasoning across long horizons and performing
multiple complex motions.
TAMP frameworks [13] operate on symbolic world states
and transitions, allowing planners to reason hierarchically:
plan at an abstract “high-level” and refine plans into executable
“low-level” motions. While effective in many cases, TAMP’s
reliance on explicit discrete symbolic representations imposes
significant limitations [23]. For example,
the symbolic world representation must be sufficiently fine
to distinguish between two semantically similar (e.g., the plate
is on the table) but geometrically different states (e.g., the
plate can or cannot be grasped). Such symbolic representations
used in TAMP often require extensive domain knowledge and
manual engineering, making TAMP less suited for novel tasks
or environments lacking predefined symbolic abstractions.
In this work, we introduce a new algorithmic approach
for long-horizon skill-based planning that substantially de-
viates from previous work. In contrast to prior works, our
algorithmic framework treats skills as key stakeholders in the
planning process for discovering both the sequence of skills
and their corresponding parameters. It does so by directing
exploration towards regions where imperfect skills perform
better, leveraging what we call generator skills to propose
novel world configurations and trajectories without specified
goals, and utilizing what we call connector skills to generate
transition between these configurations by solving boundary
value problems. This strategy enables our framework to forgo
task-specific symbolic abstractions and the need for initiation,
termination, and effect sets, allowing the composition of
generic skills to solve long-horizon manipulation tasks.
Our contributions are threefold:
•We propose a new framework for long-horizon skill-
centric planning and complement it with a theoretical
foundation defining algorithmic completeness and skill
properties.
•We introduce M OSAIC , a skill-centric planning algo-
rithm that efficiently composes generic local manipulation
skills to solve long-horizon tasks, leveraging both goal-
conditioned and goal-agnostic skills.
•We demonstrate the efficacy of M OSAIC across diverse
simulated environments and real-world robotic manipu-
lation tasks, showcasing its ability to generalize across
tasks and scale to complex scenarios.
Through these contributions, M OSAIC offers a powerful al-
ternative for skill-based planning. This paper presents an initial
exploration of a new family of algorithms that we hope would
push the boundaries of long-horizon robotic manipulation andpave the way for robust and scalable solutions to complex
planning problems that utilize skills.
II. R ELATED WORK
This section situates M OSAIC within the landscape of prior
work. We categorize relevant prior efforts into three main
areas: Task and Motion Planning, Single Policy Methods, and
Skill-Based Methods.
Task and Motion Planning (TAMP) addresses long-
horizon planning problems by interleaving abstract symbolic
task planning with geometric motion planning. TAMP systems
require users to define a symbolic task specification and a
symbolic world state that can be augmented by actions [13].
These systems are powerful but require manual engineering
and privileged task-specific information for determining the
symbolic representations and available transitions [7]. While
there is a significant body of work that aims to learn different
TAMP modules to avoid manual engineering [19, 24, 17, 34,
25, 29, 21, 9], this approach introduces its own engineering
challenges in designing effective learning systems around the
classical TAMP structure. These challenges include designing
appropriate learning modules that interface with existing sym-
bolic planners and collecting training data to learn accurate
symbolic abstractions. M OSAIC takes a different approach by
eliminating the need for symbolic task specifications, oper-
ating directly with learned skills without requiring additional
domain-specific knowledge.
Single Policy Methods aim to learn one policy that can
solve tasks under a specified goal condition. Janner et al.
[15] and Chi et al. [6], use diffusion models to learn task-
specific policies. These models have shown impressive success
in manipulation tasks but face challenges in generalization to
unseen tasks and in long-horizon reasoning. On the reinforce-
ment learning side, Florensa et al. [10] and Forestier et al.
[11] explore automatic goal generation in curriculum learning
for guiding a learning process via subgoals, which allows the
agent to effectively explore and make incremental progress
toward a long-horizon task goal. By doing so they incorporate
domain knowledge via a structural prior into the learning
process for aiding agents focus their exploration within a
prohibitively large space of possible actions that exponentially
explodes with the task horizon. Veeriah et al. [33] focuses
on the learning process as well, proposing an approach for
discovering useful options in multi-task RL environments for
learning an agent policy faster. Methods in this category focus
on making the full-horizon learning problem, which is data-
intensive and temporally complex, more tractable by augment-
ing the learning process with task-specific information. In
contrast, M OSAIC focuses on leveraging skill-primitives that
are task-agnostic and easy to learn for solving different long-
horizon tasks.
Skills-based Methods focus on composing skills (e.g.,
learned policies, analytic controllers, etc.) to solve long-
horizon tasks. The main challenges these methods address
arewhich skills to choose within a sequential composition
(a discrete problem) and what parameters to assign to these
skills (often a continuous problem). A central method in this
space is skill chaining by Konidaris and Barto [18], which
builds on the options framework in Reinforcement Learning
(RL) [32] by incrementally discovering and learning local
skill policies that are invoked iteratively to achieve subgoals
within an RL agent’s rollout [18, 2, 3, 4, 37]. Others, like
Nasiriany et al. [26], explicitly select skills and parameters
with dedicated policies and a finite skill library. However, the
explicit sequential nature of these frameworks forces skills to
define initiation, termination, and often effect sets that limit
their generality.
Sivaramakrishnan et al. [30] introduced a motion planning
approach for vehicles using learned dynamics that, despite not
being originally applied to planning with skills, can be seen
as a step towards relaxing the restrictive requirements on skill
definition. Their method first learns a goal-conditioned vehicle
dynamics policy in an obstacle-free space, then, for a given
environment, it generates a roadmap where vertices represent
vehicle configurations and edges indicate feasible connections
via the learned controller. At test time this roadmap serves
as guidance for forward sampling-based planning. Lu et al.
[22] also uses sampling-based planners to explore the abstract
space of higher-order skills in non-episodic lifelong RL.
A common thread among most of the proposed methods
is the directionality of their reasoning. In forward -reasoning
methods, skills and their parameters are usually chosen from
the space dictated by termination sets of previously selected
skills. Similarly, backward -reasoning methods train policies
by choosing skills that terminate at initialization sets of later
skills. Both schemes make it challenging to identify skills or
parameters that lie far from the currently explored set, yet
can be executed by the robot with high confidence and are
relevant to the overall task. M OSAIC takes a fundamentally
different approach by making the planning process skill-
centric. Specifically, the planner iterates between generating
“know-how-to-execute” skills and trying to connect these skills
in a way that progresses toward solving the overall long-
horizon planning problem.
III. P ROBLEM DEFINITION : SKILL -CENTRIC PLANNING
The skill-centric planning problem is a fundamental and
practical problem in robotics. It seeks to leverage a collection
of known skills to compute a trajectory for a robot whose
execution would modify a world state from its current speci-
fication to a target specification.
Formally, let Q ⊆Rndenote the configuration space of
a robot with ndegrees-of-freedom (DOF), e.g. 6-DOF for a
robotic manipulator with 6joints, and let X ⊆Rmbe the
state space of the planning problem, often called the world
state , where n≤m. To facilitate set membership checks for
states (e.g., whether a state is a goal state or belongs to some
equivalence class), let us define ξ:X → { 0,1}asbinary
conditions ξ∈Ξ. We define a trajectory τto be a mapping
τ: [0,1]→ X , and Tbe the trajectories space. We name the
space of parameterized skills , i.e., motor controllers that mapskill parameters θto trajectories τ∈T[1], the action space
A.
Our definition of parameterized skills can be seen as modi-
fiedoptions used by the options framework [32]. We define a
skillσas a temporally extended action executed by a policy
and described by the tuple (πσ,Θσ).πσis the policy of skill
σthat returns the probability of taking action ain state x
given input parameter θ∈Θσviaπσ(a|x, θ).Θσis the skill’s
parameter space. Action amay represent low-level control
commands (e.g., joint velocities) or higher-level trajectory
segments (e.g., a sequence of waypoints), depending on the
skill’s granularity. In our formulation, the output of a skill
is a trajectory τ∈T. Importantly, we do not restrict skills
to use a single policy to produce trajectories τ. Skills may
use probabilistic policies (e.g., a neural network-based policy
such as Diffusion Policy [6]), deterministic algorithms (e.g., a
collision-free motion planners), or hybrid methods combining
multiple modules. Within this framework, we introduce two
types of skills:
Definition 1. AGenerator , denoted as Gi, is a parameterized
skill that generates trajectories without requiring specified
start and goal states.
Gi: ΘGi→T
Definition 2. AConnector , denoted as Cj, is a conditional
parameterized skill that generates trajectories conditioned on
a specified start and goal conditions.
Cj: Ξ×Ξ×ΘCj→T
A common start and goal condition for connector skills is an
equality condition to given states x′andx′′. Therefore, for
brevity when the context is clear, we interchangeably write
Cj:X × X × ΘCj→T
and imply the conditions 1{x′}and1{x′′}.
With these definitions, we formally define the action space
asA=S
iGi∪S
jCj. Given a start state xstartand goal condi-
tion function ξgoal:X → { 0,1}the objective of the planning
problem is to find a sequence of Nskills{σi|σi∈ A}N
i=1
and associated parameters {θi|θi∈Θσi}N
i=1that produce the
sequence of trajectories Π ={τ1,τ2, . . . , τN}that satisfies
the following conditions:
τ1(0) = xstart (1)
ξgoal(τN(1)) = 1 (2)
τi(1) = τi+1(0)∀i∈1, . . . , N −1 (3)
That is, the solution sequence must satisfy three key prop-
erties: the initial state of the first trajectory must be equal
to the start state (Eq. 1), adjacent trajectories must connect
continuously (Eq. 3), and the final state of the last trajectory
must satisfy the goal condition (Eq. 2). Skills produce only
valid trajectories τi: not causing collisions between the robot
and static obstacles and respecting robot dynamics. Additional
constraints (e.g., motion smoothness) and optimization objec-
tives (e.g., execution time) may be imposed on the solution.
A. Skill-centric Manipulation Planning
Robotic manipulation refers to the process of altering the
state of objects through physical interaction. A fundamental
characteristic of manipulation planning is that the system is
typically underactuated – the number of actuated DOFs is
smaller than the dimension of the state space X.
For a robotic manipulator, we define its configuration space
asQR∈Rn, where nrepresents the number of joint angles.
For each movable object Oiwe define a configuration space
QOi⊆SE(3). The complete state space is then defined as the
product of all configuration spaces, X=QR×QO1× ··· ×
QOk, where kis the number of movable objects. Since mov-
able objects are included in the state space and are not directly
actuated, the state space dimension mis generally greater than
the robot’s configuration space dimension n(m > n ), resulting
in an underactuated system. In this work we assume access to
the state of the environment1, to its approximate physics model
(e.g., a physics simulator), and to a set of parameterized skills
Athat can act as generators, connectors, or both. Additionally,
we assume that invoking a skill during planning with the
same parameters yields identical trajectories (e.g., calling a
motion planner with the same parameters will return the same
trajectory) to ensure probabilistic completeness (Sec. IV-C).
We account for execution stochasticity by evaluating each skill
multiple times and adjusting its cost based on success rate
(Sec. IV-A).
IV. M OSAIC
The core concept of M OSAIC is the construction of a di-
rected multigraph, which we call a mosaic graph , by invoking
skills of two types: generators , which produce local behavior
trajectories, and connectors , which link the generated local
trajectories. Since naively alternating between these skills can
result in excessive computational overhead, we introduce a
planning guidance module, the oracle , which orchestrates this
process by selecting appropriate skills and determining which
parts of the mosaic graph to connect. In this section, we outline
the algorithm’s main structure and then provide an overview
the role of the oracle.
A. Algorithmic Approach
Our algorithmic framework, which we term M OSAIC (de-
tailed in Alg. 1), aims to explore the trajectory space Tand
find a sequence of skill-trajectories whose application to the
given start state xstartwould result in a state satisfying a goal
condition ξgoal. M OSAIC achieves this by leveraging a given
skill library Σand oracle Oto construct the mosaic graph
(line 1), where each node represents a trajectory generated by
a generator skill ( σ∈ G), and each directed edge corresponds
to a trajectory produced by a connector skill ( σ∈ C) (Fig.
1The planner operates under the assumption of full observability and does
not explicitly address perception uncertainty during planning. In our physical
setup, we use an RGB-D camera to estimate the world state, demonstrating
how M OSAIC can function with real-world perception inputs. However,
handling perception uncertainty is outside the scope of this work and remains
an important direction for future research, where algorithmic reasoning over
uncertainty will be incorporated into the planning process.Algorithm 1: MOSAIC
Input: Start state xstart∈ X
Goal termination condition function
ξgoal:X → { 0,1}
Skill library Σ ={σ}M
m=1
Oracle Ofor skill and trajectory selection
Output: Sequence of trajectories Π ={τ1, . . . , τN}
1M=DirectedMultigraph()
2G ← {G i∈Σ} // Generators container
3C ← {C j∈Σ} // Connectors container
4whileM.NODES =∅do
5 forσ∈ G do
6 θ←O.SampleParameters (σ)
7 forτ∈σ(θ)do
8 M.AddNode (τ)
9 end
10 end
11end
12τstart← {xstart}
13M.AddNode (τstart)
14while¬M.HasPath (xstart,{x|ξgoal(x) = 1})do
15 σ←O.ChooseSkill (Σ,M) // Oracle selects
next skill to apply with Mas context
16 θ←O.SampleParameters (σ)
17 ifσ∈ C then
// Apply connector skill
18 ξ0, ξ1←O.ChooseCondsToConnect (M)
19 trajectories ←σ(ξ0, ξ1, θ)
20 forτ∈trajectories do
21 M.AddEdge (τ,Cost (τ))
22 end
23 else
// Apply generator skill
24 trajectories ←σ(θ)
25 forτ∈trajectories do
26 M.AddNode (τ)
27 end
28 end
29end
// Return shortest path from start to any goal state
30return M.ShortestPath (xstart,{x|ξgoal(x) = 1})
1). Components in the mosaic graph span portions of the
trajectory space Twhere skills are effective, and connec-
tions between these facilitate finding satisfactory trajectory
sequences even when the planning horizon is long.
The algorithm starts by invoking all available generators
and adding all valid trajectories to the mosaic graph as
disconnected nodes (lines 4–11). If no nodes were added to
the graph due to generator failures, the algorithm repeatedly
queries the generators with different parameters until it finds
at least one valid node. Once the graph contains a valid node,
the main loop begins exploring the skill-trajectory space and
adding vertices and edges to the mosaic graph.
The main loop of M OSAIC (lines 14–29) begins with the
oracle selecting a skill to invoke next (line 15), which can be
either a generator or a connector, and samples its parameters.
If the selected skill is a connector, the oracle assigns it a
start condition and goal condition (line 18). Most often, the
connector skill chooses two skill trajectories τ0andτ1from
the mosaic and sets ξ0:=1{τ0(1)}andξ1:=1{τ1(0)}
to attempt connecting them. Otherwise, the connector will
attempt to connect a chosen skill trajectory τ0to the goal
condition by setting ξ0:=1{τ0(1)}andξ1:=ξgoal. Next,
the selected skill σis invoked, and only the valid trajectories
it generates are added to the mosaic graph – as edges in
the case of a connector (line 19) or as nodes in the case
of a generator (line 24). If a skill produces more than one
trajectory per call, e.g., via neural network batch inference or
parallel computation on CPU cores, M OSAIC creates multiple
vertices or edges efficiently and leverages this information
to estimate a confidence value associated with the selected
skill by considering the fraction of invalid trajectories in
the batch (lines 17–23 and 24–28). Finally, if a connection
exists in the mosaic between the start state and a terminal
state that satisfies the goal condition, the algorithm finds the
corresponding shortest path (line 30) and terminates.
B. Oracle
The M OSAIC algorithm explores the trajectory space T
to find effective skill sequences between a start state and
a goal condition. While it is possible to discover effective
skill sequences in diverse regions of the trajectory space by
using only generator and connector skills, naively alternating
between them may require exploring a significant portion of
Tbefore discovering skills that are relevant to the task at hand
(see experimental results for M OSAIC -Roadmap in sec. V-E).
To address this, M OSAIC leverages an oracle module to direct
the planning process toward discovering skills that are not only
effective and robust but also task-relevant. The oracle balances
the exploration of Twith the exploitation of the knowledge
aggregated so far in the mosaic graph Mby choosing which
skills σto invoke within M OSAIC , deciding whether to use
them as generators or connectors, and suggesting parameters
θto assign them.
In this paper, we introduce a simple domain-independent
oracle module that efficiently balances exploration and ex-
ploitation using statistical and probabilistic mechanisms.2We
briefly describe it here and provide more details in the
Appendix. Our proposed domain-independent statistical or-
acle leverages metrics derived from the state of the mosaic
graph and observed skill performance to guide its decisions.
Specifically, this oracle considers four metrics: the mosaic
graph connectivity (via a node-to-edge ratio) to inform whether
to invoke generator or connector skills, the per-skill success
rates to identify robust skills worth exploiting, the per-skill
failure frequencies to penalize poorly performing skills, and
the invocation frequency associated with each skill to bal-
ance exploration. The oracle incorporates this information in
probabilities associated with the different choices in all of its
2We wish to highlight the significant opportunities that lie in developing
sophisticated oracles to adapt M OSAIC to specific use cases or systems.
Effective oracle modules could inform the planning process with domain-
specific knowledge tailored to particular applications or leverage learned
models trained on task-specific data, for example.decision points and selects outcomes at random. Importantly,
this randomness ensures that all skills maintain a non-zero
probability of being chosen at each iteration of the algorithm.
By doing so, our oracle can flexibly explore the spaces of
skills, parameters, and skill-sequences.
C. Theoretical Analysis: Probabilistic Completeness
In this section, we formally define probabilistic complete-
ness (PC) in the context of skill-centric planning and provide
a proof sketch demonstrating why M OSAIC is PC. To define
PC with respect to a given skill library, we first introduce the
concept of a feasible solution:
Definition 3. Given a start state xstart, a goal condition
function ξgoal:X → { 0,1}, and a library of skills Σ =
{σm∈ A}M
m=1, a solution trajectory is said to be feasible
under Σif it is decomposable into a sequence of trajectory
segments, Π = {τ1, . . . , τN}such that ∀τi∈Π,∃σi∈
Σwith parameters θ∈Θσithat can generate τi.
Given the definition of feasibility, a PC algorithm must
guarantee that, asymptotically, it will discover a sequence of
trajectories that compose a feasible solution if one exists:
Definition 4. LetΠALG
kbe the set of trajectories discovered
by a skill-centric planning algorithm ALG at its k-th iteration.
Additionally, let Π∗be the set of all feasible solutions for the
planning problem. We say that the algorithm is probabilisti-
cally complete if
lim
k→∞P(∃Π∈ΠALG
k|Π∈Π∗) = 1
To establish that M OSAIC is probabilistically complete,
it must guarantee two properties. First, as the number of
iterations k→ ∞ , the algorithm must ensure that all generator
skills in the library ( G ∈Σ) are invoked with every possible
parameter configuration. This exhaustive exploration is neces-
sary to ensure that all potential trajectory segments that could
form part of a feasible solution are eventually discovered.
Second, M OSAIC must guarantee that, asymptotically, every
disconnected node has been attempted to be connected using
all possible connector skills from the library ( C ∈Σ), with
every possible parameter configuration.
MOSAIC is probabilistically complete due to the random
elements introduced by its oracle. The oracle ensures that,
with a non-zero probability, every generator skill is invoked
with all parameter configurations, and parameters themselves
are also sampled randomly. Similarly, every pair of discon-
nected nodes in the mosaic graph is eventually attempted to
be connected using all connector skills and their randomly
sampled parameters. By incorporating randomness in both skill
selection and connection attempts, the algorithm guarantees
that, asymptotically, all trajectory segments and connections
will be explored, leading to the discovery of a feasible solution
with probability 1if one exists.
(a) Transport.
 (b) Transport in Clutter.
 (c) Transport Among Movable Objects.
Fig. 2: Our simulated experimental setups with their physical counterparts. The goal condition for the planner is to place the
plate in the bin, and the level of complexity increases between test scenarios with an increased number of additional objects.
In Scenario 1: Transport (a), the plate is the only object on the table and the robot must push it to the edge in order to pick
it up. Scenario 2: Transport in Clutter (b) includes additional objects on the table. The robot must still move the plate to the
edge without displacing other objects. The robot cannot touch other objects, but the plate may contact them without moving
or tilting them. Scenario 3: Transport Among Movable Objects (c) allows the robot to interact with more objects on the table.
In our tests, the robot discovered the need to clear space for manipulating the plate by moving the chips can elsewhere.
V. E XPERIMENTAL ANALYSIS
To evaluate the efficacy of M OSAIC , we conduct a series
of experiments on a variety of simulated environments and
validate our results on a real-world setup. In our experiments,
we wished to quantify the effectiveness of M OSAIC in solving
long-horizon manipulation tasks when only having access to
a limited number of imperfect generic skills, and compare it
to the performance of a set of baseline algorithms. Please see
Fig. 2 for an overview of our experimental setup and refer to
our accompanying video to watch real-world demonstrations
of M OSAIC in the same experimental scenarios.
A. Experimental Setup
We set up three scenarios with increasing levels of complex-
ity to evaluate the performance of M OSAIC . In all scenarios,
no prior domain or task-specific knowledge was given beyond
the provided goal condition. Our objective was to test the
hypothesis that, given a robot equipped with a set of generic
skills capabilities, M OSAIC can determine how to leverage
them to accomplish the task. The testing scenarios were as
follows:
1) Scenario 1, Plate Transport :The robot was required to
remove a plate from a table and place it in a bin. However,
the plate’s geometry prevented the robot from grasping it when
it was placed in the interior of the table. Therefore, to solve
the task the planner must have discovered a sequence of skill-
trajectories that first reorients the plate, then grasps it from the
side, and finally transports it to the bin.
2) Scenario 2, Plate Transport in Clutter :Extends the first
scenario by surrounding the plate with other objects on the
table. In this scenario, the robot was required to avoid the other
objects while transporting the plate to the bin. This scenario
emphasizes the need for the robot to adapt its selected skills
to the environment’s geometry.
3) Scenario 3, Plate Transport Among Movable Objects :
expands the dimensionality of the planning problem to include
an additional object that the robot is allowed to manipulate.
The robot is tasked with transporting the plate from a tabletop
to a bin and may reorient the other object to facilitate the task.B. Skill Library
In our experimental analysis, the robot was given access to
a library of learned and analytical object-centric skills: Push ,
Pick,Transport andRearrange . The skills can function as gen-
erators, connectors, or both. We briefly outline their operation
here, and expand on their description in the Appendix.
ThePush Skill acts on objects by pushing them and can
be used both as a generator and a connector. In our imple-
mentation, the Push skill generates pushing motions using
a learned diffusion policy [6] model trained to push objects
toward specified directions over short distances (up to 25 cm).
This skill’s policy is imperfect and reached a success rate of
only 70% after training. When used as a generator, this skill
creates new world configurations and generates local pushing
trajectories within them. When invoked as a connector, the
skill generates a push trajectory to move a specified object
between its positions across two previously generated world
states. Additionally, it employs a collision-free motion planner
to connect the corresponding robot configurations.
The Pick skill is a highly localized skill attempting to
generate a grasping trajectory for an object. It is used as a
generator. This skill computes a grasp pose by analyzing the
object’s geometry and the arm’s kinematics, and employs a
screw-based motion policy to plan a motion from a pre-grasp
pose to the grasp pose and a retraction.
TheTransport skill aims to move objects between a start
state and a goal condition; it is a connector skill. This is
carried out by assuming that the object is grasped by the robot
and using a motion planning algorithm [20, 31] to generate a
collision-free robot trajectory that would properly move the
object. If by the end of the motion the object does not fulfill
the goal condition (e.g., because the object was not grasped
to begin with), the skill trajectory generation fails.3
Finally, the Rearrange skill, used only in Scenario 3,
combines both pick-and-place with Push capabilities, enabling
3In practice, our implementation of the Transport skill avoids invoking a
motion planning algorithm if an object is not in-hand by first analyzing the
geometry of the world state and checking if the object is grasped.
Fig. 3: Comparison of algorithms across experimental scenarios. The top row corresponds to a simpler environment without
additional objects (50 data points), the middle row represents a more complex scenario with objects obstructing plate-pushing
(20 data points), and the bottom row involves manipulating an additional object to clear space for plate transport from the
tabletop (20 data points). Left: success rate comparison. Middle: planning time density estimation, median, interquartile range
(IQR), and average. Right: “head-to-head” comparison. For each pair of algorithms, we identify all tests that both solved
successfully and, among these, compare the average ratio of performance metrics. Relative planning times are in the upper-
right triangle and relative solution sequence lengths are in the lower-left triangle of the confusion matrix. Each cell shows the
performance of the “row algorithm” compared to the “column algorithm.” Seeking to evaluate M OSAIC , smaller values are
better in the first row and larger values are better in the first column.
the robot to rearrange multiple objects with prehensile and
non-prehensile policies. This skill serves as a connector.
At each skill invocation, we roll out its policy in a physics
simulator to observe the outcomes of any robot-object inter-
actions and determine the validity of skill trajectories. In our
implementation we use the Sapien [36, 14] simulator.C. Evaluations Metrics
The primary metric we used to evaluate the performance
of algorithms within each scenario is the success rate. We
defined an attempt to solve a task as successful if the planner
has been able to compose the skills at its disposal such that
the recovered sequence of skill-trajectories terminates at a state
satisfying the goal condition ξgoal. Additionally, we collected
the number of steps taken to complete a task (i.e., the number
of trajectories composing a solution sequence), as a proxy
for the difficulty of a task and the quality of the recovered
solutions. Finally, we recorded the runtime of algorithms to
assess their practical efficiency.
D. Baseline Algorithms
We compared M OSAIC to four long-horizon planning
frameworks that compose skill primitives to solve manip-
ulation tasks. Our first baseline, named Skills as Options ,
emulates the behavior of the options framework and skill
chaining [18]. The second baseline, CEM , is a feedback-based
planner that performs local optimization by sampling action
sequences, executing the first action of the best candidate, and
replanning at each step in a receding horizon manner. The
remaining two baselines progressively move away from the
options framework by relaxing its incremental skill discovery
process, allowing for flexible skill generation using generators
and connection through connectors , thus aligning more closely
with M OSAIC ’s approach. We detail each baseline below.
The Skills as Options algorithm mimics the behavior of
skill chaining by exploring the space of skills incrementally,
with each new skill beginning at the termination state of
another. To realize this behavior, we create an action space
ˆAcomprised of modified generator skills that operate under
start-state conditioning, effectively solving an initial-value
problem when generating skill trajectories. Specifically, the
action space ˆA:=ˆGwith each skill being a mapping
of the form ˆGi:X × ΘGi→T. The Skills as Options
algorithm searches over the space of world states x∈ X
in a breadth-first fashion. The algorithm begins by creating
a node for the initial state xstartand generates successor nodes
via the skills in ˆAsuch that each successor set to a node
xis{x′|x′=τ′(1),τ′=ˆGi(x, θ), θ∈ΘˆGi}. Skill
parameters are randomly chosen. The motion planner skill is
goal-conditioned and attempts to connect a state xto a goal
state. The search terminates when a node has an associated
state xwhere ξgoal(x) = 1 . Finally, a path is reconstructed
and returned.
CEM (Cross-Entropy Method [27]) approaches long-
horizon planning by identifying promising sequences of skill
parameters in a receding horizon manner. At each iteration, it
samples a batch of skill-parameter sequences from ˆA, evalu-
ates them based on task progress, selects the top-performing
candidates, and refines the sampling distribution accordingly.
It then executes the first skill in the most promising sequence,
updates the world state, and repeats this process until reaching
the goal or exhausting the allowed iterations.
MOSAIC -Roadmap deviates from the sequential nature of
Skills as Options , which requires discovered skills to begin
at termination states of other skills, by allowing arbitrary
generation of skill trajectories without conditioning on any
previously discovered skill motions. This baseline is inspired
by the Probabilistic Roadmap (PRM) [16] structure and is
modifying the oracle in M OSAIC accordingly. Specifically,
MOSAIC -Roadmap follows a two-phase approach: roadmap
generation and path computation. To generate the roadmap,the algorithm repeatedly uses generator skills Gi∈ A to obtain
skill trajectories τand attempts to connect their start τ(0)
and goal τ(1)to the knearest generated trajectories τ′in the
roadmap with all connector skills Ci∈ A.4Once the roadmap
is constructed, two vertices, {xstart}and{xgoal}—each rep-
resenting a singleton skill trajectory corresponding to the
start and goal configurations—are added to the roadmap. The
algorithm then attempts to connect these vertices to their
nearest neighbors using the available connector skills Cj∈ A.
If successful, Dijkstra’s algorithm [8] is used to compute a
sequence of skill trajectories that connect them. If no such
sequence is found, the algorithm reports failure.
Incremental M OSAIC -Roadmap is our closest baseline
to M OSAIC . Its operation is generally similar to M OSAIC -
Roadmap, with the main difference between them being that
the oracle for this baseline calls for additional rounds of skill
generation and connection as long as the start and goal nodes
are in disconnected components on the roadmap. This ensures
that the algorithm will keep progressing towards a solution as
long as one does not yet exist within the roadmap.
E. Experimental Results
Through our experiments, we wanted to shed light on
three primary points: M OSAIC ’s capacity for solving long-
horizon skill-centric planning problems, the importance of its
oracle module, and how M OSAIC ’s skill discovery and com-
position strategy compare with the commonly used directed
approach (forward/backward search). Our experimental results
are shown in Fig. 3.
To assess algorithms’ scalability in terms of skill complexity
and task horizon, we tested four evaluated algorithms in three
scenarios with increasing levels of difficulty. We observed that
MOSAIC ’s approach to skill discovery and composition ef-
fectively solved the long-horizon manipulation planning tasks
across all experiments with high success rates. Compared to
baselines, M OSAIC solved more problems successfully and
did so significantly faster. In terms of solution quality, which
we define here as the length of the skill sequence found,
MOSAIC ’s solutions were of similar quality to those of the
baselines (see confusion matrices in Fig. 3). This suggests
that M OSAIC does not compromise on solution quality while
offering greater stability and finding solutions faster when
compared to baselines.
The results of the Skills as Options baseline, which operates
in a manner representative of current methods that rely on
sequential skill discovery, offer intuition for how common
skill discovery and composition methods may behave in our
testing scenarios. The behavior of Skills as Options in the
testing scenarios followed an expected trend: failing to solve
more problems as the difficulty of the tests increased. Skills
as Options solved an impressive number of problems when
only allowed to manipulate one object (Scenarios 1 and 2),
but failed to solve most of the problems in the third scenario,
4In this work the distance metric between world state x, x′∈ X is
the square root of the sum of squared per-object pose differences (position
difference and quaternion distance).
which included additional skills and object interactions. This
behavior stemmed in part from the directionality of the skill
discovery process, which can be inefficient at discovering
skills that are drastically different from those found before.
During our experimentation, we have seen cases where unfa-
vorable explored states caused all subsequent attempted skill
generations to fail – making the search process fail as a whole.
From a runtime efficiency perspective, since the space of skill
combinations grows exponentially with the number of skills
inΣ, we would expect Skills as Options to scale poorly with
the number of available skills.
CEM failed to solve most of the problems, achieving a zero
success rate in the second and third scenarios. Similar to Skills
as Options , this can be attributed to the directionality of its
skill discovery process. However, CEM exhibited additional
weaknesses due to its reliance on local optimization and the
absence of any backtracking mechanism. Without the ability
to recover from suboptimal exploration paths or escape local
minima, CEM struggled as task complexity increased, particu-
larly in environments requiring diverse skill compositions and
long-horizon planning.
In contrast to Skills as Options and CEM, our other
two baselines explored the skill-trajectory space in a less
constrained fashion. While performing better than Skills as
Options across all scenarios, they showed an interesting dis-
agreement in their success rates. These baselines, M OSAIC -
Roadmap andIncremental MOSAIC -Roadmap , overall trended
like Skills as Options, but their rate of reduction in success
rate was different. Incremental M OSAIC -Roadmap performed
better than M OSAIC -Roadmap in the second scenario, but the
reverse was true in the third. These results highlight how
different skill discovery and composition strategies do not
work equally well in different problem scenarios. Therefore,
keeping a fixed exploration strategy for all problem instances
may lead to unexpected poor performance. Instead, it could
be better for algorithms to flexibly adapt their exploration
methodology to the scenario at hand. In this light, we attribute
part of M OSAIC ’s stable performance to its oracle module
that adaptively navigates the exploration of the skill-trajectory
space.
VI. C ONCLUSION
In this paper, we present M OSAIC , a skill-centric algorith-
mic framework for solving long-horizon tasks by composing
task-independent generic skills. M OSAIC offers a new outlook
on skill-based planning where the skills themselves guide the
planning process toward regions where those are likely to
succeed. M OSAIC ’s structure breaks away from conventional
paradigms on multiple axes. It operates without requiring sym-
bolic representations, does not impose restrictive requirements
on the definitions of skills, and freely explores the skill-
trajectory space, not discovering skills incrementally like is
commonly done. M OSAIC is simple and modular, and operates
with two main modules: the generative mechanism, where gen-
erator skills are responsible for producing local trajectories in
new world configurations, and the linking mechanism, whereconnector skills establish connections between the generated
skill trajectories. We believe that M OSAIC opens up exciting
opportunities for using powerful task-specific learned skills for
solving real-world long-horizon tasks.
This paper lays the foundation for skill-centric planning
with M OSAIC by rigorously defining the problem formulation
and testing the proposed approach in a domain where no
symbolic abstractions are available. Experimentally, M OSAIC
demonstrated its effectiveness by achieving high success rates
while significantly reducing planning time compared to the
baselines. As this is a first venture into this skill-centric
paradigm, there are exciting opportunities for future work.
For example, developing different types of oracle modules,
scaling the world state to handle many movable obstacles, and
allowing M OSAIC to reason over partially defined states, could
be impactful. In addition, an interesting direction for future
research is to incorporate closed-loop execution and robust
control. Modifying M OSAIC ’s output to include parameterized
policies alongside skill-trajectories would allow the system to
flexibly adapt to changes in real-time during execution and
reduce the sim-to-real gap. Furthermore, future work may
include parallelizing computational modules and parts of the
planner, potentially leveraging GPU acceleration to improve
overall efficiency.
VII. L IMITATIONS
The aim of this work is to provide the foundation for
future skill-centric planning research as well as to propose
a flexible algorithmic framework that can solve problems in
this space. In light of recent progress in robotic skill learning,
we are excited about future research in long-horizon skill-
centric planning and offer a few directions for future work.
First, the output of M OSAIC in its current form is a long-
horizon trajectory composed of robot and object configura-
tions. Executing this trajectory is performed in an open-loop
fashion with respect to objects, which can lead to execution
failures particularly due to the complexities of robot-object
interactions. We identified two possible ways to address this
limitation. First, narrowing the sim-to-real gap by better mod-
eling the world can help skills improve their estimates for
interaction outcomes through physics simulators. With that,
individual skills could produce trajectories that could translate
more closely between simulation and the real world.
Second, the inherent stochasticity of interactions in the
physical world can make execution deviate from simulation. A
way to combat this could be to have M OSAIC output policies
that can be executed closed loop with respect to objects
in addition to trajectories. Another significant open question
is the scalability of the approach to many movable objects
in the state space. The complexity of the planning problem
grows exponentially with the number of movable objects [35],
and while M OSAIC aims to navigate through the state space
without systematic exploration, it still requires careful consid-
eration of how the connection mechanism can be deployed
when multiple movable objects need to be repositioned.
REFERENCES
[1] Barrett Ames, Allison Thackston, and George Konidaris.
Learning symbolic representations for planning with
parameterized skills. In 2018 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS) ,
pages 526–533. IEEE, 2018.
[2] Akhil Bagaria and George Konidaris. Option discovery
using deep skill chaining. In International Conference
on Learning Representations , 2019.
[3] Akhil Bagaria, Jason Senthil, Matthew Slivinski, and
George Konidaris. Robustly learning composable options
in deep reinforcement learning. In Proceedings of the
30th International Joint Conference on Artificial Intelli-
gence , 2021.
[4] Akhil Bagaria, Jason K Senthil, and George Konidaris.
Skill discovery for exploration and planning using deep
skill graphs. In International Conference on Machine
Learning , pages 521–531. PMLR, 2021.
[5] Joao Carvalho, An T Le, Mark Baierl, Dorothea Koert,
and Jan Peters. Motion planning diffusion: Learning
and planning of robot motions with diffusion models.
InInternational Conference on Intelligent Robots and
Systems . IEEE, 2023.
[6] Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau,
Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran
Song. Diffusion policy: Visuomotor policy learning via
action diffusion. The International Journal of Robotics
Research , 2024.
[7] Matthew Crosby, Francesco Rovida, Mikkel Rath Peder-
sen, Ronald PA Petrick, and V olker Kr ¨uger. Planning for
robots with skills. In 4th ICAPS Workshop on Planning
and Robotics 2016 , pages 49–57. ICAPS, 2016.
[8] EW DIJKSTRA. A note on two problems in connexion
with graphs. Numerische Mathematik , 1:269–271, 1959.
[9] Xiaolin Fang, Caelan Reed Garrett, Clemens Eppner,
Tom´as Lozano-P ´erez, Leslie Pack Kaelbling, and Dieter
Fox. Dimsam: Diffusion models as samplers for task
and motion planning under partial observability. In 2024
IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS) , pages 1412–1419. IEEE, 2024.
[10] Carlos Florensa, David Held, Xinyang Geng, and Pieter
Abbeel. Automatic goal generation for reinforcement
learning agents. In International conference on machine
learning , pages 1515–1528. PMLR, 2018.
[11] S ´ebastien Forestier, R ´emy Portelas, Yoan Mollard, and
Pierre-Yves Oudeyer. Intrinsically motivated goal ex-
ploration processes with automatic curriculum learning.
Journal of Machine Learning Research , 23(152):1–41,
2022.
[12] Caelan Garrett, Ajay Mandlekar, Bowen Wen, and Dieter
Fox. Skillmimicgen: Automated demonstration genera-
tion for efficient skill learning and deployment. arXiv
preprint arXiv:2410.18907 , 2024.
[13] Caelan Reed Garrett, Rohan Chitnis, Rachel Holladay,
Beomjoon Kim, Tom Silver, Leslie Pack Kaelbling, andTom´as Lozano-P ´erez. Integrated task and motion plan-
ning. Annual review of control, robotics, and autonomous
systems , 4(1):265–293, 2021.
[14] Jiayuan Gu, Fanbo Xiang, Xuanlin Li, Zhan Ling,
Xiqiang Liu, Tongzhou Mu, Yihe Tang, Stone Tao,
Xinyue Wei, Yunchao Yao, Xiaodi Yuan, Pengwei Xie,
Zhiao Huang, Rui Chen, and Hao Su. Maniskill2: A uni-
fied benchmark for generalizable manipulation skills. In
International Conference on Learning Representations ,
2023.
[15] Michael Janner, Yilun Du, Joshua Tenenbaum, and
Sergey Levine. Planning with diffusion for flexible
behavior synthesis. In International Conference on
Machine Learning . PMLR, 2022.
[16] L.E. Kavraki, P. Svestka, J.-C. Latombe, and M.H. Over-
mars. Probabilistic roadmaps for path planning in high-
dimensional configuration spaces. IEEE Transactions on
Robotics and Automation , pages 566–580, 1996.
[17] Beomjoon Kim, Leslie Kaelbling, and Tom ´as Lozano-
P´erez. Guiding search in continuous state-action spaces
by learning an action sampler from off-target search
experience. In Proceedings of the AAAI Conference on
Artificial Intelligence , volume 32, 2018.
[18] George Konidaris and Andrew Barto. Skill discovery in
continuous reinforcement learning domains using skill
chaining. Advances in neural information processing
systems , 22, 2009.
[19] George Konidaris, Leslie Pack Kaelbling, and Tomas
Lozano-Perez. From skills to symbols: Learning sym-
bolic representations for abstract high-level planning.
Journal of Artificial Intelligence Research , 61:215–289,
2018.
[20] J.J. Kuffner and S.M. LaValle. Rrt-connect: An efficient
approach to single-query path planning. IEEE Interna-
tional Conference on Robotics and Automation , 2000.
[21] Jacky Liang, Mohit Sharma, Alex LaGrassa, Shivam
Vats, Saumya Saxena, and Oliver Kroemer. Search-based
task planning with learned skill effect models for lifelong
robotic manipulation. In 2022 International Conference
on Robotics and Automation (ICRA) , pages 6351–6357.
IEEE, 2022.
[22] Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mor-
datch. Reset-free lifelong learning with skill-space plan-
ning. arXiv preprint arXiv:2012.03548 , 2020.
[23] Ajay Mandlekar, Caelan Reed Garrett, Danfei Xu, and
Dieter Fox. Human-in-the-loop task and motion planning
for imitation learning. In Conference on Robot Learning ,
pages 3030–3060. PMLR, 2023.
[24] Jiayuan Mao, Tom ´as Lozano-P ´erez, Josh Tenenbaum,
and Leslie Kaelbling. Pdsketch: Integrated domain pro-
gramming, learning, and planning. Advances in Neural
Information Processing Systems , 35:36972–36984, 2022.
[25] Toki Migimatsu, Wenzhao Lian, Jeannette Bohg, and
Stefan Schaal. Symbolic state estimation with predicates
for contact-rich manipulation tasks. In 2022 International
Conference on Robotics and Automation (ICRA) , pages
1702–1709. IEEE, 2022.
[26] Soroush Nasiriany, Huihan Liu, and Yuke Zhu. Aug-
menting reinforcement learning with behavior primitives
for diverse manipulation tasks. In 2022 International
Conference on Robotics and Automation (ICRA) , pages
7477–7484. IEEE, 2022.
[27] Reuven Y Rubinstein and Dirk P Kroese. The cross-
entropy method: a unified approach to combinatorial op-
timization, Monte-Carlo simulation and machine learn-
ing. Springer Science & Business Media, 2004.
[28] Yorai Shaoul, Itamar Mishani, Shivam Vats, Jiaoyang Li,
and Maxim Likhachev. Multi-robot motion planning with
diffusion models, 2024. URL https://arxiv.org/abs/2410.
03072.
[29] Tom Silver, Ashay Athalye, Joshua B Tenenbaum, Tom ´as
Lozano-P ´erez, and Leslie Pack Kaelbling. Learning
neuro-symbolic skills for bilevel planning. arXiv preprint
arXiv:2206.10680 , 2022.
[30] Aravind Sivaramakrishnan, Sumanth Tangirala, Edgar
Granados, Noah R Carver, and Kostas E Bekris.
Roadmaps with gaps over controllers: Achieving effi-
ciency in planning under dynamics. In 2024 IEEE/RSJ
International Conference on Intelligent Robots and Sys-
tems (IROS) , pages 11064–11069. IEEE, 2024.
[31] I. A. Sucan, M. Moll, and L. E. Kavraki. The open
motion planning library. IEEE Robotics and Automation
Magazine , pages 72–82, 2012.
[32] Richard S Sutton, Doina Precup, and Satinder Singh.
between mdps and semi-mdps: A framework for temporal
abstraction in reinforcement learning. Artificial intelli-
gence , 112(1-2):181–211, 1999.
[33] Vivek Veeriah, Tom Zahavy, Matteo Hessel, Zhongwen
Xu, Junhyuk Oh, Iurii Kemaev, Hado P van Hasselt,
David Silver, and Satinder Singh. Discovery of options
via meta-learned subgoals. Advances in Neural Informa-
tion Processing Systems , 34:29861–29873, 2021.
[34] Zi Wang, Caelan Reed Garrett, Leslie Pack Kaelbling,
and Tom ´as Lozano-P ´erez. Learning compositional mod-
els of robot skills for task and motion planning. The
International Journal of Robotics Research , 40(6-7):866–
894, 2021.
[35] Gordon Wilfong. Motion planning in the presence of
movable obstacles. In Proceedings of the fourth annual
symposium on Computational geometry , pages 279–288,
1988.
[36] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao
Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu
Yuan, He Wang, Li Yi, Angel X. Chang, Leonidas J.
Guibas, and Hao Su. SAPIEN: A simulated part-based
interactive environment. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) , June
2020.
[37] Danfei Xu, Ajay Mandlekar, Roberto Mart ´ın-Mart ´ın,
Yuke Zhu, Silvio Savarese, and Li Fei-Fei. Deep af-
fordance foresight: Planning through what can be done
in the future. In 2021 IEEE international conferenceon robotics and automation (ICRA) , pages 6206–6213.
IEEE, 2021.
VIII. A PPENDIX
In this appendix we will provide additional details about
various practical components that we presented in this work.
We include specifics about the oracle module used and the
skills provided within the skill library.
A. Oracle
The oracle module within the M OSAIC algorithmic frame-
work orchestrates the exploration of the skill trajectory space
Tby selecting skills to invoke and nodes to connect. Here
we detail the specifics of the domain-independent statistical
oracle module that we used in the experiments in this work.
1) Skill Selection: The oracle chooses skills to invoke in
two stages. First (potentially) restricting the library of skills
to a subset and then choosing a particular skill within it.
Skill Type Selection : To decide whether to invoke a gen-
erator skill (to add nodes to the mosaic graph) or a connector
skill (to increase the connectivity of the graph), the oracle
uses the ratio of nodes to edges within the mosaic graph as a
heuristic. Let V(·)denote the number of nodes ( N) or edges
(E) in the mosaic graph, and let plbandpubbe the lower and
upper bounds to be used as thresholds to allow randomness
in the skill type selection process. In addition, we define a
random variable T∼U(0,1)to follow uniform distribution
in the range of 0to1. Then, the type of skill set to activate
is determined by the function:
ftype
σ(N, E) =(
CifP(T >min(pup,max(V(E)
V(N), plb)))
Σelse
Informally, the oracle restricts the available skills to be only
the connector skills if there are fewer nodes than vertices in
the mosaic graph. The probability of this restriction increases
with the ratio between nodes and edges.
Individual Skill Selection: The oracle maintains statistics
on the success rates of each skill, tracking the number of
successful trajectories generated and their contributions to the
overall connectivity of the mosaic graph. This allows the
oracle to prioritize skills with higher success rates, promoting
exploitation. Additionally, to encourage exploration, the oracle
keeps track of how often each skill has been invoked. Less
frequently used skills are given a higher likelihood of being
selected.
Letαbe a scalar parameter, sσbe the success rate of skill
σandtσbe the number of times skill σwas invoked. We
define the following function for assigning noised priority for
each skill option σi∈ftype
σ
fassign
σi(sσi, tσi) =αsσi+ (1−α)s
lnP
j(tσj+ 1)
tσi+ 1+n
where n∼ N(0,1)
Now, we pick the skill by choosing the one that has a
maximum fassign
σivalue
σ= arg max
σi∈ftype
σfassign
σi(sσi, tσi)
2) Node Pair Selection for Connection: Another decision
made by the oracle module is deciding which pairs of nodes
in the mosaic graph are worthwhile to attempt connecting.
Since the mosaic graph is initially comprised of disconnected
components representing different regions in the trajectory
space where skills are effective, the choice of node-pairs to
connect is critical for capitalizing on the recovered skill data
toward solving the task at hand. In our oracle module, the
selection of node-pairs is done in one of four randomization
modes:
1) Random selection: selecting a random node and con-
necting it to its nearest neighbor.
2) Goal bias: if any nodes in the mosaic graph are known
to be connected to a terminal node (i.e., a node whose
trajectory terminates at a state satisfying the goal con-
dition), choose one at random and attempt to connect it
to its nearest neighbor.
3) Start bias: among all nodes in the mosaic graph that are
connected to the start node (i.e., the node holding the
start state singleton trajectory), choose one at random
and attempt connecting it to its nearest neighbor.
4) Unification bias: if nodes connected to both the start
and a terminal node exist, select a pair of these for a
connection attempt.
A note on node pair selection. To avoid repeated un-
successful connection attempts, the oracle assigns penalties
to node pairs with repeated failed connections by increasing
the distance between them according to the number of failed
connection attempts made. This discourages the selection of
these nodes for future connections by modifying the results
of nearest neighbor search, thereby guiding the search toward
more promising regions of the mosaic graph. Additionally, if
two nodes are already connected, they are not considered as
pair candidates.
Letr∼U(0,1)and let ps, pg, ps−gbe the cutoffs for
determining the mode of selection. Additionally, let MODES =
{START ,GOAL ,START -GOAL ,RANDOM }be the set of possible
selection modes. Then, the mode selection function is defined
as follows:
m(MODES ) =

START ifr < p s
GOAL ifps< r < p g
START -GOAL ifpg< r < p s−g
RANDOM ifr > p s−g
B. Skills
This section provides detailed explanations of the skills
we used throughout our experiments, complementing their
introduction in Sec. V.1) Push Skill: In this work, we define two types of Push
skills: generator and connector. The generator push skill uses
adiffusion push policy to generate random object pushes
on the tabletop. Its parameters include the object’s start and
goal poses, as well as a seed (within a predefined range)
for the diffusion inference process. The connector push skill
includes two modules: the push diffusion policy for connecting
two object poses (corresponding to the end and start of the
trajectories within two nodes in the mosaic graph), and the
other is a motion planner for moving the robot to the beginning
of the push motion and for matching the robot configuration
in the second node after performing the push action. The only
parameter for this skill is the seed.
Additional details about Diffusion Policy: To implement
the push skill we use Diffusion Policy [6]. These types of
policies are generative models that learn a denoising pro-
cess to recover dynamically-feasible trajectories from noise
[15, 5, 28]. Given a dataset of trajectories, these score-
based models aim to generate new trajectories following the
underlying data distribution, conditioned on task objectives
like start and termination conditions. The diffusion inference
process involves a K-step denoising process that transforms
a noisy trajectoryKτito a feasible trajectory0τi. Using
Langevin dynamics sampling, at each step k∈K, . . . , 1, a
trajectory-space mean µi
k−1is sampled from the network µθ:
µi
k−1=µθ(kτi)
The next trajectory is then sampled with a variance schedule,
incorporating potential guidance:
k−1τi∼ N 
µi
k−1+ηβk−1∇τJ(µi
k−1)| {z }
Guidance, βk−1
Here, ∇τJ(µi
k−1)represents the gradient of additional
trajectory-space objectives. The diffusion policy incorporates a
receding horizon control, where, in this work, a simulator was
used to update both the robot’s state and the environment’s
state as feedback. For further details on diffusion policies,
please refer to Chi et al. [6]. To train our Push skill, we
collected 10,000 examples of pushing by randomly sampling
start and goal poses of an object. For each example, we
computed the object’s axis-aligned bounding box (AABB) and
push direction, then determined the initial push pose based on
these factors. Closed-loop control (Jacobian control) was used
to execute the push and bring the object to its goal pose. A
diffusion policy network was trained on this dataset, where
the observations include the robot’s configuration, end-effector
pose, object pose, and object goal pose. The network’s output
was joint actions to be executed on the robot.
2) Pick Skill, Score-based Geometric Antipodal Grasp: The
pick skill’s policy begins with selecting an antipodal grasp
that maximizes a score function while ensuring a valid inverse
kinematics (IK) solution, followed by a screw-based motion
planner that moves the robot through a sequence of poses –
pre-grasp, grasp, and retract. The procedure for selecting an
antipodal grasp is as follows: First, we acquire a point cloud
of the object of interest and estimate surface normals for each
point. This is done by identifying the k-nearest neighbors
of each point, fitting a local plane to these neighbors, and
applying downsampling for smoothing. Next, we sample a
set of antipodal grasp candidates based on the object’s axis-
aligned bounding box (AABB). For each candidate, we crop
the surface normals within the region between the gripper
fingers. Finally, we assign a score to each grasp candidate
based on the dot product between the object’s normals in the
grasp region and the gripper line, which is defined as the
normal to the contact surfaces of the fingers (line between
the two fingers).
When the Pick skill was used as a generator , its parameters
included the object’s pose and a seed (within a predefined
range) for the probabilistic components of the grasp selection
process. When used as a connector , the only parameter was
the seed.