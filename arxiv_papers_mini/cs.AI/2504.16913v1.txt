OSINT at CT2 - AI-Generated Text Detection: Tracing
Thought: Using Chain-of-Thought Reasoning to Identify
the LLM Behind AI-Generated Text⋆
Shifali Agrahari1,*,†, Sanasam Ranbir Singh1,†
1Department of Computer Science and Engineering Indian Institute of Technology Guwahati, India
Abstract
In recent years, the detection of AI-generated text has become a critical area of research due to concerns about
academic integrity, misinformation, and ethical AI deployment. This paper presents COT_Finetuned , a novel
framework for detecting AI-generated text and identifying the specific language model (LLM) responsible for
generating the text. We propose a dual-task approach, where Task A involves classifying text as AI-generated or
human-written, and Task B identifies the specific LLM behind the text. The key innovation of our method lies in
the use of Chain-of-Thought (CoT) reasoning, which enables the model to generate explanations for its predictions,
enhancing transparency and interpretability. Our experiments demonstrate that COT_Finetuned achieves high
accuracy in both tasks, with strong performance in LLM identification and human-AI classification. We also show
that the CoT reasoning process contributes significantly to the model’s effectiveness and interpretability.
Keywords
AI generated Text, LLMs, Text classification, Chain of thought
1. Introduction
Recent advancements in Natural Language Processing (NLP) have led to the development of powerful
pre-trained language models (PLMs) capable of generating highly realistic text. These models, such as
GPT-4, DeBERTa, and T5, have made significant strides in a wide range of applications, from chatbots
and text generation to machine translation and summarization. However, as the capabilities of these
models increase, so does the challenge of distinguishing between human-generated and AI-generated
text. This has raised concerns about academic integrity, misinformation, and responsible deployment of
AI technologies.
To address these issues, The AAAI 2025 DEFACTIFY 4.0 [ 1] – Workshop Series on Multimodal Fact-
Checking and Hate Speech Detection on CT2 - AI Generated Text Detection Task A: This is a binary
classification task where the goal is to determine whether each given text document was generated by
AI or created by a human. Task B: Building on Task A, this task requires participants to identify which
specific LLM generated a given piece of AI-generated text. For this task, participants will know that the
text is AI-generated and must predict whether it was produced by models such as GPT 4.0, DeBERTa,
FalconMamba, Phi-3.5, or others.
In this paper, we propose a novel framework, Chain of Thought Finetuning ( COT_Finetuned ),
for the dual task problem of AI-generated text detection and identification of the specific language
model (LLM) responsible for the generation of the text. Our approach builds upon the concept of
Chain-of-Thought (CoT) reasoning, which provides a structured and explainable process for classifying
text and identifying the model behind its generation.
To solve these tasks, we introduce a fine-tuned model that is capable of jointly predicting both the AI
vs. Human classification (Task A) and the specific LLM responsible for generating the text (Task B).
De-Factify 4: 4th Workshop on Multimodal Fact Checking and Hate Speech Detection, co-located with AAAI 2025. Pennsylvania.
⋆You can use this document as the template for preparing your publication. We recommend using the latest version of the
ceurart style.
*Corresponding author.
†These authors contributed equally.
/envel⌢pe-⌢pena.shifali@iitg.ac.in (S. A. ); ranbir@iitg.ac.in (S. R. Singh)
©2022 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).arXiv:2504.16913v1  [cs.CL]  23 Apr 2025
Figure 1: Proposed detector model for binary classification task A & multi classification task B.
The key innovation in our approach is the use of Chain-of-Thought (CoT) reasoning, which allows the
model to generate explanations for its decisions, enhancing interpretability and transparency. These
explanations not only provide insights into the classification process but also assist in understanding
the stylistic choices and patterns unique to different LLMs.
2. Related Work
Detecting AI-generated text has gained significant attention in recent years, with numerous methods
proposed to tackle this challenge. Typically, this task is framed as a binary classification problem,
distinguishing human-written text from machine-generated content [ 2,3,4]. Existing approaches can
be broadly categorized into three main types: supervised methods, unsupervised methods, and hybrid
approaches.
Supervised methods rely on labeled datasets to train detection models. Research in this area includes
works like [ 5,6,2,7,8,9], which demonstrate that supervised approaches generally achieve strong
performance. However, these methods are prone to overfitting, particularly when dealing with limited
or domain-specific training data [ 10,11]. Unsupervised methods, such as zero-shot detection techniques
[12,4,10,11,13,14], leverage the capabilities of pre-trained language models to classify text without
task-specific training. Adversarial methods also fall within this category, focusing on evaluating detector
robustness against perturbations. For example, [ 15] assesses the impact of character-level modifications
like misspellings, using French as a case study. Similarly, [ 16] introduces DIPPER, a generative model
trained to paraphrase paragraphs and evade detection.
Hybrid approaches combine feature-based methods with machine learning or neural models. They
often utilize metrics such as word count, vocabulary richness, and readability scores, fused with machine
learning or fine-tuned neural networks for detection [ 12,17,18,19,20]. Fusion and ensemble strategies
have also been explored to enhance detection accuracy.
3. Methodology
3.1. Overview
In this paper, we propose a novel method called COT_Finetuned that combines chain-of-thought
(CoT) reasoning with fine-tuned classification models to identify whether a given text document is
AI-generated or human-written, and to further identify which specific Language Model (LLM) generated
an AI-generated document. The method is designed to handle two tasks simultaneously:
•Task A: Binary classification to determine if the text is AI-generated or human-written.
•Task B: For AI-generated texts, multi-class classification to identify which LLM (e.g., GPT-4.0,
DeBERTa, FalconMamba, Phi-3.5, etc.) generated the text.
3.2. COT_Finetuned Method
TheCOT_Finetuned method uses a chain-of-thought reasoning process to explain and classify the
given text documents. The method generates two outputs: a classification result for Task A and, if
applicable, an identification of the LLM for Task B, along with the reasoning behind each classification.
3.2.1. Dataset
The dataset consists of text documents 𝑑𝑖, each paired with two labels for classification:
•Label 𝐴∈{0,1}: The label for Task A, where 0 indicates that the document is human-written
and 1 indicates the document is AI-generated.
•Label 𝐵∈{GPT-4.0 ,DeBERTa ,FalconMamba ,Phi-3.5 , . . .}: The label for Task B, indicating the
specific LLM that generated the text (if Label 𝐴= 1).
In addition, the model will generate the reasoning for its classification decision based on the provided
labels.
3.2.2. Method Process
Let𝑑𝑖be the document, and let Label 𝐴andLabel 𝐵represent the labels for Task A and Task B, respec-
tively.
For Task A (binary classification) & Task B (multi-class classification):
If Label 𝐴= 0,then Label 𝐵=Human .If Label 𝐴= 1,then Label ^𝐵= arg max
𝑗ℒ(ℳ𝑗|𝑑𝑖),(1)
whereℒ(ℳ𝑗|𝑑𝑖)is the likelihood of model ℳ𝑗generating the document 𝑑𝑖. The model assigns
Label 𝐵based on the highest likelihood.
3.2.3. Loss Function
The model is fine-tuned using a combined loss function that incorporates the classification loss for both
Task A and Task B, along with a reasoning loss component.
•Classification Loss : Binary cross-entropy loss is used for Task A (human-written vs. AI-
generated), while cross-entropy loss is employed for Task B (identifying the specific LLM for
AI-generated documents):
ℒclassification (𝑦^𝑖, 𝑦𝑖) =−(𝑦𝑖·log(𝑦^𝑖) + (1−𝑦𝑖)·log(1−𝑦^𝑖)). (2)
•Total Loss : The total loss combines the both classification task:
ℒtotal=ℒclassification (𝑦^𝐴
𝑖, 𝑦𝐴
𝑖) +ℒclassification (𝑦^𝐵
𝑖, 𝑦𝐵
𝑖) (3)
3.2.4. Training and Fine-Tuning
The model is fine-tuned on a training dataset consisting of text documents 𝑑𝑖, the true labels for Task A
(𝑦𝑖) and Task B ( 𝑦𝐵
𝑖), and their corresponding reasoning ( 𝑟𝑖). The training process involves optimizing
the model parameters to minimize the total loss using gradient-based optimization algorithms such as
Adam.
Process Overview :
1. Each document 𝑑𝑖is labeled as either AI-generated or human-written ( 𝑦𝑖∈{AI,Human}).
2. For each document 𝑑𝑖, a prompt 𝑝(𝑑𝑖, 𝑦𝑖)is created:
𝑝(𝑑𝑖, 𝑦𝑖) ="Why is this particular 𝑑𝑖generated by 𝑦𝑖?". (4)
3. The prompt is passed to the LLaMA model, generating a reasoning 𝑟𝑖:
𝑜𝑖=LLaMA (𝑝(𝑑𝑖, 𝑦𝑖))→𝑟𝑖, (5)
where 𝑟𝑖provides insights into why 𝑑𝑖is classified as either AI-generated or human-written.
4.For binary classification (Task A), the text 𝑑𝑖, label 𝑦𝑖, and reasoning 𝑟𝑖are passed to a pre-trained
language model (PLM) such as BERT. The output layer applies a sigmoid activation for binary
classification.
5.IfLabel 𝐴= 1 (AI-generated), the future text 𝑑𝑖, label 𝑦𝐵
𝑖, and reasoning 𝑟𝐵
𝑖are passed to a
fine-tuned PLM for multi-class classification of Task B.
By iteratively optimizing the loss, the model learns to classify documents and generate high-quality
reasoning, ensuring accurate and interpretable predictions. After completing the training, pass the test
dataset to the model, which will return Labe_A and Label_B.
4. Experiments
4.1. Dataset and Processioning
For each task, the organizers provided three datasets [ 21]: Train, Dev and Test snap mentioned in
appendix. Table A.1. Training and development data with labels (AI or human) for the development
phase and for the evaluation phase, testing data without labels for both tasks. All descriptions with
respect to the size of dataset is mentioned in Table 4.1.
Train set Test set Val set Total
7320 1570 1570 10500
Table 1
Statistics of the dataset used in the study, showing the number of samples in the training, test, and validation
sets.
The goal of this task is to classify each text document as AI-generated or human-written. Let the data
set consist of text documents 𝐷={𝑑1, 𝑑2, . . . , 𝑑 𝑛}, where each document 𝑑𝑖is labeled as AI-generated
or human-written. Specifically, for each document 𝑑𝑖, a label 𝑦𝑖∈{AI,Human}is assigned, where
𝑦𝑖=Human indicates a human-written document and 𝑦𝑖=AIindicates an AI-generated document.
The training dataset consists of tuples {(𝑑𝑖, 𝑦𝑖, 𝑦𝐵
𝑖, 𝑟𝑖, 𝑟𝐵
𝑖)}, where: 𝑑𝑖: The input text document, 𝑦𝑖:
The true label for Task A (AI-generated or Human-written), 𝑦𝐵
𝑖: The true label for Task B (specific LLM
if AI-generated) and 𝑟𝑖,𝑟𝐵
𝑖: The reasoning behind the classification decision.
4.2. Experimental Setup
For both Task, the hyperparameters include an epoch size ranging from 50 to 250, while the batch size
is fixed at 32, determined by the available GPU resources. Further details of the experimental setup are
presented in the Appendix A.1. For this experiment, we consider pre-trained language models such as
BERT for both tasks.
5. Results and Analysis
Table 5 show the results of the Leaderboard on test dataset. Table 3 shows the F1-scores for different
methods applied to Task A (binary classification) and Task B (multi-class classification).
For Task A, Bert outperforms Roberta with an F1-score of 0.742 compared to 0.672. The addition
of Chain-of-Thought (COT) reasoning improves the performance of both models, with Bert + COT
achieving the highest F1-score of 0.898.
For Task B, the F1-scores are lower across all methods, reflecting the increased complexity of the
multi-class classification task. Bert + COT outperforms all other methods with a score of 0.307, while
Roberta + COT achieves 0.198.
Overall, COT reasoning improves performance in both tasks, with Bert + COT being the most effective
method, especially for Task A. However, Task B remains challenging, highlighting the need for further
improvements in multi-class classification.
S.No Name Team Name Score for Task-A Score for Task-B
1 Avinash Trivedi Sarang 1 0.9531
2 Duong Anh Kiet dakiet 0.9999 0.9082
3 Vijayasaradhi Indurthi tesla 0.9962 0.9218
4 Shrikant Malviya SKDU 0.9945 0.7615
5 Harika Abburi Drocks 0.9941 0.627
6 Manoj Saravanan Llama_Mamba 0.988 0.4551
7 Chinnappa Guggilla AI_Blues 0.9547 0.4698
8 Xinlong Zhang NLP_great 0.9157 0.1874
9 Shifali Agrahari Osint 0.8982 0.3072
10 Xiaoyu Xiaoyu 0.803 0.5696
11 Rohan R Rohan 0.7546 0.4053
Table 2
Presents the final leaderboard scores for all participating teams in Task A and Task B. The scores have been
officially provided by the organizer, reflecting the performance of each team based on the F1 score.
Method Score for Task-A Score for Task-B
RoBERTa 0.672 0.143
BERT 0.742 0.249
RoBERTa + COT 0.792 0.198
BERT + COT 0.898 0.307
Table 3
F1-Scores of Different Methods for Task-A and Task-B
6. Conclusion
In this paper, we introduced COT_Finetuned , a dual-task framework for detecting AI-generated text
and identifying the specific language model (LLM) that produced it. By leveraging Chain-of-Thought
(CoT) reasoning, we not only achieved strong performance in distinguishing between human and AI-
generated text but also provided explanations for the model’s decisions, contributing to the transparency
and interpretability of AI classification tasks. Our experiments on a real-world dataset showed that
COT_Finetuned performs competitively in both tasks, outperforming traditional classification models
while offering valuable insights into the text generation process.
We also observed that the CoT reasoning mechanism enhances the model’s understanding of the
text’s stylistic features, enabling it to better identify subtle patterns associated with different LLMs. The
dual-task nature of the framework allows for a more comprehensive analysis of AI-generated content,
making it useful for applications in academic integrity, content moderation, and AI ethics.
In conclusion, the proposed method offers an effective solution to the challenges of detecting AI-
generated text and identifying the underlying models, providing both high accuracy and interpretability.
References
[1]R. Roy, G. Singh, A. Aziz, S. Bajpai, N. Imanpour, S. Biswas, K. Wanaskar, P. Patwa, S. Ghosh,
S. Dixit, N. R. Pal, V. Rawte, R. Garimella, A. Das, A. Sheth, V. Sharma, A. N. Reganti, V. Jain,
A. Chadha, Overview of text counter turing test: Ai generated text detection, in: proceedings of
DeFactify 4: Fourth workshop on Multimodal Fact-Checking and Hate Speech Detection, CEUR,
2025.
[2]R. Zellers, A. Holtzman, H. Rashkin, Y. Bisk, A. Farhadi, F. Roesner, Y. Choi, Defending against
neural fake news, Advances in neural information processing systems 32 (2019).
[3]S. Gehrmann, H. Strobelt, A. M. Rush, Gltr: Statistical detection and visualization of generated
text, arXiv preprint arXiv:1906.04043 (2019).
[4]D. Ippolito, D. Duckworth, C. Callison-Burch, D. Eck, Automatic detection of generated text is
easiest when humans are fooled, arXiv preprint arXiv:1911.00650 (2019).
[5]Z. Wang, J. Cheng, C. Cui, C. Yu, Implementing bert and fine-tuned roberta to detect ai generated
news by chatgpt, ArXiv abs/2306.07401 (2023). URL: https://api.semanticscholar.org/CorpusID:
259145150.
[6]A. Uchendu, Z. Ma, T. Le, R. Zhang, D. Lee, Turingbench: A benchmark environment for turing
test in the age of neural text generation, arXiv preprint arXiv:2109.13296 (2021).
[7]W. Zhong, D. Tang, Z. Xu, R. Wang, N. Duan, M. Zhou, J. Wang, J. Yin, Neural deepfake detection
with factual structure of text, arXiv preprint arXiv:2010.07475 (2020).
[8]Y. Liu, Z. Zhang, W. Zhang, S. Yue, X. Zhao, X. Cheng, Y. Zhang, H. Hu, Argugpt: evaluating,
understanding and identifying argumentative essays generated by gpt models, arXiv preprint
arXiv:2304.07666 (2023).
[9]X. Liu, Z. Zhang, Y. Wang, H. Pu, Y. Lan, C. Shen, Coco: Coherence-enhanced machine-generated
text detection under data limitation with contrastive learning, arXiv preprint arXiv:2212.10341
(2022).
[10] E. Mitchell, Y. Lee, A. Khazatsky, C. D. Manning, C. Finn, Detectgpt: Zero-shot machine-generated
text detection using probability curvature, in: International Conference on Machine Learning,
PMLR, 2023, pp. 24950–24962.
[11] J. Su, T. Y. Zhuo, D. Wang, P. Nakov, Detectllm: Leveraging log rank information for zero-shot
detection of machine-generated text, arXiv preprint arXiv:2306.05540 (2023).
[12] I. Solaiman, M. Brundage, J. Clark, A. Askell, A. Herbert-Voss, J. Wu, A. Radford, G. Krueger, J. W.
Kim, S. Kreps, et al., Release strategies and the social impacts of language models, arXiv preprint
arXiv:1908.09203 (2019).
[13] A. Hans, A. Schwarzschild, V. Cherepanova, H. Kazemi, A. Saha, M. Goldblum, J. Geiping, T. Gold-
stein, Spotting llms with binoculars: Zero-shot detection of machine-generated text, 2024, URL:
https://arxiv. org/abs/2401.12070 (????).
[14] R. Shijaku, E. Canhasi, Chatgpt generated text detection, Publisher: Unpublished (2023).
[15] W. Antoun, V. Mouilleron, B. Sagot, D. Seddah, Towards a robust detection of language model-
generated text: Is chatgpt that easy to detect?, in: Actes de CORIA-TALN 2023. Actes de la 30e
Conférence sur le Traitement Automatique des Langues Naturelles (TALN), volume 1: travaux de
recherche originaux–articles longs, 2023, pp. 14–27.
[16] K. Krishna, Y. Song, M. Karpinska, J. Wieting, M. Iyyer, Paraphrasing evades detectors of ai-
generated text, but retrieval is an effective defense, Advances in Neural Information Processing
Systems 36 (2024).
[17] A. Shah, P. Ranka, U. Dedhia, S. Prasad, S. Muni, K. Bhowmick, Detecting and unmasking ai-
generated texts through explainable artificial intelligence using stylistic features, International
Journal of Advanced Computer Science and Applications 14 (2023).
[18] H.-Q. Nguyen-Son, N.-D. T. Tieu, H. H. Nguyen, J. Yamagishi, I. E. Zen, Identifying computer-
generated text using statistical analysis, in: 2017 Asia-Pacific Signal and Information Processing
Association Annual Summit and Conference (APSIPA ASC), IEEE, 2017, pp. 1504–1511.
[19] L. Mindner, T. Schlippe, K. Schaaff, Classification of human-and ai-generated texts: Investigating
features for chatgpt, in: International Conference on Artificial Intelligence in Education Technology,
Springer, 2023, pp. 152–170.
[20] T. Kumarage, H. Liu, Neural authorship attribution: Stylometric analysis on large language models,
in: 2023 International Conference on Cyber-Enabled Distributed Computing and Knowledge
Discovery (CyberC), IEEE Computer Society, 2023, pp. 51–54.
[21] R. Roy, G. Singh, A. Aziz, S. Bajpai, N. Imanpour, S. Biswas, K. Wanaskar, P. Patwa, S. Ghosh,
S. Dixit, N. R. Pal, V. Rawte, R. Garimella, A. Das, A. Sheth, V. Sharma, A. N. Reganti, V. Jain,
A. Chadha, Defactify-text: A comprehensive dataset for human vs. ai generated text detection,
in: proceedings of DeFactify 4: Fourth workshop on Multimodal Fact-Checking and Hate Speech
Detection, CEUR, 2025.
A. Appendix
A.1. Dataset detail
Prompt Human Gemma-2-
9BMistral-
7BQwen-2-
72BLlama-8B Yi-Large GPT-4-o
Roberta
Karmel,
First
Woman
Named to
the S.E.C.,
Dies at 86.Roberta
Karmel,
the first
woman
...........Roberta
Karmel, who
made his-
tory as the
first woman
appointed
........Roberta
Karmel, a
trailblazer
who be-
came......Roberta
Karmel, Se-
curities and
Exchange
Commis-
sion...........Roberta
Karmel,
the first
female com-
missioner
of.......Roberta
Karmel,
the first
woman
to serve
on the
U.S6.......Roberta
Karmel,
who made
history first
woman
appointed
to the
S.E.C........
In the age
of coron-
avirus, the
only way
you can
see Milan
is to fly
through it.Messages
From
Quar-
antine
transcript
..........# In the Age
of Coron-
avirus, the
Only Way
You Can See
Milan is to
Fly.......Title:
"Exploring
Milan in
the Age
of Coro-
navirus:
..........In the Age
of Coron-
avirus, the
Only Way to
See Milan is
to Fly .......I**The
New York
Times**
**IN THE
AGE OF
CORON-
AVIRUS,.......**Title:
Navigating
Milan in
the Age
of Coron-
avirus: .......**Title: In
the Age
of Coron-
avirus, the
Only Way
*.........
Table 4
Comparison of Text Generated by Different Models
Text Label_A Label_B
Roberta Karmel, the first woman ap-
pointed to the U.S. Securities and Ex-
change Commission (S.E.C.), passed away
at 86...........0 Human
# In the Age of Coronavirus, the Only Way
You Can See Milan is to Fly Through It
Milan, the fashion capital of the world, is
known for its bustling piazzas.......1 Gemma-2- 9B
Roberta Karmel, a trailblazer who became
the first woman to serve on the S.E.C., has
died at the age of .....1 Mistral-7B
Table 5
Span of Training Dataset
A.2. Training detail
Hyperparameter Setup: Fine-tuning PLM
Epochs 10-250
Batch Size 5
k 6 layer
Learning Rate 2×10−5
Optimizer Adam
L2 Regularization Weight decay: 0.01
Loss Function Classification & Reasoning Loss
Table 6
Hyperparameter settings for Setup 1: Fine-tuning PLM.
Table 7: Classification and Reasoning of Review Texts
Review Text Classification Reasoning
I just received this dress and
I’m blown away by how well it
fits!Human Personal excitement and spe-
cific details about the fit and
quality. Emotional tone and
specific product-related com-
mentary suggest a human re-
view.
Theft room hotel staff stayed
hotel beginning September,
evening checking returned
money stolen room. Our room
door wasn’t forced entry staff,
reported girlHuman Describes a negative experi-
ence with a lot of detail about
a theft incident, hotel response,
and follow-up efforts. The writ-
ing style has imperfections, a
hallmark of a human review.
I just wore this to a wedding
and I’m absolutely obsessed!
It’s the most flattering dress
I’ve ever owned. The mate-
rial is so soft and drapes per-
fectly, and it’s incredibly com-
fortable... Silent fresh momentLLM Very brief and lacks specific
detail or personal involvement.
Feels like a generated response
without the emotional depth or
depth of thought typical in hu-
man reviews.