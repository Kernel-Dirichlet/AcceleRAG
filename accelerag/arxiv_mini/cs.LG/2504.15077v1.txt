arXiv:2504.15077v1  [cs.LG]  21 Apr 2025Think2SQL: Reinforce LLM Reasoning Capabilities
for Text2SQL
Simone Papicchio
Politecnico di Torino, Turin, Italy
EURECOM, Biot, France
simone.papicchio@polito.it
simone.papicchio@eurecom.frSimone Rossi
EURECOM, Biot, France
simone.rossi@eurecom.fr
Luca Cagliero
Politecnico di Torino, Turin, Italy
luca.cagliero@polito.itPaolo Papotti
EURECOM, Biot, France
paolo.papotti@eurecom.fr
Abstract
Large Language Models (LLMs) have shown impressive capabil ities in transform-
ing natural language questions about relational databases into SQL queries. De-
spite recent improvements, small LLMs struggle to handle qu estions involving
multiple tables and complex SQL patterns under a Zero-Shot L earning (ZSL) set-
ting. Supervised Fine-Tuning (SFT) partially compensate t he knowledge deﬁcits
in pretrained models but falls short while dealing with quer ies involving multi-
hop reasoning. To bridge this gap, different LLM training st rategies to reinforce
reasoning capabilities have been proposed, ranging from le veraging a thinking
process within ZSL, including reasoning traces in SFT, or ad opt Reinforcement
Learning (RL) strategies. However, the inﬂuence of reasoni ng on Text2SQL per-
formance is still largely unexplored.
This paper investigates to what extent LLM reasoning capabi lities inﬂuence their
Text2SQL performance on four benchmark datasets. To this en d, it considers the
following LLM settings: (1) ZSL, including general-purpos e reasoning or not; (2)
SFT, with and without task-speciﬁc reasoning traces; (3) RL , leveraging execution
accuracy as primary reward function; (4) SFT+RL, i.e, a two- stage approach that
combines SFT and RL.
The results show that general-purpose reasoning under ZSL p roves to be ineffec-
tive in tackling complex Text2SQL cases. Small LLMs beneﬁt f rom SFT with
reasoning much more than larger ones, bridging the gap of the ir (weaker) model
pretraining. RL is generally beneﬁcial across all tested mo dels and datasets, partic-
ularly when SQL queries involve multi-hop reasoning and mul tiple tables. Small
LLMs with SFT+RL excel on most complex datasets thanks to a st rategic balance
between generality of the reasoning process and optimizati on of the execution ac-
curacy. Thanks to RL, the 7B Qwen-Coder-2.5 model performs o n par with 100+
Billion ones on the Bird dataset.
1 Introduction
The ever-increasing volume of data stored in relational dat abases and the impressive diffusion of
Large Language Models (LLMs) have jointly paved the way for n ew accessible ways to query multi-
table databases. The Text2SQL task involves converting nat ural language questions about relational
tables into executable SQL queries [ 12]. Thanks to Text2SQL models, end-users who are not proﬁ-
Preprint. Under review.
cient in SQL coding can simply access relational data by usin g LLMs as a proxy. Tackling Text2SQL
is particularly challenging as not only involves expressin g ﬁrst- or second-order logic conditions in
SQL but also reasoning about the underlying question’s mean ing and its relation to the database
schema [ 13].
Neural network-based solutions to Text2SQL have evolved fr om classical sequence-to-sequence
and graph networks (e.g., [ 54,2]) to Transformer-based architectures [ 1] and, more recently, to
LLM-based solutions [ 27]. Thanks to the advanced language understanding capabilit ies of their
pretrained models, LLMs have remarkably boosted Text2SQL p erformance, particularly on multi-
table datasets [ 26,56]. However, LLMs’ performance under Zero-Shot Learning (ZS L) signiﬁcantly
varies depending on the number of model parameters [ 5]. While small LLMs (i.e., models with 3-8
Billions of parameters) suffer from limited language under standing and reasoning capabilities, larger
ones are typically trained on multi-domain data thus lackin g the adequate level of specialization to
be competitive on domain-speciﬁc data [ 6].
To overcome the limitations of ZSL, Supervised Fine-Tuning (SFT) is among the mostly used LLM
adaptation strategies [ 52]. It entails specializing the language model parameters fo r a given down-
stream task, such as Text2SQL. Since SFT requires task-spec iﬁc data (e.g., pairs of natural language
questions and the corresponding SQL queries), the curation of an annotated SQL-centric corpus [ 25]
is critical. Moreover, even when training data and resource s are appropriate, small LLMs typically
show limited generality and reasoning capabilities, espec ially while coping with complex database
schema and SQL patterns [ 36].
Reinforcement Learning (RL) techniques have recently prov ed to be the most effective in improving
LLM reasoning capabilities [ 10]. Although this LLM training strategy have led to state-of- the-art
results in several downstream tasks, such as mathematical r easoning and Python/Java coding, its
inﬂuence on Text2SQL performance is still largely unexplor ed.
In this paper, we thoroughly analyze the inﬂuence of LLM reas oning capabilities on Text2SQL
performance. To achieve this goal, we evaluate both pretrai ned LLMs with thinking capabilities and
LLMs specialized for reasoning on Text2SQL under the follow ing settings:
Zero-Shot Learning (ZSL) with general-purpose reasoning . We consider pretrained LLMs
(e.g., [ 41,21]) that already incorporate the reasoning steps in their pre trained model, but are not
speciﬁcally suited to the Text2SQL task.
Supervised Fine-Tuning (SFT) with reasoning . We ﬁne-tune small LLMs for the Text2SQL task.
We prepare a task-speciﬁc dataset covering SQL patterns wit h varying levels of complexity. SFT
examples are enriched with reasoning traces to make the prob lem solving step explicit to the LLM
during training.
Reinforcement Learning (RL) . We tailor RL to the Text2SQL task. The LLM repetitively perf orms
actions consisting of shortlisting the best SQL query to sol ve the input question among a predeﬁned
set of candidates. We adopt Group-Relative Policy Optimiza tion (GRPO) [ 41] and reward the LLM
answers by using the established execution accuracy metric [56].
Supervised Fine-Tuning and Reinforcement Learning (SFT+R L). We employ a two-step ap-
proach combining SFT with RL. The idea behind it is to special ize the model on problem solving
using RL while keeping the generality of reasoning models [ 10].
Our evaluation aims to address the following Research Quest ions (RQs):
RQ1 ) Is reasoning beneﬁcial for Text2SQL performance under dif ferent LLM training settings?
RQ2 ) Which is the most appropriate strategy to train LLMs to reas on about Text2SQL?
RQ3 ) Which is the best trade-off between model generalization a nd specialization?
To answer RQ1, we compare the results of LLMs under the ZSL set ting with and without reasoning
as well as the performance of LLMs under the SFT setting with a nd without reasoning traces. The
goal is to explore the inﬂuence of reasoning on the performan ce of LLMs with different numbers of
parameters, pretraining strategies, and across testing da tasets with different characteristics.
To answer RQ2, we compare the performance of LLMs under (1) ZS L with reasoning vs. (2) SFT
with reasoning vs. (3) RL vs. (4) SFT+RL. The goal is to compar e different strategies to incor-
2
porate reasoning capabilities in LLM training for Text2SQL , with particular attention to the model
performance achieved on complex SQL patterns.
To answer RQ3, we analyze the LLMs’ performance across diver se datasets, ranging from general-
purpose to domain-speciﬁc content. The purpose is to clarif y whether reasoning is beneﬁcial to
achieve model generality across different datasets and dom ains while preserving the overall accuracy
of the SQL query generator.
The rest of the paper is organized as follows. Section 2introduces the preliminary notions and the
Text2SQL problem formulation. Section 3describes the methodology used to assess the inﬂuence
of reasoning on Text2SQL performance. Section 4describes the experimental settings and the main
results. Finally, Section 5draws conclusions and discusses limitations and future ext ensions of the
present work.
2 Preliminaries
In this section, we introduce the notation and fundamental f ormulations used throughout this work.
Let a sequence of discrete tokens be represented as z= (z1,z2,...,z T),where each zt∈ V, and
Vis a ﬁnite vocabulary set with cardinality V. We consider a large language model (LLM) param-
eterized by θ, formalized as a probabilistic autoregressive model πθ, instantiated as a decoder-only
transformer architecture [ 38]. For a given sequence z, the model deﬁnes a factorized distribution
over the sequence space:
πθ(z) =T/productdisplay
t=1πθ(zt|z<t), (1)
wherez<t= (z1,...,z t−1)is the left-truncated context of length t−1. Each conditional probability
πθ(zt|z<t)is computed via a series of masked multi-head self-attentio n layers. Causal masking
ensures that attention weights for token ztare computed only over z<t, preserving the autoregressive
property. All model parameters, including the token embedd ings, attention weights, feedforward
weights, normalization scales, and the output projection m atrix, are collected in θ.
2.1 Fine-Tuning Language Models
Supervised Fine-Tuning (SFT) adapts a pretrained language modelπθto a distribution Pof se-
quences that reﬂect desired linguistic or task-speciﬁc beh avior. Let z= (z1,z2,...,z T)denote a
token sequence drawn from z∼ P. The SFT objective maximizes the likelihood of sequences un der
πθ, which corresponds to minimizing the expected negative log -likelihood:
Lfull(θ) =−Ez∼P/bracketleftBiggT/summationdisplay
t=1logπθ(zt|z<t)/bracketrightBigg
. (2)
For tasks with an explicit decomposition into an input segme nt and a target segment—such as QA,
summarization, or assistant-style dialogue—the data dist ribution consists of pairs (x,y)∼ P, where
x= (x1,...,x n)is the conditioning prompt and y= (y1,...,y m)is the supervised output. In such
settings, the model conditions on xand predicts the continuation y, with the loss computed only over
the target tokens:
Lcond(θ) =−E(x,y)∼P/bracketleftBiggm/summationdisplay
t=1logπθ(yt|x/bardbly<t)/bracketrightBigg
, (3)
where/bardbldenotes the concatenation operator. This alternative obje ctive is often preferred in practice,
as it allows for more efﬁcient training by focusing on the rel evant output tokens and ignoring the
input tokens [ 9,57,51]. More recently, Shi et al. [ 42] showed that models trained with the SFT
objective in Eq. (2) can be superior to Eq. (3) when the target sequence is signiﬁcantly shorter than
the input sequence. In the case of distillation of reasoning models, the output sequence will be
considerebly longer than the input sequence, and the SFT obj ective in Eq. (3) is preferred. Finally,
the expectations in Eq. (2) andEq. (3) are approximated by empirical means over a ﬁnite dataset
D={zi}N
i=1orD={(xi,yi)}N
i=1consisting of Ntraining examples. The resulting objective is
optimized via standard stochastic gradient descent or its v ariants [ 39,22].
3
2.2 Reinforcement Learning for Language Models
Reinforcement Learning from Human Feedback (RLHF) typical ly relies on policy optimization al-
gorithms to ﬁne-tune a language model πθtoward reward-aligned behavior. One of the standard
approaches is Proximal Policy Optimization (PPO) [ 40], which constrains policy updates through a
clipped surrogate objective and value-based advantage est imation. However, PPO necessitates learn-
ing and maintaining an auxiliary value function, which intr oduces instability and potential reward
misestimation.
Group-Relative Policy Optimization (GRPO) [ 41] offers a value-free alternative by computing nor-
malized group-level advantages based directly on realized rewards. Let x∼ X denote a prompt
drawn from a distribution over conditioning inputs, and let {yi}G
i=1∼πθold(· |x)beGresponse
sequences generated by the frozen reference policy πθold. Each response yi= (yi,1,...,y i,Ti)is
assigned a scalar reward Ri∈Rcomputed via a reward model.
The group-relative advantage Aifor thei-th response is deﬁned by normalizing the reward distribu-
tion over the group:
Ai=Ri−E[Rj]/radicalbig
V[Rj], j∈ {1,...,G}, (4)
whereE[Rj]andV[Rj]are the mean and variance of the rewards for the group of respo nses, re-
spectively. For each token position tin response yi, deﬁne the state as si,t=x/bardblyi,<t, and the
token-level probability ratio as
pi,t(θ) =πθ(yi,t|si,t)
πθold(yi,t|si,t).
The GRPO training objective minimizes a clipped surrogate l oss penalized by the KL divergence
from the reference policy:
LGRPO(θ) =E/bracketleftBigg
1
GG/summationdisplay
i=11
TiTi/summationdisplay
t=1min/parenleftbig
pi,t(θ)Ai,clip/parenleftbig
pi,t(θ),1−ǫ,1+ǫ/parenrightbig
Ai/parenrightbig
−βKL[πθ/bardblπθref]/bracketrightBigg
,
(5)
where the expectation is taken over the prompt distribution x∈ X , and the responses {yi}G
i=1
generated by the frozen policy πθold. Additionally, ǫis set to be the clipping parameter and βcontrols
the Kullback-Leibler divergence (KL) regularization by pe nalizing models that deviate from the
reference policy πθref(which is typically the initial pretrained model).
2.2.1 Rule-based Reward Modeling
Reward modeling is central to reinforcement learning with l anguage models, as it deﬁnes the op-
timization signal guiding the policy πθ. Learned neural reward models are commonly employed
to approximate human preferences or task-speciﬁc goals. Ho wever, they often suffer from distribu-
tional mismatch, reward hacking, and spurious correlation s [17,53,11]. These effects arise when
the model exploits imperfections in the reward predictor, l eading to high-reward outputs that do not
correspond to true task success.
An alternative is to design rule-based reward models, which deﬁne deterministic mappings from
model outputs to scalar reward values via explicit criteria . In the context of coding, for instance,
a reward function R:y/mapsto→[0,1]can be constructed by executing the generated code yagainst a
test suite and returning the fraction of passed unit tests. S uch rule-based models directly encode
correctness and task satisfaction, avoiding pathologies i ntroduced by learned approximators.
Formally, let x∼ X denote the input (e.g., a natural language instruction), an dy∼πθold(· |x)
a candidate response. The reward function R(x,y)∈Ris deﬁned deterministically via evalua-
tion procedures speciﬁed a priori. These functions are task -dependent and vary across application
domains. The resulting reward is used to construct advantag e estimates, as in GRPO.
A well-known limitation of rule-based reward models is the s parsity of the reward signal. In many
structured tasks, the reward R(x,y)may remain zero across most model outputs and attain nonzero
values only when the generation exactly satisﬁes task const raints. This sparsity complicates credit
assignment during training and may impair exploration in RL -based optimization. Techniques such
as reward shaping, curriculum learning, or relaxed matchin g criteria are sometimes introduced to
4
mitigate this issue [ 31,44,17,30]. Nonetheless, provided that the policy starts from a sufﬁc iently
strong pretrained model, this approach has been successful ly adopted in multiple recent frameworks
across general and specialized RLHF pipelines [ 10,46,55], and has been particularly effective in
settings where ground truth veriﬁcation criteria exist, su ch as program synthesis [ 23,18,8].
2.3 Text2SQL
The Text2SQL task consists in mapping natural language ques tion on databases to executable SQL
code. Let x∈ X denote a natural language input (e.g., a user question), and lety∈ Y sqldenote a
corresponding structured output in SQL syntax. The output s paceYsqlcomprises syntactically valid
SQL codes consistent with a given database schema S, which speciﬁes the collection of relational
tables, attributes, and their types. In addition to the sche ma, auxiliary context Mmay be provided.
This includes task-speciﬁc metadata such as database descr iptions, natural language annotations,
examples of prior queries, or column-level summaries.
The schema is tokenized into a uniﬁed model-readable repres entation via a a deterministic transfor-
mationφ:S /mapsto→ V , which converts the schema Sinto a token sequence φ(S)compatible with the
model’s input vocabulary. For this study, we adopt the schem a representation prompt commonly
used in prior works for its proven effectiveness [ 4,16].
The model πθdeﬁnes a conditional distribution over the SQL code y= (y1,...,y T)given the input
xand the schema context:
πθ(y|x,φ(S),M) =T/productdisplay
t=1πθ(yt|x/bardblφ(S)/bardblM/bardbly<t). (6)
For each prompt x, the target may be a set of logically equivalent parses Y∗(x)⊆ Y sql, all yielding
identical execution results. Learning may proceed by optim izing the marginal log-likelihood over
this set or by selecting a canonical representative from Y∗(x)during training [ 58,50,43]. In this
study, the latter approach is adopted, and the model is train ed to predict a single SQL code y∗∈
Y∗(x)for each input x.
To manage large schemas, modern systems restrict φ(S)to a localized substructure φ(Sx), where
Sx⊆ S is retrieved via schema linking, lexical overlap, or learne d attention [ 49,37,45,7,3]. To
better isolate the reasoning process in Text2SQL and disent angle schema linking from SQL genera-
tion, we restrict Sxduring both training and inference to include only the table s and their complete
schema that are directly relevant to the question x.
3 Methodology
To evaluate the inﬂuence of reasoning on the Text2SQL task, w e employed several training strategies.
These strategies include supervised ﬁne-tuning (SFT), rei nforcement learning (RL), and a hybrid
approach combining both. Each strategy was designed to asse ss the impact of reasoning traces on
model performance.
Supervised Fine-Tuning (SFT). In the SFT approach, the model was trained on the curated data set
described in Section 3.1. The training objective was to minimize the cross-entropy l oss between the
predicted SQL code and the ground truth SQL code. Reasoning t races were included as additional
input to guide the model in understanding the logical steps r equired to generate the correct SQL.
Reinforcement Learning (RL). For RL, we used execution accuracy as the primary reward sign al.
The model was ﬁne-tuned using the GRPO algorithm, where the r eward was computed based on the
correctness of the generated SQL code’s execution results. To encourage the generation of reasoning
traces, we also included a secondary reward signal based on s yntatical checks. Details on the rewards
are provided in Section 3.2.
Hybrid Approach. The hybrid approach combined SFT and RL. The model was ﬁrst tr ained us-
ing SFT to leverage the labeled dataset and then ﬁne-tuned wi th RL to further optimize execution
accuracy and reasoning quality. This two-stage training pr ocess aimed to balance the beneﬁts of
supervised learning and reinforcement learning.
5
3.1 SFT Dataset Creation
This section describes the creation of a complex reasoning d ataset tailored for the Text2SQL task.
Text2SQL was chosen due to its practical signiﬁcance, its pr ominence in recent advancements [ 19],
and its familiarity to large language models (LLMs). LLMs ha ve shown strong performance on
established benchmarks [ 56,26,24]. Furthermore, SQL queries, unlike some other logical form s,
can be executed and veriﬁed for correctness, making them par ticularly suitable for this study.
Data Collection. The initial phase of our methodology involved the acquisiti on of high-quality,
human-annotated Text2SQL datasets. For this study, we sele cted the BIRD dataset [ 26], recognized
for its extensive scope and diversity.
The BIRD training set comprises 9,428 data points derived fr om 69 heterogeneous databases span-
ning 37 professional domains, including blockchain, healt hcare, education, and hockey. Each data
point consists of a natural language (NL) question , a corresponding SQL code , and supplementary
evidence . The evidence serves as additional context to resolve ambig uities in the schema or NL
questions.
Data Quality and Complexity Curation. To ensure the reliability and robustness of the dataset, we
implemented a rigorous two-step curation process [ 29]. First, we ﬁltered out erroneous SQL queries
and removed duplicate entries, resulting in the exclusion o f 421 instances. Second, we categorized
the remaining SQL queries based on their complexity, deﬁned by the number of SQL constructs,
into three tiers: low ( [0,7)), medium ( [7,10)), and high ( [10,+∞)). This stratiﬁcation yielded a
dataset distribution of 7,022 simple-complexity queries ( 77%), 1,549 medium-complexity queries
(17%), and 492 challenging queries (6%).
Prompt Synthetic data annotation
Answer the following question with the SQL code. Use the piec e of evidence and base
your answer on the database schema.
Given the question, the evidence and the database schema, re turn in the answer tags only
the SQL script that addresses the question.
Question:
<question>
Evidence:
<evidence>
Database Schema:
<schema>
Figure 1: Prompt used for the synthetic data annotation. <question> ,<evidence> , and<schema>
are placeholders for the actual question, evidence, and dat abase schema, respectively. The model is
expected to generate a SQL code snippet that answers the ques tion based on the provided evidence
and schema.
Synthetic Data Annotation. To enhance the dataset with reasoning traces, we utilized th e
DeepSeek-R1 model [ 10] along with its system prompt. The prompt used for synthetic annotation is
detailed in Figure 1. The hyperparameters optimized for reasoning tasks were se t to a temperature of
0.7 and a top-p of 0.95, following established best practice s in the ﬁeld [ 48,32]. This conﬁguration
ensured high-quality annotations that explicitly outline reasoning steps, enriching the dataset and
enabling the model to better comprehend the logical process es underlying SQL generation.
The ﬁnal annotated dataset consists of 1,142 instances, dis tributed as follows: 684 simple queries,
265 medium-complexity queries, and 193 challenging querie s. The 75th percentile of reasoning
token counts is 509 for simple queries, 861 for medium querie s, and 869 for challenging queries.
The dataset will be made available on Hugging Face1.
1To appear soon
6
3.2 Rewards for Reinforcement Learning
In reinforcement learning, reward signals are crucial for g uiding the model’s learning pro-
cess [ 17,53,11]. Execution accuracy, the primary reward for Text2SQL, mea sures the correctness
of generated SQL by comparing it to the ground truth. However , its binary nature poses challenges
for RL optimization, especially for smaller LLMs, as reward s often remain zero unless the SQL
is exactly correct. To encourage the model’s reasoning proc ess, we introduce an auxiliary reward
signal that evaluates the correctness of the reasoning trac e based on the appropriate use of reasoning
tags [ 10]. Additionally, to mitigate reward hacking, a penalty is ap plied when reasoning tokens are
redundantly or excessively repeated within the reasoning t race.
Execution Accuracy (EX). Execution accuracy [ 56,26] evaluates the correctness of the generated
SQL by comparing its execution results against those of the g round truth SQL on the target database.
This metric provides a binary reward, assigning a full score only when the two execution results
match exactly, row by row. While straightforward and reliab le, execution accuracy has a notable
limitation: it does not account for partially correct resul ts, such as queries that produce subsets or
supersets of the expected results, which can hinder the lear ning process in reinforcement learning
scenarios.
Format Reward. The format reward [ 10] incentivizes the model to adhere to a predeﬁned output
structure, such as the use of <think> and<answer> tags. Outputs that conform to this structure
receive a reward boost, promoting clarity and consistency. This is a sparse reward that activates only
when both the opening and closing tags for reasoning and answ ers are correctly positioned. The
reward value is 1 if the tags are correctly formatted; otherw ise, it is 0.
Tag Count Reward. To address reward hacking, where reasoning traces include u nnecessary or
excessive tags, we introduce a tag count reward. This reward penalizes the model for generating
reasoning traces with redundant tags. The reward is 1 if the n umber of tags is exactly two (one
opening and one closing tag); otherwise, it is 0.
The ﬁnal reward signal is computed as a weighted sum of the ind ividual rewards. The weights were
carefully chosen to ensure a balanced contribution from eac h reward component while maintaining
a total score of 1. This design prevents training instabilit y caused by excessively high rewards. The
ﬁnal reward is computed as follows:
R= 0.85·REX+0.10·RFormat+0.05·RTag Count, R∈[0,1] (7)
4 Reasoning for Text2SQL
4.1 Experiment Setup
The experiments were designed to evaluate the impact of reas oning on the Text2SQL task. We
employed several training strategies, including supervis ed ﬁne-tuning (SFT), reinforcement learning
(RL), and a hybrid approach combining both. Each strategy wa s designed to assess the inﬂuence of
reasoning traces on model performance. The research questi ons we aim to address are:
•RQ1 : Does reasoning improve the performance of Text2SQL models ?
•RQ2 : What is the best training strategy to learn reasoning for Te xt2SQL?
•RQ3 : How do models generalize to unseen databases?
Training and Evaluation datasets. For training, we use two datasets: the original BIRD dataset
for RL and the reasoning-augmented BIRD dataset described i n Section 3.1for SFT. The original
BIRD dataset is ﬁltered to remove duplicates and erroneous S QL queries, resulting in a cleaned
set of9,007examples (after discarding 412instances). The reasoning-augmented BIRD dataset
contains1,142examples, which are split into 913training and 229validation samples, following an
80%/20% ratio.
We evaluate our models on the BIRD development set, which con sists of1,530instances (Sim-
ple#924 , Medium #461 and Challenging #143 ), as the test set is not publicly available. To
7
Table 1: Performance comparison of open-source and proprie tary models on the Bird Dev dataset
for the Text2SQL task. All models were evaluated with a tempe rature setting of 0.7 and a top p
value of 0.95. Llama models correspond to version 3.1 and Turbo means the model is quantized
8bit. Think2SQL-3B and Think2SQL-7B denote the models trai ned exclusively with RL.
Model Reasoning Simple Medium Challenging Weigthed A VG
Open-source LLMs ( <10B)
DeepSeek-Qwen-1.5B ✓ 0.056 0 .004 0 .0 0.035
Qwen2.5-Coder-0.5B ✓ 0.126 0 .033 0 .035 0.089
DeepSeek-Qwen-7B ✓ 0.297 0 .113 0 .049 0.218
Qwen2.5-Coder-1.5B ✓ 0.351 0 .184 0 .077 0.275
Llama3-8b ✗ 0.436 0 .260 0 .133 0.355
Qwen2.5-Coder-3B ✗ 0.469 0 .267 0 .196 0.382
Qwen2.5-Coder-7B ✗ 0.548 0 .388 0 .294 0.476
Open-source LLMs (10-100B)
DeepSeek-Qwen-32B ✓ 0.542 0 .347 0 .217 0.453
DeepSeek-Llama-70B ✓ 0.552 0 .371 0 .203 0.465
QwQ-32B ✓ 0.550 0 .427 0 .280 0.488
Qwen2.5-Coder-14B ✗ 0.610 0 .456 0 .364 0.541
Llama-70B-Turbo ✗ 0.618 0 .469 0 .350 0.548
Qwen2.5-Coder-32B ✗ 0.623 0 .482 0 .329 0.553
Open-source LLMs ( >100B)
DeepSeek-R1 ✓ 0.588 0 .440 0 .294 0.518
Llama-405B-Turbo ✗ 0.630 0 .477 0 .371 0.560
Closed-source LLMs
gpt-4o-mini-2024-07-18 ✗ 0.545 0 .401 0 .301 0.479
o3-mini-2025-01-31 ✓ 0.561 0 .406 0 .329 0.510
gpt-4o-2024-08-06 ✗ 0.619 0 .447 0 .343 0.541
Our Models
Think2SQL-3B ✓ 0.569 0 .381 0 .196 0.485
Think2SQL-7B ✓ 0.619 0 .463 0 .406 0.552
assess model robustness and generalization, we also evalua te on the SPIDER dataset [ 56], a widely
recognized benchmark for Text-to-SQL tasks, along with its challenging variants Spider-Syn [ 14]
and Spider-DK [ 15]. Spider-Syn tests robustness to paraphrased questions by introducing schema-
related synonyms, while Spider-DK evaluates the model’s ab ility to incorporate domain knowledge
by modifying both natural language questions and correspon ding SQL queries to include implicit
relationships or background knowledge not explicitly stat ed in the schema. For all evaluations, we
report Execution Accuracy as the primary metric.
Training setup. To answer the posed research questions, we trained multiple models using different
training strategies. Our experiments are based on the Qwen- Coder-2.5 model family [ 21], focusing
speciﬁcally on the 3B and 7B variants.
We consider three training approaches: supervised ﬁne-tun ing (SFT), reinforcement learning (RL),
and a hybrid approach (SFT + RL) that combines both. In the res ults section, each model is denoted
with the respective subscript to indicate the training stra tegy used.
For SFT, models are trained for 5 epochs using a batch size of 1 28 and a learning rate of 4×10−5,
with the AdamW optimizer [ 28]. Training is conducted on 4 NVIDIA A100 GPUs, each with 80GB
of memory.
For RL, we employ the GRPO algorithm [ 41] with a batch size of 256, a learning rate of 1×10−6,
and 16 generations per batch, training for 1 epoch. This setu p uses 8 NVIDIA H100 GPUs, each
also with 80GB of memory.
The hybrid approach (SFT + RL) involves initializing the RL t raining from a model that has been
previously ﬁne-tuned with SFT.
8
During both training and evaluation, we use the same prompt i n Figure 1and restrict the database
schema to include only the tables relevant to the given quest ion. This design choice is deliberate:
it helps isolate the reasoning capabilities of the model by r emoving the confounding inﬂuence of
schema linking. This allows us to more directly assess the mo del’s ability to generate SQL from
natural language. Our approach aligns with recent work in th e Text2SQL domain [ 7,3], where SQL
generation is treated separately from schema linking.
Model Baselines. To validate our results, we compare a range of open- and close d-source models,
with and without reasoning capabilities. Table 1summarizes the selected models, which vary in
size and architecture. All models are evaluated in zero-sho t mode using the same training prompt
(Figure 1), with a temperature of 0.7, top p of0.95, and a 30k token generation limit.
We include two main model families: Qwen-Coder-2.5 and LLaM A 3.1 [ 20]. Qwen-Coder-2.5
allows comparison between reasoning (our) and non-reasoni ng variants. We also include the recent
general-purpose reasoning model from the Qwen family; QwQ [ 47]. The LLaMA family is used to
benchmark DeepSeek-R1 distilled versions against their so urce models. We also include the 405B
LLaMA and the 671B DeepSeek-R1 models.
Among closed-source models, we include o3-mini [ 35], GPT-4o [ 34], and its mini variant [ 33]. Mod-
els over 70B parameters are evaluated via Together-AI, and c losed-source models via the OpenAI
API.
4.2 Main Results
The Table 1presents a performance comparison of various open-source a nd proprietary models
on the Bird Dev dataset for the Text2SQL task, categorized by model size and reasoning capa-
bility. Among open-source models, the Qwen2.5-Coder-32B achieved the highest weighted av-
erage accuracy (0.553) in the 10-100B category, while Llama-405B-Turbo led the>100B cate-
gory with a weighted average of 0.560. Proprietary models al so performed competitively, with
gpt-4o-2024-08-06 achieving a weighted average of 0.541. The Think2SQL models demon-
strate strong performance across all evaluation categorie s, particularly on challenging examples.
Think2SQL-7B achieves a weighted average score of 0.552, ranking second o verall among all
models tested—outperforming both open-source and closed- source models of signiﬁcantly larger
size, including Llama-70B-Turbo and multiple GPT-4 varian ts. Notably, it achieves the highest
score on the Challenging subset (0.406), indicating superior reasoning and general ization abilities.
Think2SQL-3B also performs competitively, with a weighted average of 0.4 85, surpassing all mod-
els below 10B parameters and several larger models, such as Q wQ-32B and DeepSeek-Llama-70B.
When directly compared to their non-reasoning counterpart s of similar size, the Think2SQL mod-
els consistently outperform them. Think2SQL-3B exceedsQwen2.5-Coder-3B in all difﬁculty
categories, with a weighted average of 0.485 ( +0.10). Likewise, Think2SQL-7B outperforms
Qwen2.5-Coder-7B across the board, particularly on moderate and challenging instances-0.388
vs. 0.463 ( +0.07) and 0.294 vs 0.406 ( +0.11), respectively. These improvements highlight the ef-
fectiveness of incorporating reasoning in training and und erscore the competitive edge of our models
even when compared to state-of-the-art baselines with simi lar architectures and parameter counts.
Reasoning capabilities do not always lead to improved perfo rmance. For instance, all the distilled
DeepSeek models with reasoning perform worse than their non -reasoning counterparts. This sug-
gests that generic reasoning skills are not sufﬁcient to sol ve the Text2SQL task effectively. Instead,
these results highlight the importance of task-speciﬁc tra ining: models must be explicitly exposed
to structured reasoning within the domain in order to learn h ow to apply reasoning effectively. With-
out this targeted supervision, even models equipped with ge neral reasoning abilities may struggle to
generalize to complex, domain-speciﬁc queries.
Takeaway 1: Think2SQL
Incorporating reasoning signiﬁcantly enhances model perf ormance on the Text2SQL task,
particularly for challenging examples. However, general r easoning capabilities alone are
insufﬁcient. Task-speciﬁc reasoning traces are essential for better performance.
9
Table 2: Ablation over different training strategies
Model Simple Medium Challenging A VG
Qwen2.5-Coder-3B 0.469 0 .267 0 .196 0.382
Qwen2.5-Coder-3B-SFT NO−THINK 0.510 0 .325 0 .224 0.427
Qwen2.5-Coder-3B-SFT 0.531 0 .366 0 .301 0.460
Qwen2.5-Coder-3B-RL EX 0.569 0 .381 0 .273 0.485
Qwen2.5-Coder-3B-SFT-RL EX 0.560 0 .370 0 .343 0.482
Qwen2.5-Coder-7B 0.548 0 .388 0 .294 0.476
Qwen2.5-Coder-7B-SFT 0.548 0 .403 0 .287 0.479
Qwen2.5-Coder-7B-RL EX 0.619 0.463 0.406 0.552
Qwen2.5-Coder-7B-SFT-RL EX 0.590 0 .422 0 .343 0.516
4.3 Ablation on different training strategies
Table 2presents an ablation study comparing different training st rategies applied to Qwen2.5-Coder
models of two sizes (3B and 7B parameters). Across both model sizes, we observe that applying
SFT with reasoning traces consistently improves performan ce over the base model. Notably, the
SFTNO−THINK variant, which lacks reasoning traces, shows a signiﬁcant d rop in performance
compared to the full SFT model, particularly on the more chal lenging examples. For the 3B model,
theSFTvariant outperforms the SFTNO−THINK of 0.04 points on average, indicating that the rea-
soning traces are beneﬁcial for the model’s performance.
The model trained only with RL further boosts performance, e specially on the more challenging
subsets. The combination of SFT and RL ( SFT-RL EX) leads to robust and balanced improvements,
although in some cases, pure RLEXslightly outperforms it.
For the larger Qwen2.5-Coder-7B model, we see a clear performance gain over its 3B counter-
part across all difﬁculty levels, indicating that model sca le remains a strong factor especially for
RL when trained with GRPO. The best overall result is obtaine d withQwen2.5-Coder-7B-RL EX,
achieving the highest average score (0.552) and the stronge st performance on Simple and Medium
and Challenging examples.
Takeaway 2: RL vs SFT
SFT improves over base models, especially for smaller LLMs. RLimproves performance
across all difﬁculty levels, particularly in more complex s cenarios.
Table 3: Analysis robustness for different datasets.
Model Spider EX% Spider-Syn EX% Spider-DK EX%
Qwen2.5-Coder-3B 0.725 0 .632 0 .602
Qwen2.5-Coder-3B-SFT 0.770 0 .718 0 .634
Qwen2.5-Coder-3B-RL EX 0.777 0 .717 0.680
Qwen2.5-Coder-3B-SFT-RL EX 0.783 0.760 0.658
Qwen2.5-Coder-7B 0.776 0 .703 0 .652
Qwen2.5-Coder-7B-SFT 0.799 0 .774 0 .652
Qwen2.5-Coder-7B-RL EX 0.804 0 .774 0.707
Qwen2.5-Coder-7B-SFT-RL EX 0.824 0.780 0.673
4.4 Reasoning robustness on different datasets
Table 3reports EX% of various training strategies across differen t dataset variants: the original
Spider dataset, its synonym-augmented version (Spider-Sy n), and the more challenging domain-
knowledge variant (Spider-DK). We observe that all trainin g strategies consistently outperform the
baseQwen2.5-Coder models for both the 3B and 7B sizes. This conﬁrms the generali zability of
our training strategies.
10
The best overall performance is achieved by the combined SFT-RL EXstrategy, which yields the
highest accuracy on Spider and Spider-Syn datasets, demons trating stronger generalization when
reasoning traces are incorporated during training. Howeve r, on Spider-DK, which demands deeper
domain knowledge, pure RL appears to offer an advantage— RLEXoutperforms all other strategies
on this subset for both model sizes.
Takeaway 3: Generalization with SFT + RL
Combining SFT and RL yields the most generalizable models, e xcelling across diverse
datasets. This suggests that integrating reasoning traces with reinforcement learning enables
models to better adapt to complex and unseen scenarios.
5 Conclusions, limitations, and future work
This paper investigated the inﬂuence of reasoning capabili ties on the performance of LLMs for the
Text2SQL task. We evaluated different training strategies —Zero-Shot Learning (ZSL) with and
without general-purpose reasoning, Supervised Fine-Tuni ng (SFT) with and without task-speciﬁc
reasoning traces, Reinforcement Learning (RL) optimizing execution accuracy, and a combined
SFT+RL approach—across multiple benchmark datasets.
Our ﬁndings answer three research questions. RQ1: while gen eral-purpose reasoning in pretrained
LLMs offers limited beneﬁts for complex Text2SQL under ZSL, incorporating task-speciﬁc reason-
ing traces via SFT signiﬁcantly improves performance, part icularly for smaller models. RQ2: RL
proved highly effective across all models and datasets, esp ecially for queries demanding multi-hop
reasoning - pure RL often yielded the best performance on cha llenging subsets. RQ3: the combined
SFT+RL strategy demonstrated strong generalization acros s diverse datasets, suggesting it strikes
an effective balance between learning general reasoning pa tterns (via SFT) and optimizing for task-
speciﬁc correctness (via RL). Our Think2SQL-7B model, trai ned with RL, achieved performance
on par with models exceeding 100 billion parameters on the BI RD dataset, showcasing the power of
targeted reasoning reinforcement.
Despite these promising results, this study has limitation s. We deliberately isolated the SQL genera-
tion process by providing the relevant database schema subs et, excluding the challenge of automated
schema linking, which is critical in real-world applicatio ns. Our RL reward function, while effec-
tive, primarily relied on binary execution accuracy, which can be sparse. Furthermore, the SFT
training relied on synthetically generated reasoning trac es, whose style and quality might inﬂuence
outcomes. The core experiments focused on the Qwen-Coder-2 .5 family, and ﬁndings might vary
across different model architectures.
Future work should focus on exploring more sophisticated re ward modeling for RL, potentially in-
corporating learned reward models or rewarding partial que ry correctness to further enhance training
efﬁciency and performance. Evaluating these reasoning-fo cused training strategies on a wider range
of LLM architectures and sizes, as well as on diverse, potent ially proprietary or enterprise-speciﬁc
datasets, would also be valuable. Finally, a deeper qualita tive analysis of model failure cases could
provide further insights into how different reasoning stra tegies succeed or falter on speciﬁc types of
complex queries.
References
[1] G. Badaro, M. Saeed, and P. Paolo. Transformers for Tabul ar Data Representation: A Survey
of Models and Applications. Transactions of the Association for Computational Linguis tics,
11:227–249, 2023. doi: doi.org/10.1162/tacl a00544. (p. 2)
[2] B. Bogin, J. Berant, and M. Gardner. Representing schema structure with graph neural
networks for text-to-sql parsing. In A. Korhonen, D. R. Trau m, and L. M` arquez, edi-
tors, Proceedings of the 57th Conference of the Association for Co mputational Linguistics,
ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers , pages 4560–
4565. Association for Computational Linguistics, 2019. do i: 10.18653/V1/P19-1448. URL
https://doi.org/10.18653/v1/p19-1448 . (p. 2)
11
[3] H. A. Cafero˘ glu and ¨O. Ulusoy. E-sql: Direct schema linking via question enrich ment in
text-to-sql. arXiv preprint arXiv:2409.16751 , 2024. (pp. 5and9)
[4] S. Chang and E. Fosler-Lussier. How to prompt llms for tex t-to-sql: A study in zero-shot,
single-domain, and cross-domain settings. arXiv preprint arXiv:2305.11853 , 2023. (p. 5)
[5] Y . Chang, X. Wang, J. Wang, Y . Wu, L. Yang, K. Zhu, H. Chen, X . Yi, C. Wang, Y . Wang,
W. Ye, Y . Zhang, Y . Chang, P. S. Yu, Q. Yang, and X. Xie. A survey on evaluation of large
language models. ACM Trans. Intell. Syst. Technol. , 15(3), mar 2024. ISSN 2157-6904. doi:
10.1145/3641289. URL https://doi.org/10.1145/3641289 . (p. 2)
[6] P. B. Chen, F. Wenz, Y . Zhang, M. Kayali, N. Tatbul, M. J. Ca farella, C ¸ . Demi-
ralp, and M. Stonebraker. BEA VER: an enterprise benchmark f or text-to-
sql. CoRR , abs/2409.02038, 2024. doi: 10.48550/ARXIV .2409.02038. URL
https://doi.org/10.48550/arXiv.2409.02038 . (p. 2)
[7] S.-A. Chen, L. Miculicich, J. Eisenschlos, Z. Wang, Z. Wa ng, Y . Chen, Y . Fujii, H.-T. Lin,
C.-Y . Lee, and T. Pﬁster. Tablerag: Million-token table und erstanding with language models.
Advances in Neural Information Processing Systems , 37:74899–74921, 2024. (pp. 5and9)
[8] X. Chen, M. Lin, N. Sch¨ arli, and D. Zhou. Teaching large l anguage models to self-debug.
arXiv preprint arXiv:2304.05128 , 2023. (p. 5)
[9] W.-L. Chiang, Z. Li, Z. Lin, Y . Sheng, Z. Wu, H. Zhang, L. Zh eng, S. Zhuang, Y . Zhuang, J. E.
Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.
See https://vicuna. lmsys. org (accessed 14 April 2023) , 2(3):6, 2023. (p. 3)
[10] DeepSeek-AI, D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang , R. Xu, Q. Zhu, S. Ma, P. Wang,
X. Bi, X. Zhang, X. Yu, Y . Wu, Z. F. Wu, Z. Gou, Z. Shao, Z. Li, Z. G ao, A. Liu, B. Xue,
B. Wang, B. Wu, B. Feng, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Rua n, D. Dai, D. Chen,
D. Ji, E. Li, F. Lin, F. Dai, F. Luo, G. Hao, G. Chen, G. Li, H. Zha ng, H. Bao, H. Xu,
H. Wang, H. Ding, H. Xin, H. Gao, H. Qu, H. Li, J. Guo, J. Li, J. Wa ng, J. Chen, J. Yuan,
J. Qiu, J. Li, J. L. Cai, J. Ni, J. Liang, J. Chen, K. Dong, K. Hu, K. Gao, K. Guan, K. Huang,
K. Yu, L. Wang, L. Zhang, L. Zhao, L. Wang, L. Zhang, L. Xu, L. Xi a, M. Zhang, M. Zhang,
M. Tang, M. Li, M. Wang, M. Li, N. Tian, P. Huang, P. Zhang, Q. Wa ng, Q. Chen, Q. Du,
R. Ge, R. Zhang, R. Pan, R. Wang, R. J. Chen, R. L. Jin, R. Chen, S . Lu, S. Zhou, S. Chen,
S. Ye, S. Wang, S. Yu, S. Zhou, S. Pan, S. S. Li, S. Zhou, S. Wu, S. Ye, T. Yun, T. Pei,
T. Sun, T. Wang, W. Zeng, W. Zhao, W. Liu, W. Liang, W. Gao, W. Yu , W. Zhang, W. L.
Xiao, W. An, X. Liu, X. Wang, X. Chen, X. Nie, X. Cheng, X. Liu, X . Xie, X. Liu, X. Yang,
X. Li, X. Su, X. Lin, X. Q. Li, X. Jin, X. Shen, X. Chen, X. Sun, X. Wang, X. Song, X. Zhou,
X. Wang, X. Shan, Y . K. Li, Y . Q. Wang, Y . X. Wei, Y . Zhang, Y . Xu, Y . Li, Y . Zhao, Y . Sun,
Y . Wang, Y . Yu, Y . Zhang, Y . Shi, Y . Xiong, Y . He, Y . Piao, Y . Wan g, Y . Tan, Y . Ma, Y . Liu,
Y . Guo, Y . Ou, Y . Wang, Y . Gong, Y . Zou, Y . He, Y . Xiong, Y . Luo, Y . You, Y . Liu, Y . Zhou,
Y . X. Zhu, Y . Xu, Y . Huang, Y . Li, Y . Zheng, Y . Zhu, Y . Ma, Y . Tang , Y . Zha, Y . Yan, Z. Z.
Ren, Z. Ren, Z. Sha, Z. Fu, Z. Xu, Z. Xie, Z. Zhang, Z. Hao, Z. Ma, Z. Yan, Z. Wu, Z. Gu,
Z. Zhu, Z. Liu, Z. Li, Z. Xie, Z. Song, Z. Pan, Z. Huang, Z. Xu, Z. Zhang, and Z. Zhang.
Deepseek-r1: Incentivizing reasoning capability in llms v ia reinforcement learning, 2025. URL
https://arxiv.org/abs/2501.12948 . (pp. 2,5,6, and 7)
[11] T. Everitt, M. Hutter, R. Kumar, and V . Krakovna. Reward tampering problems and solutions
in reinforcement learning: A causal inﬂuence diagram persp ective. Synthese , 198(Suppl 27):
6435–6467, 2021. (pp. 4and7)
[12] J. Fan, Z. Gu, S. Zhang, Y . Zhang, Z. Chen, L. Cao, G. Li, S. Madden, X. Du, and N. Tang.
Combining small language models and large language models f or zero-shot nl2sql. Proc.
VLDB Endow. , 17(11):2750–2763, July 2024. ISSN 2150-8097. doi: 10.147 78/3681954.
3681960. URL https://doi.org/10.14778/3681954.3681960 . (p. 1)
[13] A. Floratou, F. Psallidas, F. Zhao, S. Deep, G. Hagleith er, W. Tan, J. Cahoon, R. Alotaibi,
J. Henkel, A. Singla, A. V . Grootel, B. Chow, K. Deng, K. Lin, M . Campos, K. V . Emani,
V . Pandit, V . Shnayder, W. Wang, and C. Curino. Nl2sql is a sol ved problem... not! In
Conference on Innovative Data Systems Research , 2024. (p. 2)
[14] Y . Gan, X. Chen, Q. Huang, M. Purver, J. R. Woodward, J. Xi e, and P. Huang. Towards
robustness of text-to-sql models against synonym substitu tion. In Proceedings of the 59th
Annual Meeting of the Association for Computational Lingui stics and the 11th International
12
Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 2505–2515,
2021. (p. 8)
[15] Y . Gan, X. Chen, and M. Purver. Exploring underexplored limitations of cross-domain text-to-
sql generalization. In Proceedings of the 2021 Conference on Empirical Methods in N atural
Language Processing , pages 8926–8931, 2021. (p. 8)
[16] D. Gao, H. Wang, Y . Li, X. Sun, Y . Qian, B. Ding, and J. Zhou . Text-to-sql empowered by
large language models: A benchmark evaluation. arXiv preprint arXiv:2308.15363 , 2023. (p.
5)
[17] L. Gao, J. Schulman, and J. Hilton. Scaling laws for rewa rd model overoptimization. In
International Conference on Machine Learning , pages 10835–10866. PMLR, 2023. (pp. 4,5,
and7)
[18] J. Gehring, K. Zheng, J. Copet, V . Mella, Q. Carbonneaux , T. Cohen, and G. Synnaeve.
Rlef: Grounding code llms in execution feedback with reinfo rcement learning. arXiv preprint
arXiv:2410.02089 , 2024. (p. 5)
[19] Google. Gemini 2.0 model updates: 2.0 ﬂash, ﬂash-lite, pro experimental, 2025. Accessed:
2025-04-01. (p. 6)
[20] A. Grattaﬁori, A. Dubey, A. Jauhri, A. Pandey, A. Kadian , A. Al-Dahle, A. Letman, A. Mathur,
A. Schelten, A. Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 ,
2024. (p. 9)
[21] B. Hui, J. Yang, Z. Cui, J. Yang, D. Liu, L. Zhang, T. Liu, J . Zhang, B. Yu, K. Lu, et al. Qwen2.
5-coder technical report. arXiv preprint arXiv:2409.12186 , 2024. (pp. 2and8)
[22] D. P. Kingma and J. Ba. Adam: A method for stochastic opti mization. arXiv preprint
arXiv:1412.6980 , 2014. (p. 3)
[23] H. Le, Y . Wang, A. D. Gotmare, S. Savarese, and S. C. H. Hoi . Coderl: Mastering code
generation through pretrained models and deep reinforceme nt learning. Advances in Neural
Information Processing Systems , 35:21314–21328, 2022. (p. 5)
[24] F. Lei, J. Chen, Y . Ye, R. Cao, D. Shin, S. Hongjin, Z. SUO, H. Gao, W. Hu, P. Yin, et al.
Spider 2.0: Evaluating language models on real-world enter prise text-to-sql workﬂows. In The
Thirteenth International Conference on Learning Represen tations . (p. 6)
[25] H. Li, J. Zhang, H. Liu, J. Fan, X. Zhang, J. Zhu, R. Wei, H. Pan, C. Li, and H. Chen. Codes:
Towards building open-source language models for text-to- sql.Proc. ACM Manag. Data , 2(3),
May 2024. doi: 10.1145/3654930. URL https://doi.org/10.1145/3654930 . (p. 2)
[26] J. Li, B. Hui, G. Qu, J. Yang, B. Li, B. Li, B. Wang, B. Qin, R . Geng, N. Huo, et al. Can llm
already serve as a database interface? a big bench for large- scale database grounded text-to-
sqls. Advances in Neural Information Processing Systems , 36, 2024. (pp. 2,6, and 7)
[27] X. Liu, S. Shen, B. Li, P. Ma, R. Jiang, Y . Luo, Y . Zhang, J. Fan, G. Li, and N. Tang.
A survey of NL2SQL with large language models: Where are we, a nd where are we
going? CoRR , abs/2408.05109, 2024. doi: 10.48550/ARXIV .2408.05109. URL
https://doi.org/10.48550/arXiv.2408.05109 . (p. 2)
[28] I. Loshchilov and F. Hutter. Decoupled weight decay reg ularization. arXiv preprint
arXiv:1711.05101 , 2017. (p. 8)
[29] N. Muennighoff, Z. Yang, W. Shi, X. L. Li, L. Fei-Fei, H. H ajishirzi, L. Zettlemoyer, P. Liang,
E. Cand` es, and T. Hashimoto. s1: Simple test-time scaling, 2025. (p. 6)
[30] S. Narvekar, B. Peng, M. Leonetti, J. Sinapov, M. E. Tayl or, and P. Stone. Curriculum learning
for reinforcement learning domains: A framework and survey .Journal of Machine Learning
Research , 21(181):1–50, 2020. (p. 5)
[31] A. Ng. Policy Invariance Under Reward Transformations : Theory and Application to Reward
Shaping. In Proceedings of the 16th International Conference on Machin e Learning , page 278,
1999. (p. 5)
[32] OpenAI. Reasoning guide, 2024. (p. 6)
[33] OpenAI. Gpt-4o mini: Advancing cost-efﬁcient intelli gence.
https://openai.com/index/gpt-4o-mini-advancing-cost -efficient-intelligence ,
July 2024. Accessed: 2025-04-18. (p. 9)
13
[34] OpenAI. Gpt-4o system card. Technical report, OpenAI, August 2024. URL
https://openai.com/index/gpt-4o-system-card . Accessed: 2025-04-18. (p. 9)
[35] OpenAI. Openai o3-mini system card. Technical report, OpenAI, January 2025. URL
https://openai.com/index/o3-mini-system-card/ . Accessed: 2025-04-18. (p. 9)
[36] S. Papicchio, P. Papotti, and L. Cagliero. Qatch: Autom atic evaluation of sql-centric tasks on
proprietary data. ACM Transactions on Intelligent Systems and Technology , 2025. (p. 2)
[37] M. Pourreza and D. Raﬁei. Din-sql: Decomposed in-conte xt learning of text-to-sql with self-
correction. Advances in Neural Information Processing Systems , 36:36339–36348, 2023. (p.
5)
[38] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, e t al. Improving language understanding
by generative pre-training. (p. 3)
[39] H. Robbins and S. Monro. A stochastic approximation met hod. The annals of mathematical
statistics , pages 400–407, 1951. (p. 3)
[40] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. K limov. Proximal policy optimization
algorithms. arXiv preprint arXiv:1707.06347 , 2017. (p. 4)
[41] Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y . Li, Y . Wu, et al.
Deepseekmath: Pushing the limits of mathematical reasonin g in open language models. arXiv
preprint arXiv:2402.03300 , 2024. (pp. 2,4, and 8)
[42] Z. Shi, A. Yang, B. Wu, L. Aitchison, E. Yilmaz, and A. Lip ani. Instruction tuning with loss
over instructions. Advances in Neural Information Processing Systems , 37:69176–69205, 2024.
(p.3)
[43] R. Sun, S. ¨O. Arik, A. Muzio, L. Miculicich, S. Gundabathula, P. Yin, H. Dai, H. Nakhost,
R. Sinha, Z. Wang, et al. Sql-palm: Improved large language m odel adaptation for text-to-sql
(extended). arXiv preprint arXiv:2306.00739 , 2023. (p. 5)
[44] R. S. Sutton, A. G. Barto, et al. Reinforcement learning: An introduction , volume 1. MIT press
Cambridge, 1998. (p. 5)
[45] S. Talaei, M. Pourreza, Y .-C. Chang, A. Mirhoseini, and A. Saberi. Chess: Contextual harness-
ing for efﬁcient sql synthesis. arXiv preprint arXiv:2405.16755 , 2024. (p. 5)
[46] K. Team, A. Du, B. Gao, B. Xing, C. Jiang, C. Chen, C. Li, C. Xiao, C. Du, C. Liao, et al.
Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599 , 2025.
(p.5)
[47] Q. Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL
https://qwenlm.github.io/blog/qwq-32b/ . (p. 9)
[48] TogheterAI. Prompting deepseek-r1, 2025. (p. 6)
[49] B. Wang, R. Shin, X. Liu, O. Polozov, and M. Richardson. R at-sql: Relation-aware schema
encoding and linking for text-to-sql parsers. arXiv preprint arXiv:1911.04942 , 2019. (p. 5)
[50] B. Wang, C. Ren, J. Yang, X. Liang, J. Bai, L. Chai, Z. Yan, Q.-W. Zhang, D. Yin, X. Sun,
et al. Mac-sql: A multi-agent collaborative framework for t ext-to-sql. arXiv preprint
arXiv:2312.11242 , 2023. (p. 5)
[51] Y . Wang, Y . Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khash abi, and H. Hajishirzi. Self-
instruct: Aligning language models with self-generated in structions. In A. Rogers, J. Boyd-
Graber, and N. Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association fo r
Computational Linguistics (Volume 1: Long Papers) , pages 13484–13508, Toronto, Canada,
July 2023. Association for Computational Linguistics. doi : 10.18653/v1/2023.acl-long.754.
URLhttps://aclanthology.org/2023.acl-long.754/ . (p. 3)
[52] J. Wei, M. Bosma, V . Y . Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V .
Le. Finetuned language models are zero-shot learners. In The Tenth International Conference
on Learning Representations, ICLR 2022, Virtual Event, Apr il 25-29, 2022 . OpenReview.net,
2022. URL https://openreview.net/forum?id=gEZrGCozdqR . (p. 2)
[53] L. Weng. Reward hacking in reinforcement learning. lilianweng.github.io , Nov 2024. (pp. 4
and7)
14
[54] C. Xiao, M. Dymetman, and C. Gardent. Sequence-based st ructured prediction for seman-
tic parsing. In Proceedings of the 54th Annual Meeting of the Association fo r Computa-
tional Linguistics, ACL 2016, August 7-12, 2016, Berlin, Ge rmany, Volume 1: Long Pa-
pers. The Association for Computer Linguistics, 2016. doi: 10.1 8653/V1/P16-1127. URL
https://doi.org/10.18653/v1/p16-1127 . (p. 2)
[55] Q. Yu, Z. Zhang, R. Zhu, Y . Yuan, X. Zuo, Y . Yue, T. Fan, G. L iu, L. Liu, X. Liu, H. Lin, Z. Lin,
B. Ma, G. Sheng, Y . Tong, C. Zhang, M. Zhang, W. Zhang, H. Zhu, J . Zhu, J. Chen, J. Chen,
C. Wang, H. Yu, W. Dai, Y . Song, X. Wei, H. Zhou, J. Liu, W.-Y . Ma , Y .-Q. Zhang, L. Yan,
M. Qiao, Y . Wu, and M. Wang. Dapo: An open-source llm reinforc ement learning system at
scale, 2025. (p. 5)
[56] T. Yu, R. Zhang, K. Yang, M. Yasunaga, D. Wang, Z. Li, J. Ma , I. Li, Q. Yao, S. Roman,
et al. Spider: A large-scale human-labeled dataset for comp lex and cross-domain semantic
parsing and text-to-sql task. In 2018 Conference on Empirical Methods in Natural Language
Processing, EMNLP 2018 , pages 3911–3921. Association for Computational Linguist ics, 2018.
(pp.2,6,7, and 8)
[57] X. Yu, Q. Wu, Y . Li, and Z. Yu. LIONs: An empirically optim ized approach to align lan-
guage models. In Y . Al-Onaizan, M. Bansal, and Y .-N. Chen, ed itors, Proceedings of the 2024
Conference on Empirical Methods in Natural Language Proces sing, pages 8732–8753, Miami,
Florida, USA, Nov. 2024. Association for Computational Lin guistics. doi: 10.18653/v1/2024.
emnlp-main.496. URL https://aclanthology.org/2024.emnlp-main.496/ . (p. 3)
[58] V . Zhong, C. Xiong, and R. Socher. Seq2sql: Generating s tructured queries from natural
language using reinforcement learning. arXiv preprint arXiv:1709.00103 , 2017. (p. 5)
15