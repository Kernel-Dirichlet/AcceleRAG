Towards Explainable AI: Multi-Modal Transformer for
Video-based Image Description Generation
Lakshita Agarwal1and Bindu Verma2*
1Department of Information Technology, Delhi Technological University, Delhi, 110042,
India.
2*Assistant Professor, Department of Information Technology, Delhi Technological
University, Delhi, 110042, India.
*Corresponding author(s). E-mail(s): bindu.cvision@gmail.com;
Contributing authors: lakshitaa3@gmail.com;
Abstract
Understanding and analyzing video actions are essential for producing insightful and contextualized
descriptions, especially for video-based applications like intelligent monitoring and autonomous sys-
tems. The proposed work introduces a novel framework for generating natural language descriptions
from video datasets by combining textual and visual modalities. The suggested architecture makes use
of ResNet50 to extract visual features from video frames that are taken from the Microsoft Research
Video Description Corpus (MSVD), and Berkeley DeepDrive eXplanation (BDD-X) datasets. The
extracted visual characteristics are converted into patch embeddings and then run through an encoder-
decoder model based on Generative Pre-trained Transformer-2 (GPT-2). In order to align textual and
visual representations and guarantee high-quality description production, the system uses multi-head
self-attention and cross-attention techniques. The model’s efficacy is demonstrated by performance
evaluation using BLEU (1-4), CIDEr, METEOR, and ROUGE-L. The suggested framework outper-
forms traditional methods with BLEU-4 scores of 0.755 (BDD-X) and 0.778 (MSVD), CIDEr scores
of 1.235 (BDD-X) and 1.315 (MSVD), METEOR scores of 0.312 (BDD-X) and 0.329 (MSVD), and
ROUGE-L scores of 0.782 (BDD-X) and 0.795 (MSVD). By producing human-like, contextually rele-
vant descriptions, strengthening interpretability, and improving real-world applications, this research
advances explainable AI.
Keywords: Video-based Description Generation, Transformer Models, Explainable AI, Autonomous
Systems, Natural Language Processing
1 Introduction
In computer vision and natural language process-
ing, creating informative textual descriptions of
events that occur in video sequences is a funda-
mental problem known as ”video description gen-
eration” [1]. In contrast to static image captioning,
video-based description generation involves anawareness of object interactions, temporal depen-
dencies, and contextual linkages in order to pro-
vide narratives that are both contextually rich
and coherent. Applications like intelligent surveil-
lance, multimedia retrieval, autonomous driving,
and human-computer interaction greatly benefit
from these capabilities. Capturing dynamic scene
1arXiv:2504.16788v1  [cs.CV]  23 Apr 2025
changes, identifying pertinent objects, and pro-
ducing descriptions that appropriately reflect the
changing context are the challenges [2] [3]. Con-
ventional methods for comprehending videos, such
as object detection and action recognition, mostly
concentrate on figuring out ”what” is happen-
ing in a scene. Explaining ”why” things happen
is more difficult, though, particularly in complex,
multi-agent systems where behaviour is influenced
by environmental suggestions [4]. Known as video
captioning or video-based description generation,
the process combines textual and visual modal-
ities to generate interpretable and informative
natural language descriptions [5] [6]. The ability
to produce context-aware and semantically rele-
vant video descriptions has greatly improved with
recent developments in deep learning, especially
transformer-based systems.
This work presents a novel transformer-based
architecture for generating video descriptions by
effectively combining text and image modalities.
Using a GPT-2 encoder-decoder with multi-head
attention and ResNet50 for visual feature extrac-
tion, the model is trained on diverse datasets,
including MSVD and BDD-X. Training is opti-
mized with gradient accumulation and mixed
precision to boost efficiency. Evaluated using
BLEU (1–4), CIDEr, METEOR, and ROUGE-L,
the model produces fluent, contextually relevant
descriptions aligned with human annotations, con-
tributing to advancements in video explainability
and AI-driven understanding.
The major contributions of the proposed work
are summarized below:
•To provide natural language descriptions of
video sequences, the following work proposes a
transformer-based architecture that integrates
a GPT-2-based language model with the visual
features retrieved by ResNet50.
•The method combines video datasets from var-
ious sources (MSVD, and BDD-X), allowing
the model to generalise across domains. High-
quality training samples are guaranteed via
sophisticated preprocessing.
•Gradient accumulation and mixed precision
training are used to optimise the model, increas-
ing computing efficiency without sacrificing out-
put quality.•Fluency, contextual relevance, and coherence
are ensured by thoroughly evaluating the gen-
erated descriptions using BLEU (1–4), CIDEr,
METEOR, and ROUGE-L standards.
•Although this method is extremely beneficial for
intelligent transportation and autonomous driv-
ing, it can also be used in other fields, including
robotics, assistive technology, and surveillance.
The remaining sections of the paper are orga-
nized as follows: Section 2 examines relevant stud-
ies in multimodal learning and video description
generation. The suggested methodology, includ-
ing feature extraction, model architecture, and
dataset preprocessing, is described in Section 3.
Experimental results and assessment results are
presented in Section 4. Lastly, conclusions and
possible future study directions are covered in
Section 5.
2 Related Work
In computer vision and natural language process-
ing, video-based description creation is an essen-
tial task that allows systems to produce textual
summaries of visual content. The creation of video
descriptions must consider temporal relationships,
object interactions, and dynamic scene changes
across frames, in contrast to static image caption-
ing. Early video description generation approaches
included recurrent neural networks (RNNs) or
long short-term memory (LSTM) networks for
text synthesis and convolutional neural networks
(CNNs) for feature extraction.
A Sequence-to-Sequence (Seq2Seq) model
trained on MSVD [7] was presented by Venu-
gopalan et al. [8] to produce video descriptions.
However, because of the limits of LSTMs, these
models have trouble handling contextual incon-
sistencies and long-range dependencies. Later, to
make generated descriptions more relevant, atten-
tion techniques were added [9]. Transformer-based
models have led to a considerable improvement in
video captioning systems. End-to-end dense Video
Captioning was proposed by Zhou et al. [10],
who used spatiotemporal attention mechanisms
to grasp multiple frames. Compared to con-
ventional RNN-based models, MART (Memory-
Augmented Recurrent Transformer), which was
introduced by Lei et al. [11], performs more
2
effectively at capturing long-term video depen-
dencies. More recently, multimodal pretraining for
video-language understanding has been investi-
gated by VideoBERT [12] and ClipBERT [13].
These methods achieve state-of-the-art outcomes
on datasets such as MSVD [7] and MSR-VTT [14]
by aligning textual and visual representations.
Datasets such as Berkeley DeepDrive eXplana-
tion (BDD-X) [15] have been useful for produc-
ing explainable AI-driven annotations for vehicle-
based video description generation. Studies like
Shoman et al. [4] integrated explainability pro-
cesses into vision-language models and concen-
trated on justification-based video descriptions for
autonomous driving. An attention-guided trans-
former was developed by Cui et al. [16] to improve
contextual reasoning in captioning models based
on BDD-X.
Recent advancements in multimodal learning
combine text, audio, and visual data. Hori et
al.[17] introduced an audio-visual attention model
using speech and scene context, while Yu et al.[18]
enhanced captioning on datasets like Flickr8k and
MSVD using spatiotemporal object graphs. These
methods improve contextual awareness, particu-
larly in dynamic scenarios like vehicle interactions.
Despite progress, generating accurate, human-like
video descriptions remains challenging. The pro-
posed approach addresses this by combining a
GPT-2 encoder-decoder with ResNet50-based fea-
ture extraction, outperforming prior methods on
BLEU (1–4), CIDEr, METEOR, and ROUGE-L.
It refines multimodal embeddings for context-
aware captions and boosts efficiency with gradient
accumulation and mixed precision training. By
bridging explainable vehicular (BDD-X) and gen-
eral (MSVD) video captioning, the model delivers
interpretable, context-rich descriptions suitable
for real-world use.
3 Overview of the Proposed
Work
The proposed study introduces a transformer-
based, context-aware reasoning framework for
generating logical and relevant video descriptions.
Leveraging the BDD-X and MSVD datasets, the
system learns from both action-justified driv-
ing videos and diverse scene representations. It
employs an optimized GPT-2 encoder-decodermodel with multi-head attention and ResNet50-
based visual feature extraction to enhance con-
textual understanding. Training is guided by
action-justification pairs from BDD-X and cap-
tioned sequences from MSVD, promoting coher-
ent, human-like output. Gradient accumulation
and mixed precision training boost computational
efficiency without compromising accuracy. Eval-
uations using BLEU (1–4), CIDEr, METEOR,
and ROUGE-L show superior performance over
traditional methods. The basic architecture of
the proposed framework is shown in Figure 1,
which further demonstrates the pipeline for data
pre-processing, model training, and description
generation.
3.1 Data Pre-processing
The input data includes video frames from the
BDD-X and MSVD datasets. Video frames are
extracted, pertinent images are filtered, and then
the frames are transformed into structured input
for the model as part of the pre-processing
pipeline. ResNet50, which converts spatial infor-
mation from images into high-dimensional feature
vectors, is used for feature extraction. After that,
a linear projection and position encoding method
are used to patch and embed these feature vectors.
The GPT-2 encoder-decoder module then receives
the processed embeddings and aligns the word/-
token embeddings with the appropriate visual
features. To further improve model resilience, tok-
enization, data augmentation, and train-test splits
are used.
3.2 Model Architecture
The proposed method combines a GPT-2 encoder-
decoder model to generate context-aware descrip-
tions of vehicle actions with ResNet50 to extract
image features. The model is fine-tuned on the
BDD-X and MSVD datasets to become optimal
in dynamic scenarios for generating meaningful
descriptions. The training process employs gradi-
ent accumulation and mixed precision training to
achieve optimal computing efficiency.
ResNet50 acts as a feature extractor by passing
each input frame x∈R3×H×Wthrough a series
of convolutional, batch normalization, ReLU, and
residual layers. The output feature vector fis
3
Fig. 1 : Framework for the Proposed Model (ResNet50-GPT2): The system incorporates ResNet50 for
image feature extraction and a GPT-2 encoder-decoder model to generate context-aware video-based
image descriptions.
computed as:
f= ResNet50( x) = AvgPool( Fres(x)) (1)
where, Fresdenotes the output of the final convo-
lutional block, and AvgPool is the global average
pooling operation applied to extract the final
feature representation f∈R2048.
Using position embeddings P∈RN×dand
a learnable linear projection Wp∈R2048×d, the
visual characteristics retrieved by ResNet50 are
transformed into embedded visual tokens:
Ev=fWp+P (2)
The GPT-2 model uses these embedded
patches as input tokens to produce descrip-
tions that are logical and sensitive to context.
The model utilizes self-attention mechanisms to
enhance contextual understanding, formulated as:
Self-Attention( Q, K, V ) = SoftmaxQKT
√dk
V
(3)
where dkis the key vector dimension, and Q, K, V
represent the query, key, and value matrices.
To maintain the semantic relevance of the
generated descriptions to video-based activities,
the multi-head cross-attention method aligns
image embeddings with textual descriptions.The decoder uses attention-weighted contextual
embeddings to improve the textual output:
Decoder-Attention( Q, K, V ) = SoftmaxQKT
√dk
V
(4)
The final word prediction probability is com-
puted as:
yt= Softmax( Woht+bo) (5)
where Woandboare the output weight matrix and
bias, respectively, and htrepresents the hidden
state at time step t.
To enhance generalization, the model employs
a combined loss function incorporating cross-
entropy loss and L2 regularization:
L=−TX
t=1logP(yt|y<t) +λ∥θ∥2(6)
where λis the regularization parameter and P(yt|
y<t) denotes the probability of predicting ytgiven
prior words.
In BLEU-4, CIDEr, METEOR, and ROUGE-
L metrics, the proposed framework outperforms
traditional methods, ensuring that the generated
descriptions are sensible, logical, and context-
aware. These metrics are utilized to analyze the
4
contextual validity and linguistic quality of the
descriptions generated by the GPT-2 model for
unseen videos upon training. The method achieves
performance improvement over traditional meth-
ods by contrasting the generated descriptions with
ground-truth annotations. Ultimately, the frame-
work generates natural language narratives that
effectively describe dynamic visual scenes, pro-
moting explainability in a range of applications.
The following summarizes Algorithm 1 for the
proposed model and its evaluation:
Algorithm 1 Context-Aware Description Gener-
ation
1:Input: Video frames (BDD-X, MSVD),
image-caption pairs (Flickr8k)
2:Output: Generated descriptions, Evaluation
Scores
3:1. Preprocessing and Embedding
4:Extract and embed image frames using patch
encoding
5:Tokenize and format textual descriptions
6:2. Dataset Split
7:Create balanced train-test sets with captions
and action-justification pairs
8:3. Feature Extraction
9:Visual features ←ResNet50( I)
10:Project features and tokenize text using GPT-
2
11:4. Model Training
12:Initialize GPT-2 with multi-head attention
and cross-modal fusion
13:Train with AdamW, gradient accumulation,
and mixed precision
14:5. Inference and Evaluation
15:foreach test sample do
16: Generate and decode descriptions
17:end for
18:Compute BLEU-4, CIDEr, METEOR,
ROUGE-L
19:6. Output
20:Present evaluation scores and visualize results
4 Experimental Analysis
The proposed model was implemented in PyTorch
and evaluated on Google Colab Pro+ using
52 GB RAM, an NVIDIA A100 GPU (40 GBVRAM), and a virtual Intel Xeon CPU. It gen-
erates context-aware descriptions using a GPT-2
encoder-decoder (approximately 66.7M parame-
ters) and ResNet50 for visual feature extraction.
Input frames were resized to (batch size, 224,
224, 3) . Training was conducted over 75 epochs
with the AdamW optimizer (learning rate 1 ×
10−5, weight decay 0.01), using a batch size of 32
and gradient accumulation. Mixed precision train-
ing and gradient clipping were applied to improve
computational efficiency and training stability.
The model was trained using the MSVD dataset1,
which contains short descriptive captions for
diverse video clips, and the BDD-X dataset2,
which includes over 26K annotated actions across
8.4M frames and 6,970 videos. During prepro-
cessing, linguistic descriptions were tokenized and
structured into action-justification pairs, while
image frames were embedded via ResNet50 and
projected linearly into patch embeddings. These
embeddings were then fed into the multi-head self-
attention module of the GPT-2 encoder-decoder
to align image and text contexts effectively. Per-
formance was evaluated on a 20% test split
using BLEU-4, CIDEr, METEOR, and ROUGE-
L. The model consistently outperformed conven-
tional captioning techniques, demonstrating its
ability to generate logical, contextually accu-
rate descriptions across diverse visual inputs.
The architecture’s use of self- and cross-attention
mechanisms ensured semantic accuracy and nar-
rative coherence, reinforcing its applicability in
real-world visual captioning tasks. The following
Table 1 depicts the overall architectural structure
of the proposed framework.
4.1 Ablation Study:
Utilising the BDD-X and the MSVD datasets,
an ablation research was carried out to evalu-
ate the contribution of various components in
the suggested model. The research methodically
eliminated or altered important architectural com-
ponents, such as multi-head attention, gradient
accumulation and mixed precision training opti-
misations, and visual feature extraction (ResNet-
50), in order to assess each component’s effect
1https://www.kaggle.com/datasets/vtrnanh/
msvd-dataset-corpus
2https://github.com/JinkyuKimUCB/BDD-X-dataset
5
Table 1 : Experimental Setup and Performance Metrics
Component Details
Framework Used PyTorch
Image Feature Extractor ResNet-50
Model GPT-2 Transformer-based Encoder-Decoder
Input Size (224, 224, 3)
Batch size 32
Number of Epochs 75
Learning Rate 1×10−5
Optimizer AdamW
Training Strategy Gradient Accumulation, Mixed-Precision Training
Datasets Used BDD-X, MSVD, Filtered Flickr8k
Evaluation Metrics BLEU 1-4, CIDEr, METEOR, ROUGE-L
Table 2 : Ablation Study: Comparing the Impact of Model Components on Different Datasets
Dataset Model Variant BLEU-1 BLEU-2 BLEU-3 BLEU-4 CIDEr METEOR ROUGE-L
BDD-X Baseline GPT-2 Model 0.693 0.668 0.638 0.601 0.921 0.234 0.640
GPT-2 + Single-Head Attention 0.731 0.705 0.673 0.634 1.032 0.257 0.672
GPT-2 + Without Gradient
Accumulation0.785 0.758 0.723 0.681 1.142 0.278 0.710
ResNet50-GPT2 (Proposed) 0.860 0.835 0.800 0.755 1.235 0.312 0.782
MSVD Baseline GPT-2 Model 0.710 0.685 0.654 0.612 0.980 0.242 0.651
GPT-2 + Single-Head Attention 0.742 0.715 0.683 0.641 1.105 0.265 0.685
GPT-2 + Without Gradient
Accumulation0.798 0.772 0.740 0.698 1.231 0.293 0.724
ResNet50-GPT2 (Proposed) 0.880 0.855 0.823 0.778 1.315 0.329 0.795
on model performance. Model modifications were
compared using the BLEU-1 to BLEU-4, CIDEr,
METEOR, and ROUGE-L metrics, which pro-
vided a thorough assessment of the effects of
each component on the quality of description
production. As can be seen from Table 2, the
results demonstrate the importance of each ele-
ment in producing textual descriptions for video
frames that are both logical and contextually
rich. The model’s performance on the MSVD
and BDD-X datasets highlights the importance
of each architectural component. The most sig-
nificant impact came from removing ResNet-50,
leading to major performance drops—BLEU-4
and CIDEr declined by 15.4% and 25.4% on BDD-
X, and 21.3% and 25.5% on MSVD—emphasizing
the need for strong visual features. Replacing
multi-head attention with a single-head variant
caused moderate losses, notably a 16.0% drop
in CIDEr on MSVD, showing its role in captur-
ing fine-grained text-visual interactions. Omitting
gradient accumulation reduced training efficiency
and led to average BLEU-4 and CIDEr decreases
of 9.8% and 7.5%, confirming its value for sta-
ble optimization. Figure 2 illustrates these results.
The proposed framework obtained the best scores
on all evaluation measures, outperforming theablated versions on all the datasets consistently.
The improvements are especially clear in MSVD
and BDD-X, where temporal dependency mod-
eling and interpreting intricate visual scenes are
critical. This better performance is a result of
the synergy among multiple important elements:
ResNet50 for feature extraction of high-level spa-
tial features from images, GPT-2 as a strong
language model with the ability to produce fluent
and coherent text sequences, multi-head atten-
tion for modeling fine-grained alignments between
visual and linguistic modalities, and gradient
accumulation with mixed-precision training for
efficient and stable optimization. The use of GPT-
2, specifically, adds to the model’s capacity to
generate contextually accurate and linguistically
advanced descriptions. Collectively, these features
make the proposed framework capable of gen-
erating high-quality, context-aware descriptions,
with each module contributing significantly to the
overall performance improvement.
4.2 Results and Analysis:
To assess the efficacy of the suggested architec-
ture, we carried out a comprehensive performance
analysis on two benchmark datasets: MSVD and
6
Fig. 2 : Demonstration of Ablation Study for the Proposed Work
Table 3 : Results and Analysis of the Proposed Framework
Dataset BLEU-1 BLEU-2 BLEU-3 BLEU-4 CIDEr METEOR ROUGE-L
BDD-X 0.860 0.835 0.800 0.755 1.235 0.312 0.782
MSVD 0.880 0.855 0.823 0.778 1.315 0.329 0.795
BDD-X. The suggested framework was tested
against many model ablations, such as excluding
gradient accumulation, using single-head atten-
tion, and removing ResNet-50. The evaluation’s
findings are compiled in Table 3. The outcomes
shown in the table demonstrate how well our
suggested methodology performs across datasets,
consistently achieving top scores in all evaluation
metrics. While high BLEU scores reflect fluency
and grammatical accuracy, the superior perfor-
mance in CIDEr and METEOR highlights strong
semantic alignment and high-quality descriptions.
With a BLEU-4 score of 0.778 and a CIDEr
score of 1.315, the MSVD dataset showcases the
model’s strength in producing diverse and pre-
cise captions for short video content. Similarly,
the BDD-X dataset results—0.755 BLEU-4 and
1.235 CIDEr—illustrate the model’s robustness
in capturing intricate driving scenes and gener-
ating contextually appropriate, descriptive narra-
tives. Consistently high ROUGE-L scores (0.795
for MSVD and 0.782 for BDD-X) further val-
idate the framework’s ability to generate out-
puts that closely align with human references.
Notably, elevated CIDEr values emphasize the
model’s effectiveness in capturing contextual and
domain-specific nuances, while higher METEOR
scores reflect improved alignment and paraphras-
ing capabilities. Figure 3 demonstrates the graph-
ical representation of the results obtained for
the proposed framework. In general, the findingsdemonstrate that the suggested approach greatly
improves the ability of video-based descriptions,
resulting in syntactically fluid, semantically rich,
and contextually relevant descriptions.
4.3 Comparison With
State-of-the-Art Methods:
We compare the performance of the sug-
gested model with current state-of-the-art
(SOTA) captioning methods on two benchmark
datasets—BDD-X and MSVD—in order to assess
its efficacy. A thorough evaluation employing
several evaluation metrics, such as BLEU-1 to
BLEU-4, CIDEr, METEOR, and ROUGE-L, is
given by the results in Table 4. On both datasets,
the proposed framework significantly outperforms
earlier approaches, proving its capacity to pro-
duce more semantically precise and contextually
rich descriptions for behaviours involving vehicles.
Specifically, the suggested model leads in terms
of BLEU-4 and CIDEr scores, demonstrating
enhanced description coherence and relevance.
The table’s comparative analysis demonstrates
how the BDD-X and MSVD datasets have
advanced regarding vehicle action comprehension
and general video-based description generation.
Previous models like OSCAR [21], ViT-GPT2 [20],
X-LAN [19], and AoANet [22] showed consistent
improvements in descriptive quality on the BDD-
X dataset, with AoANet receiving the highest
7
Fig. 3 : Graphical Representation of the Results Obtained
Table 4 : Comparison of State-of-the-Art Models for Vehicle Action Understanding
Dataset Model BLEU-1 BLEU-2 BLEU-3 BLEU-4 CIDEr METEOR ROUGE-L
BDD-XGPT-2 (Baseline) 0.312 0.285 0.210 0.152 0.653 0.185 0.356
X-LAN [19] 0.451 0.412 0.356 0.293 0.998 0.265 0.541
ViT-GPT2 [20] 0.482 0.445 0.398 0.324 1.086 0.258 0.523
OSCAR [21] 0.510 0.472 0.423 0.358 1.124 0.276 0.548
AoANet [22] 0.532 0.495 0.442 0.372 1.189 0.289 0.573
ResNet50-GPT2 (Proposed) 0.860 0.835 0.800 0.755 1.235 0.312 0.782
MSVDGPT-2 (Baseline) 0.334 0.305 0.243 0.178 0.705 0.192 0.372
Show, Attend and Tell [23] 0.480 0.443 0.395 0.304 0.943 0.252 0.520
M2Transformer [24] 0.520 0.486 0.438 0.391 1.270 0.292 0.586
Dense Video Captioning [25] 0.498 0.463 0.412 0.357 1.135 0.268 0.552
GRU-EVE [26] 0.535 0.502 0.456 0.370 1.246 0.285 0.594
ResNet50-GPT2 (Proposed) 0.880 0.855 0.823 0.778 1.315 0.329 0.795
scores. However, with a BLEU-4 score of 0.755
and a CIDEr score of 1.235, the proposed model
outperforms all of these approaches, significantly
advancing over prior research. Similarly, on the
MSVD dataset, transformer-based models such
as M2Transformer [24] and GRU-EVE [26] per-
formed well, with GRU-EVE achieving a BLEU-4
score of 0.370 and CIDEr of 1.246. The proposed
model demonstrates clear improvements, espe-
cially in BLEU-4 (0.778) and CIDEr (1.315),
showcasing its ability to generate contextu-
ally rich and semantically aligned descriptions.
Figure 4 depicts a graphical representation of
SOTA methods. Overall, the suggested approach
continuously outperforms comparison on all
datasets, setting new benchmarks and proving
its ability to capture fine-grained contextual
information for interpreting video-based actions.
5 Conclusion and Future
Direction
The proposed approach successfully combines
ResNet50 for feature extraction with an improvedGPT-2 model, allowing for the production of con-
textually aware, high-quality descriptions of video
footage from the BDD-X and MSVD datasets.
Through the use of transformer-based language
modelling, the method guarantees descriptions
that are consistent with ground-truth annota-
tions and logical. In comparison to traditional
approaches, the combination of ResNet50 for
visual feature extraction and GPT-2 for text
generation, as well as dataset augmentation, gra-
dient accumulation, and mixed precision train-
ing, improves training efficiency and perfor-
mance. Although transformer-based architectures
are effective when used for large datasets, the
study emphasizes that domain-specific fine-tuning
is necessary to produce accurate and compre-
hensible descriptions. Future steps will involve
implementing cross-modal attention mechanisms,
sophisticated vision-based encoders, and opti-
mized preprocessing methods to further improve
model adaptability. The generalization and com-
patibility across other domains would be enhanced
by expanding datasets to encompass a wider
8
Fig. 4 : Graphical Representation of the SOTA Methods
range of real-world circumstances. By enhanc-
ing interpretability, transparency, and decision-
making clarity, this research advances explainable
AI and increases the adaptability and dependabil-
ity of automated video comprehension for a range
of applications.
Acknowledgment
The authors sincerely thank the Object-Oriented
Programming Laboratory, Department of IT,
DTU, Delhi, India, for providing the necessary
resources to complete the research.
Statements and Declarations
Data Availability Statement: The paper con-
tains links to all datasets.
Competing Interests: The authors declare they
have no competing financial interests.
Conflict of Interest: The authors declare no
conflict of interest.
References
[1] N. Aafaq, A. Mian, W. Liu, S.Z. Gilani,
M. Shah, Video description: A survey of
methods, datasets, and evaluation metrics.
ACM Computing Surveys (CSUR) 52(6),
1–37 (2019)
[2] T. Li, H. Wang, X. Li, W. Liao, T. He,
P. Peng, Generative planning with 3d-
vision language pre-training for end-to-
end autonomous driving. arXiv preprint
arXiv:2501.08861 (2025)[3] L. Agarwal, B. Verma, From methods to
datasets: A survey on image-caption gener-
ators. Multimedia Tools and Applications
83(9), 28077–28123 (2024)
[4] M. Shoman, D. Wang, A. Aboah, M. Abdel-
Aty, Enhancing traffic safety with parallel
dense video captioning for end-to-end event
analysis , in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern
Recognition (2024), pp. 7125–7133
[5] Y. Li, C. Wu, L. Li, Y. Liu, J. Zhu, Cap-
tion generation from road images for traffic
scene modeling. IEEE Transactions on intel-
ligent transportation systems 23(7), 7805–
7816 (2021)
[6] C. Gou, Y. Zhou, D. Li, Driver attention
prediction based on convolution and trans-
formers. The Journal of Supercomputing
78(6), 8268–8284 (2022)
[7] D. Chen, W.B. Dolan, Collecting highly paral-
lel data for paraphrase evaluation , inProceed-
ings of the 49th annual meeting of the asso-
ciation for computational linguistics: human
language technologies (2011), pp. 190–200
[8] S. Venugopalan, M. Rohrbach, J. Donahue,
R. Mooney, T. Darrell, K. Saenko, Sequence
to sequence-video to text , inProceedings of the
IEEE international conference on computer
vision (2015), pp. 4534–4542
[9] L. Yao, A. Torabi, K. Cho, N. Ballas, C. Pal,
H. Larochelle, A. Courville, Describing videos
by exploiting temporal structure , inProceed-
ings of the IEEE international conference on
computer vision (2015), pp. 4507–4515
[10] L. Zhou, Y. Zhou, J.J. Corso, R. Socher,
C. Xiong, End-to-end dense video captioning
with masked transformer , in Proceedings of
9
the IEEE conference on computer vision and
pattern recognition (2018), pp. 8739–8748
[11] J. Lei, L. Wang, Y. Shen, D. Yu, T.L.
Berg, M. Bansal, Mart: Memory-augmented
recurrent transformer for coherent video
paragraph captioning. arXiv preprint
arXiv:2005.05402 (2020)
[12] C. Sun, A. Myers, C. Vondrick, K. Mur-
phy, C. Schmid, Videobert: A joint model for
video and language representation learning , in
Proceedings of the IEEE/CVF international
conference on computer vision (2019), pp.
7464–7473
[13] J. Lei, L. Li, L. Zhou, Z. Gan, T.L. Berg,
M. Bansal, J. Liu, Less is more: Clipbert
for video-and-language learning via sparse
sampling , inProceedings of the IEEE/CVF
conference on computer vision and pattern
recognition (2021), pp. 7331–7341
[14] J. Xu, T. Mei, T. Yao, Y. Rui, Msr-vtt:
A large video description dataset for bridg-
ing video and language , in Proceedings of
the IEEE conference on computer vision and
pattern recognition (2016), pp. 5288–5296
[15] J. Kim, A. Rohrbach, T. Darrell, J. Canny,
Z. Akata, Textual explanations for self-
driving vehicles , inProceedings of the Euro-
pean conference on computer vision (ECCV)
(2018), pp. 563–578
[16] C. Cui, Y. Ma, X. Cao, W. Ye, Y. Zhou,
K. Liang, J. Chen, J. Lu, Z. Yang, K.D. Liao,
et al., A survey on multimodal large language
models for autonomous driving , inProceed-
ings of the IEEE/CVF Winter Conference on
Applications of Computer Vision (2024), pp.
958–979
[17] C. Hori, T. Hori, T.Y. Lee, Z. Zhang,
B. Harsham, J.R. Hershey, T.K. Marks,
K. Sumi, Attention-based multimodal fusion
for video description , in Proceedings of the
IEEE international conference on computer
vision (2017), pp. 4193–4202
[18] J. Yu, J. Li, Z. Yu, Q. Huang, Multimodal
transformer with multi-view visual represen-
tation for image captioning. IEEE trans-
actions on circuits and systems for video
technology 30(12), 4467–4480 (2019)
[19] Y. Pan, T. Yao, Y. Li, T. Mei, X-linearattention networks for image captioning , in
Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition
(2020), pp. 10971–10980
[20] I. Vasireddy, G. HimaBindu, B. Ratnamala,
Transformative fusion: Vision transformers
and gpt-2 unleashing new frontiers in image
captioning within image processing. Inter-
national Journal of Innovative Research in
Engineering & Management 10(6), 55–59
(2023)
[21] X. Li, X. Yin, C. Li, P. Zhang, X. Hu,
L. Zhang, L. Wang, H. Hu, L. Dong, F. Wei,
et al., Oscar: Object-semantics aligned pre-
training for vision-language tasks , in Com-
puter Vision–ECCV 2020: 16th European
Conference, Glasgow, UK, August 23–28,
2020, Proceedings, Part XXX 16 (Springer,
2020), pp. 121–137
[22] L. Huang, W. Wang, J. Chen, X.Y. Wei,
Attention on attention for image captioning ,
inProceedings of the IEEE/CVF interna-
tional conference on computer vision (2019),
pp. 4634–4643
[23] O. Vinyals, A. Toshev, S. Bengio, D. Erhan,
Show and tell: A neural image caption gener-
ator, inProceedings of the IEEE conference
on computer vision and pattern recognition
(2015), pp. 3156–3164
[24] M. Cornia, M. Stefanini, L. Baraldi,
R. Cucchiara, Meshed-memory transformer
for image captioning , in Proceedings of
the IEEE/CVF conference on computer
vision and pattern recognition (2020), pp.
10578–10587
[25] R. Krishna, K. Hata, F. Ren, L. Fei-Fei,
J. Carlos Niebles, Dense-captioning events in
videos , inProceedings of the IEEE interna-
tional conference on computer vision (2017),
pp. 706–715
[26] N. Aafaq, N. Akhtar, W. Liu, S.Z. Gilani,
A. Mian, Spatio-temporal dynamics and
semantic attribute enriched visual encoding
for video captioning , in Proceedings of the
IEEE/CVF conference on computer vision
and pattern recognition (2019), pp. 12487–
12496
10