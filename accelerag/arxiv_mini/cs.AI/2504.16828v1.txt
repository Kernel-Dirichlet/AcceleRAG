Process Reward Models That Think
Muhammad Khalifa♡, Rishabh Agarwal♠, Lajanugen Logeswaran♢,
Jaekyeom Kim♢, Hao Peng♣, Moontae Lee♢, Honglak Lee♡♢, Lu Wang♡
♡University of Michigan♠Mila
♢LG AI Research♣University of Illinois Urbana-Champaign
khalifam@umich.edu
Abstract
Step-by-step verifiers—also known as process reward models (PRMs)—are a key
ingredient for test-time scaling. PRMs require step-level supervision, making them
expensive to train. This work aims to build data-efficient PRMs as verbalized step-
wise reward models that verify every step in the solution by generating a verification
chain-of-thought (CoT). We propose THINK PRM , a long CoT verifier fine-tuned
on orders of magnitude fewer process labels than those required by discriminative
PRMs. Our approach capitalizes on the inherent reasoning abilities of long CoT
models, and outperforms LLM-as-a-Judge and discriminative verifiers—using only
1% of the process labels in PRM800K—across several challenging benchmarks.
Specifically, THINK PRM beats the baselines on ProcessBench, MATH-500, and
AIME ’24 under best-of-N selection and reward-guided search. In an out-of-domain
evaluation on a subset of GPQA-Diamond and LiveCodeBench, our PRM surpasses
discriminative verifiers trained on the full PRM800K by 8% and 4.5%, respectively.
Lastly, under the same token budget, THINK PRM scales up verification compute
more effectively compared to LLM-as-a-Judge, outperforming it by 7.2% on a
subset of ProcessBench. Our work highlights the value of generative, long CoT
PRMs that can scale test-time compute for verification while requiring minimal
supervision for training.1
103104105
Training samples7075808590F1-score8K process labels
Over 700K process labelsTraining data efficiency: ProcessBench
2021222324
Number of beams5055606570Accuracy (%)
Reward-guided search: MATH-500ThinkPRM DiscPRM LLM-as-a-Judge
Figure 1: Left: THINK PRM -14B, trained on 8K process labels or 1K synthetic examples, outperforms
discriminative PRMs trained on about 100x more data on ProcessBench (Zheng et al., 2024). Right: THINK PRM -
1.5B, trained using the same 8K labels, outperforms LLM-as-a-judge and discriminative verifiers in reward-
guided search on MATH-500. The LLM-as-a-judge in both figures uses the same base model as T HINK PRM.
1Our code, data, and models will be released at https://github.com/mukhal/thinkprm .
Preprint.arXiv:2504.16828v1  [cs.LG]  23 Apr 2025
1 Introduction
Reasoning with large language models (LLMs) can substantially benefit from utilizing more test
compute (Jaech et al., 2024; Guo et al., 2025; Aky ¨urek et al., 2024). This typically depends on a
high-quality process reward model (PRM)—also known as process verifier—to select promising
paths for search or ranking (Cobbe et al., 2021; Li et al., 2023; Wu et al., 2024; Brown et al., 2024).
Given a problem-solution pair, the PRM provides a score that indicates whether the solution or part
of it is correct. To date, PRMs have mostly assumed the form of discriminative classifiers, trained to
discern correct from incorrect reasoning (Uesato et al., 2022; Zhang et al., 2025). However, training
discriminative PRMs requires access to process labels, i.e., step-level annotations, which either
require extensive human annotation (Lightman et al., 2023; Zheng et al., 2024), gold step-by-step
solutions (Khalifa et al., 2023), or compute-intensive rollouts (Luo et al., 2024; Wang et al., 2023b;
Chen et al., 2024a).
LLM-as-a-judge can enable generative verification by treating verification as text generation without
any training (Wang et al., 2023a; Liu et al., 2023b; Zheng et al., 2023). Despite the advantages of
LLM-as-a-judge such as data-efficiency and interpretability, it has been known to perform quite poorly
compared to specialized reward models in the context of complex reasoning (Lambert et al., 2024;
Zhang et al., 2024b; Chen et al., 2024c), as uncustomized LLMs frequently fail to recognize incorrect
reasoning (Huang et al., 2023; Zhang et al., 2024a; Ye et al., 2024). This raises the question: How can
we maintain both the data-efficiency and interpretability advantages and the superior performance of
discriminative PRMs?
8K 16K 24K 32K
Thinking up to (#tokens)7476788082848688F1-score (%)
Scaling verifier compute: ProcessBench
ThinkPRM
LLM-as-a-judgeDiscPRM
Figure 2: THINK PRM supports scaling veri-
fication compute by thinking longer.We approach this question with the following insight:
LLM-as-a-judge verification can greatly benefit from scal-
ing up the verifier’s inference compute—specifically, by
enabling it to think through a verification chain of thought
(CoT). To this end, we repurpose open-weight large rea-
soning models (LRMs; Team 2024; Guo et al. 2025; Li
et al. 2025) as the foundation for generative PRMs through
lightweight tuning. Specifically, we train LRMs using
synthetic data (Kim et al., 2023; Wang et al., 2024), utiliz-
ing as few as 8K process labels, and obtain THINK PRM
—a PRM that not only surpasses LLM-as-a-judge under
the same base model, but also outperforms discrimina-
tive PRMs trained on two orders of magnitude more data
across a variety of test-time scaling scenarios.
To obtain THINK PRM , we train four different reason-
ing models, namely R1-Distill-Qwen {1.5B,7B,14B }(Guo
et al., 2025), and QwQ-32B-Preview (Team, 2024) and
our training is extremely efficient, e.g., training our largest
PRM takes only 4 hours on a single A100 80GB. We extensively evaluate THINK PRM as a standalone
verifier on ProcessBench (Zheng et al., 2024), under different Best-of-N selection and verifier-guided
search scenarios—using both in- and out-of-domain tasks.
THINK PRM -14B, based on R1-Distill-Qwen-14B, outperforms a discriminative PRM based on the
same base model in terms of accuracy while using far fewer supervision signals as in Fig. 1 left. In
addition, THINK PRM -1.5B, based on R1-Distill-Qwen-1.5B, demonstrates strong performance on
MATH-500 (Hendrycks et al., 2021) under guided beam search, shown in Fig. 1 right. Lastly, as
shown in Fig. 2, THINK PRM can effectively utilize more verification compute than LLM-as-a-judge,
by forcing it to think for more tokens. All these results are obtained while training on 1K synthetic
examples, utilizing only 8K process labels.
Our work highlights the promise of long CoT verification that verifies reasoning with reasoning, and
that can effectively scale up both generator and verifier compute. To summarize, this work makes the
following major contributions:
•We introduce a simple recipe to obtain THINK PRM —a generative PRM that verifies solutions
via long CoT reasoning. Concretely, THINK PRM can be efficiently trained from off-the-shelf
reasoning model via finetuning on synthetic data.
2
•THINK PRM uses only 8K process labels and outperforms discriminative PRMs, trained on two
orders of magnitude more labels by up to 8% Macro F1 points on ProcessBench.
•We further show that THINK PRM outperforms strong baseline PRMs in best-of-N and guided-
search setups, on two in-domain tasks: MATH-500, AIME 2024, and two out-of-domain tasks:
GPQA-Diamond and LiveCodeBench. THINK PRM also outperforms LLM-as-a-judge baselines,
and can enable better scaling of verifier compute by thinking longer.
2 Background and Related Work
Process reward models (PRMs) evaluate reasoning solutions step-by-step by taking a problem and a
multi-step solution prefix as input, and producing a sequence of scores, each assessing the correctness
or quality of a specific step. PRMs can be categorized as discriminative or generative.
Discriminative PRMs. Discriminative PRMs are trained as classifiers that directly predict numeri-
cal correctness scores for each step, typically relying on extensive step-level annotations (Uesato et al.,
2022; Lightman et al., 2023; Zhang et al., 2025). Given a solution prefix, discriminative PRMs encode
the solution text and employ a classification head to produce step-level scores, usually optimized with
binary cross-entropy. An overall correctness score for a solution is obtained by aggregating these
step-level scores (Beeching et al.; Snell et al., 2024; Wu et al., 2024). While discriminative PRMs are
effective and straightforward, they do not utilize the language-modeling head of the base language
model, making training expensive and labor-intensive (Yuan et al., 2024). Additionally, they offer
limited interpretability and require fixed inference-time compute, restricting their dynamic scalability
at test-time (Zhang et al., 2024a; Mahan et al., 2024).
Generative PRMs and LLM-as-a-Judge. Generative PRMs (Zheng et al., 2023; Zhu et al., 2023)
frame verification as a language-generation task, producing step-level correctness decisions as natural
language tokens (e.g., “correct” or “incorrect”), typically accompanied by a verification chain-of-
thought (CoT). Generative PRMs rely on the standard language modeling objective, training on
verification rationales rather than on binary labels. Step-level correctness scores can be derived from
generative PRMs by computing conditional token probabilities, e.g., P(“correct” ). This approach
leverages the strengths of LLMs in text generation, making generative PRMs inherently interpretable
and scalable (Zhang et al., 2024a; Mahan et al., 2024; Wang et al., 2023a; Ankner et al., 2024).
Recent work on generative PRMs often utilizes off-the-shelf LLMs prompted to critique solu-
tions—known as LLM-as-a-Judge (Zheng et al., 2024). However, LLM-as-a-Judge can be unreliable,
sensitive to prompt phrasing, and prone to invalid outputs, such as infinite looping or excessive
overthinking (Bavaresco et al., 2024), issues we further confirm in this work. Prior results with
reasoning models like QwQ-32B-Preview (Team, 2024) show promise, but their practical utility in
test-time scaling remains limited without additional training (Zheng et al., 2024).
Test-Time Scaling and PRMs. Test-time scaling techniques, such as Best-of-N selection (Charniak
& Johnson, 2005; Khalifa et al., 2023; Snell et al., 2024) and tree-based search (Wu et al., 2024; Yao
et al., 2023; Chen et al., 2024c; Wan et al., 2024), leverage additional inference-time compute to
improve reasoning performance. Central to these approaches is the quality of the verifier used to score
and select solutions (Beeching et al.; Snell et al., 2024). While both discriminative and generative
PRMs can guide these processes, generative PRMs uniquely support simultaneous scaling of both
generator and verifier compute (Zhang et al., 2024a; Kalra & Tang, 2025). We show that generative
PRMs trained based on long CoT models (Jaech et al., 2024; Guo et al., 2025; Muennighoff et al.,
2025) enable sequential scaling of verification compute by forcing longer verification CoT.
Motivated by limitations of existing approaches, our work builds upon prior efforts that train reward
models using synthetic data (Zhu et al., 2023; Wang et al., 2024), aiming to develop generative PRMs
with minimal, carefully filtered synthetic step-level supervision. Specifically, we demonstrate that a
generative PRM fine-tuned with as few as 8K synthetic verification chains substantially improves
over LLM-as-a-Judge PRMs, and outperforms discriminative PRMs trained on datasets orders of
magnitude larger.
3
Example verification chain-of-thought
<think>
Alright, I’ve got this math problem about finding the surface area of a solid after cutting out a small
cube from a larger rectangular prism. The original prism has dimensions 4 by 2 by 2, and a 1 by 1 by 1
cube is removed from one corner. The provided solution has four steps, and I need to review each one
to see if it’s correct.
Let’s start with Step 1.
Step 1: The solution calculates the surface area of the original rectangular prism. The formula used
isAoriginal = 2(lw+lh+wh), which is correct for a rectangular prism. Plugging in the dimensions
l= 4,w= 2, andh= 2, the calculation goes like this:
Aoriginal = 2(4 ·2 + 4·2 + 2·2) = 2(8 + 8 + 4) = 2 ·20 = 40
This seems right. So, Step 1 is \\boxed {correct }.
Step 2: Here, the solution talks about the effect of cutting out the 1 by 1 by 1 cube from one corner. It
says that removing the cube takes away three faces but adds three new faces. Each face is 1 by 1, so
each has an area of 1 square unit.
Wait a minute, is this accurate? When you cut out a cube from a corner, you are indeed removing
material, but how does that affect the surface area?
Let me visualize this. Imagine a rectangular prism and cutting out a small cube from one corner. The
cube has three faces exposed to the outside of the prism. When you remove the cube, those three faces
are no longer part of the surface area. However, the removal also creates three new internal faces where
the cube was taken out, which are now part of the surface area.
...
Alternatively, perhaps I can think about the new solid’s dimensions.
But I think the initial approach is correct.
Therefore, Step 2 is \\boxed {correct }.
...
Step 3: This step calculates the net change in surface area as zero, which follows from Step 2.
Since the net change is zero, the surface area remains the same as the original.
So, Step 3 is \\boxed {correct }.
Step 4: Concludes that the surface area of the new solid is 40, the same as the original.
Therefore, Step 4 is \\boxed {correct }.
Wait, but I recall that sometimes when you make a cut, especially in certain ways, the surface area can
change. But in this case, it seems to hold up.
</think>
Figure 3: Example output by THINK PRM -14B, where it verifies and labels every step in the provided prefix via
a long verification CoT. We omit the problem and solution for brevity.
3 T HINK PRM
Our goal is to obtain a data-efficient, yet powerful, verbalized PRM. Given given problem-solution
pair, our generative PRM should verify every step in the solution via an extended chain-of-thought
(CoT) as shown in the example in Fig. 3. Generating such CoT has several advantages. First, it opens
a window to the reasoning process of the verifier, enabling better interpretability of its decisions.
Second, it capitalizes on the capabilities of reasoning models and enables strong verifiers with
minimal training. Third, this thinking process enables scaling up the verifier compute either by
parallely sampling multiple CoTs and aggregating their decisions, or by allowing the model to revise
itself in-context by forcing it to reconsider or double-check its verification.
Throughout the rest of the paper, we aim to answer the following research questions:
•RQ1: How well do large reasoning models (LRMs) perform when used as LLM-as-a-judge verifiers
for process-level evaluation? Section 3.1
•RQ2: Can lightweight finetuning on synthetic verification CoTs improve the reliability and
effectiveness of these models as process verifiers? Section 3.2
4
0 1 2 3 4 5 6 7 8
Length (K tokens)020406080100120140160FrequencyOverthinking,
repetition,
infinite looping, etc.LLM-as-a-Judge
012345678
Length (K tokens)050100150200250300FrequencyThinkPRM
Accurate Verification Inaccurate VerificationFigure 4: Verifier performance on ProcessBench in light of CoT lengths. On the left, LLM-as-a-judge produces
excessively long chains including repetition, infinite looping, and overthinking, leading to worse verifier
performance since the output never terminates. Training on collected syntehtic data substantially reduces these
issues as shown in the T HINK PRM plot on the right.
•RQ3: How does a finetuned verbalized PRM ( THINK PRM ) compare to discriminative PRMs and
LLM-as-a-Judge baselines under different test-time scaling scenarios? Section 4
3.1 RQ1: LLM-as-a-judge PRMs are suboptimal
This section highlights limitations we observe when using off-the-shelf reasoning models as process
verifiers, suggesting the need for finetuning. For evaluation, we use ProcessBench (Zheng et al., 2024),
which includes problem-solution pairs with problems sourced from four different math benchmarks,
labeled based on the index of the first incorrect step, if exists. We report the F1-score over the two
most challenging subsets of ProcessBench: OlympiadBench (He et al., 2024) and OmniMath (Gao
et al., 2024), each comprised of 1K problem-prefix pairs. Table 2 shows LLM-as-a-judge F1 scores on
each subset and a sample verification chain generated by QwQ-32B-Preview is displayed in Fig. 25
in Appendix F.
We observe the following issues with LLM-as-a-judge verification. First, the verification quality is
highly sensitive to the instruction wording, e.g., changing a few words in the instruction could affect
the F1-score by up to 3-4 points in some cases. Second, a substantial number of the generated chains
include invalid judgments , i.e., chains without an extractable overall label as clear in Section 3.2.
Such invalid judgements are caused by the following. In some cases, final decision was in the wrong
format than instructed e.g., the model tries to solve the problem from scratch rather than verify the
given solution—a behavior likely stemming from the model training process. In addition, we noted
multiple instances of overthinking (Chen et al., 2024b; Cuadron et al., 2025), which prevents the
model from terminating within the maximum token budget, and infinite looping/repetitions , where
the model gets stuck trying alternative techniques to verify the solutions.
Fig. 4 (left) shows a histogram of verification CoT lengths generated by R1-Qwen-14B in the LLM-
as-a-judge setting. Accurate CoTs tend to be shorter, typically under 3000 tokens, while inaccurate
CoTs are more evenly distributed and spike sharply around 7000–8000 tokens, highlighting the
prevalence of overthinking and looping in long chains. We show examples of these behaviors in
Appendix B. In the next section, we mostly fix these issues via lightweight finetuning over synthetic
verification CoTs.
3.2 RQ2: Finetuning on synthetic verification chains boosts LLM-as-a-judge verification
Inspired by recent work on reducing overthinking in long CoT models that by training (Yu et al., 2024;
Kang et al., 2024), we aim to improve LLM-as-a-judge performance via finetuning on high-quality
verification data. Collecting real data would be expensive, so we rely on filtered synthetic data
(Zelikman et al., 2022; Singh et al., 2023; Dong et al., 2023; Zhang et al., 2024b; Wang et al., 2024)
also known as rejection sampling finetuning. To keep our approach simple, we refrain from more
expensive training techniques such as reinforcement learning or preference-based learning.
5
?Step 1: …Step 2: …Step 3: …<think>…Step 1 accurately… and is \boxed{correct}…Step 2 omits… \boxed{incorrect}…..Step 3 … \boxed{incorrect}</think>2. Compare against process labels 
<think>…Step 1 calculates… Therefore is \boxed{correct}…Step 2 … is \boxed{correct}…..Step 3… is \boxed{incorrect}</think>Finetuning data. . . SolutionProblemReasoning model Step 1: CorrectStep 2: CorrectStep 3: Incorrect
✅
❌
✅
✅
✅
Step 1: CorrectStep 2: CorrectStep 3: Incorrect1. Sample veriﬁcation chains
3. Keep good chains
❌
✅
Discard!
❌
Reasoning Model Figure 5: Collecting verification chains for finetuning. First, we prompt a reasoning model, in our case QwQ-
32B-Preview to critique a given solution to a problem. Then, we sample multiple verification chains, which we
judge against gold process labels from PRM800K, only keeping chains that match the gold process labels.
Synthetic data collection. As training data, we sample synthetic verification CoTs from QwQ-32B-
Preview, prompting it to verify each step in a solution prefix, using the instruction shown in Fig. 14.
The problems and corresponding step-by-step solutions come from the PRM800K dataset (Lightman
et al., 2023), which provides both model-generated solutions and human-verified step-level labels.
We sample until we obtain 1K verification CoTs that satisfy the following criteria: (i)they must
follow the expected format (i.e., include an extractable decision label for each step inside \boxed{}
as shown in Fig. 3), and (ii)these step decisions match the gold step labels from PRM800K, and
(iii)are under a certain token length—to avoid the excessive overthinking behaviour we observed in
Fig. 4 (left). The filtering process ensure our training data is of sufficient quality while being very
cheap to obtain.2Data collection is illustrated in Fig. 5. Notably, our filtering relies only on step-level
annotations, not on gold verification rationales or CoTs—making this data pipeline scalable and
low-overhead. For instance, in the absence of gold step-level annotations, one can obtain silver labels
via Monte Carlo rollouts (Wang et al., 2023b; Chen et al., 2024a). Statistics of the dataset are in
Appendix A.1 and a training example is in Fig. 13. We note that this data filtering process is crucial
for the performance of the resulting PRM as we show in Section 5.2.
While we train only on math data, the resulting PRM remains robust under other domains such as
Physics and code generation as we show in Section 4.2. We then proceed to train our models on the
1K collected chains. Our training is very lightweight; finetuning QwQ-32B-Preview takes only 4.5
hours on a single A100 80GB GPU. Refer to Appendix C.1 for training details.
Finetuning on synthetic verification CoTs substantially improves the verifier. THINK PRM
trains on the 1K chains and is evaluated on ProcessBench and compared to LLM-as-a-judge under
the same base model. Section 3.2 shows verifier accuracy of different models before and after our
finetuning. We note a substantial boost in F1 across all models, with the 1.5B model gaining most
improvement by over 70 F1 points, and the 14B model performing best. Looking at the ratio of invalid
judgements in Section 3.2, we also note a significant reduction in invalid labels with all models,
except for QwQ, where it slightly increases. Lastly, the reduction in overthinking and infinite looping
behavior discussed in the last section is evident, as in Fig. 4 (right), where THINK PRM generations
maintain a reasonable length (1000-5000) tokens while being substantially more accurate.
4 Main Results
This section aims to answer RQ3 introduced in section 3 by comparing THINK PRM to baselines
under different scaling scenrios. We will study how THINK PRM performs under different generation
budgets (i)best-of-N selection (Wu et al., 2024; Brown et al., 2020) and (ii)guided beam search
(Snell et al., 2024; Beeching et al.). We will also explore how THINK PRM performs when verifier
compute is scaled either in parallel i.e., by aggregating decisions over multiple verification CoTs or
2Spurious chains may still exist.
6
QwQ-32B-previewR1-Qwen-14B R1-Qwen-7BR1-Qwen-1.5B0102030405060Percentage of total (%)11.5
2.3 1.2 1.99.416.019.553.2CoT s without a valid label on ProcessBench
ThinkPRM LLM-as-a-judgeFigure 6: LLM-as-a-judge suffers from a significant
ratio of verification CoTs that do not terminate with
a parsable label i.e., \boxed{yes}or\boxed{no}.
Our finetuning process that yields THINK PRM , sub-
stantially mitigates this issue. Both verifiers are based
on R1-Distill-Qwen-14B.
QwQ-32B-previewR1-Qwen-14B R1-Qwen-7BR1-Qwen-1.5B020406080100F1-score73.286.5
73.776.0
53.070.3
45.2
5.2randomVerifier performance on ProcessBench
ThinkPRM LLM-as-a-judgeFigure 7: Verification accuracy on 2K question-
solution pairs from two most challenging subsets
of ProcessBench: OlympiadBench and OmniMath.
THINK PRM obtained by finetuning the correpond-
ing model over only 1K verification chains performs
better.
sequentially by extending verification tokens by forcing the model to double-check or self-correct its
verification process.
4.1 Experimental Setup
In the remainder of the the paper, we will mainly use our finetuned verifiers based on R1-Distill-Qwen-
1.5B and R1-Distill-Qwen-14B as these provide the best tradeoff between size and performance. We
will refer to these as T HINK PRM-1.5B and T HINK PRM-14B, respectively.
Baselines. We compare THINK PRM toDiscPRM , which is the same base model as THINK PRM ,
finetuned with binary cross-entropy on the entire PRM800K dataset, totalling 712K process labels
from 98K problem-solution pairs. Details on finetuning DiscPRMs are in Appendix C.2. We also
compare to unweighted majority voting , which merely selects the most frequent answer across the
samples (Wang et al., 2022). For our best-of-N experiments except on AIME ’24, we use verifier-
weighted majority select the best answer, which scores final answers based on the sum of the verifier
scores of their solutions (Uesato et al., 2022; Wu et al., 2024). For guided beam search, we select the
most common final answer over all beams.
Tasks and Models. We show results on three math reasoning tasks, namely 100 problems from
MATH-500 (Hendrycks et al., 2021; Lightman et al., 2023) covering all difficulty levels (see Ap-
pendix E.5 for more details), and American Invitational Mathematics Examination (AIME) problems
for 2024. Since THINK PRM was finetuned only on math data, we study the out-of-domain general-
ization on two tasks: science QA and code generation. For science QA, we use the physics subset of
GPQA-Diamond (Rein et al., 2024), consisting of 86 PhD-level multiple choice questions. For code
generation, we use 200-problem subset from the v5 release of LiveCodeBench (Jain et al., 2024).
Over MATH-500, we show results with THINK PRM -1.5B and THINK PRM -14B on two different
generator models: Qwen-2.5-14B and Llama-3.2-3B-Instruct. The former model is used for best-of-N
and the latter for beam search as search can be quite slow. Showing results with different generators
guarantees that our conclusions are not specific to a certain model family or size. For the more
challenging tasks, namely AIME24, and GPQA, we use a more capable model, namely Qwen-2.5-
32B-Instruct. For code generation, we use Qwen-2.5-Coder-7B (Hui et al., 2024). Hyperparameter
details on best-of-N and beam search are in Appendix E.
Scaling verifier compute. Compared to DiscPRMs, generative reward models enable an extra
dimension of scaling to squeeze more performance: scaling the verifier compute. THINK PRM allows
for two types of scaling. First, we use parallel scaling (Mahan et al., 2024; Brown et al., 2024), by
sampling Kindependent verification CoT and averaging their performance. We will refer to this
scaling using “@K” throughout the rest of the paper. Second, more specific to long reasoning models,
7
20212223
Number of solutions20.022.525.027.530.032.5Accuracy (%)
Generator: Qwen2.5-32B-InstructBest-of-N: AIME '24
20212223242526
Number of solutions50607080Accuracy (%)
Generator: Qwen2.5-14BBest-of-N: MATH-500
ThinkPRM-14B DiscPRM-14B LLM-as-a-judge MajorityFigure 8: Best-of-N on AIME ’24 and MATH-500. Compared to LLM-as-a-judge, DiscPRM, and (unweighted)
majority vote, T HINK PRM-14B exhibits best accuracy scaling curve.
2021222324
Number of beams55.057.560.062.565.067.570.072.5Accuracy (%)
Generator: Llama-3.2-3B-InstructGuided beam search: MATH-500
ThinkPRM-1.5B
ThinkPRM-1.5B@4
RLHFFlow-8B-DeepseekDiscPRM-1.5B
MathShepherd-7B
Figure 9: Left: Comparison to Off-the-shelf PRMs
trained on substantially more process labels than
THINK PRM.
2021222324
Number of beams56586062646668Accuracy (%)
Generator: Llama-3.2-3B-InstructFiltering based on Process vs. Outcome
Process-based (ours)
Outcome-based (GenRM)LLM-as-a-judgeFigure 10: Right: Ablating the data filtering mecha-
nism, where our process-based filtering yields better
PRMs. LLM-as-a-judge is shown with number of
beams = 16.
we use sequential scaling e.g., by enabling the model to self-correct its initial verification (Xiong
et al., 2025; Kumar et al., 2024; Ye et al., 2024). Inspired by Muennighoff et al. (2025), we use a
trigger phrase such as “Let’s verify again” to prime the model to double check its earlier verification
for potential errors. See Appendix E.4 for more details.
4.2 Results
THINK PRM outperforms DiscPRM and LLM-as-a-Judge. On best-of-N selection with MATH-
500 shown in Fig. 8 (right), THINK PRM leads to higher or comparable reasoning accuracy to
DiscPRM under all sampling budgets. The trend holds on the more challenging AIME24 as shown in
Fig. 8 left. Under verifier-guided search, Fig. 1 (right) shows beam search results on MATH-500,
with THINK PRM 1.5B clearly outperforming discPRM by about 5% points. It also outperforms
LLM-as-a-judge with the same base model, i.e., R1-Qwen-1.5B.
THINK PRM surpasses off-the-shelf PRMs. We compare THINK PRM -1.5B to two strong off-the-
shelf PRMS, namely RLHFFlow-Deepseek-PRM (Xiong et al., 2024) and MATH-Shepherd-PRM
(Wang et al., 2023b). These PRMs are trained on orders of magnitude more data and have more
parameters than THINK PRM -1.5B. We show results under verifier-guided search on MATH-500 in
Fig. 9, with THINK PRM -1.5B’s scaling curve surpassing all baselines and outperforming RLHFFlow-
Deepseek-PRM, the best off-the-shelf PRM among the ones we tested, by more than 7% at 16
beams.
8
202122232425
Number of solutions55606570Accuracy (%)
Generator: Qwen-2.5-32B-InstructBest-of-N: GPQA-physics (OOD)
202122232425
Number of solutions55.057.560.062.565.0Pass rate (%)
Generator: Qwen-2.5-Coder-7BBest-of-N: LiveCodeBench (OOD)
ThinkPRM-14B DiscPRM-14B Qwen2.5-7B-Math-PRM MajorityFigure 11: Best-of-N on two out-of-domain tasks: science QA (GPQA-Physics) and code generation (Live-
CodeBench). Although THINK PRM was only finetuned on math, it exhibits superior OOD performance than
the baselines, especially at larger sampling budgets, where the baselines performance falls short. Discriminative
PRMs struggle despite being trained on order of magnitude more process labels.
THINK PRM excels on out-of-domain tasks. As for OOD performance on GPQA-physics (Fig. 11
left), THINK PRM scales better than DiscPRM—which drops substantially at N=32—outperforming
it by 8%. On LiveCodeBench (Fig. 11 right), THINK PRM also outperforms DiscPRM by 4.5%. On
LiveCodeBench, Qwen2.5-7B-Math-PRM (Zhang et al., 2025)—a discriminative PRM trained on
subtantial amount of process labels obtained from LLM-as-a-judge data and Monte Carlo rollouts—
struggles when applied out-of-domain. Our results shed light on the fragility of discriminative
PRMs under domain shifts in contrast with generative PRMs. These results point at the strong OOD
generalization abilities of T HINK PRM compared to discriminative models.
Scaling THINK PRM compute boosts performance. Under beam search (shown in Fig. 9) parallel
scaling with THINK PRM -1.5B@4, boosts the verifier performance by more than 5% points, and
yields the best accuracy on MATH-500. In addition, parallel scaling with THINK PRM -14B@4
andTHINK PRM -14B@8 boosts best-of-N performance on MATH-500 as shown in Fig. 20 in
Appendix E.6. Now we move to sequential scaling of verifier compute by forcing THINK PRM to
recheck its own verification. Since this can be compute-intensive, we only run this on 200 problems
from OmniMath subset of ProcessBench, we compare how verification accuracy improves as we
force the model to think for longer as shown in Fig. 2. THINK PRM exhibits better scaling behaviour
compared to LLM-as-a-judge, which drops after 16K tokens, and outperforms DiscPRM-14B by 15
F1 points. In summary, THINK PRM is consistently better than LLM-as-a-Judge under parallel and
sequential scaling.
Parallel scaling vs. sequential scaling. Is it preferable to scale verifier compute in parallel or
sequentially? We answer this question by comparing the two modes of scaling under the same token
budget. Fig. 21 in Appendix E.6 shows performance of best-of-N with Qwen-2.5-14B under parallel
and sequential scaling with K= 2,4under both parallel scaling and sequential scaling. Overall,
the performance of both methods is fairly comparable, but we observe a slight advantage to parallel
scaling in some cases.
5 Analysis
This section provides further analysis and ablations to further udIn this section, we study the training
data efficiency of THINK PRM compared to discriminative PRMs (Section 5.1). Next, we validate
the effect of process-based data filtering strategy by comparing it against outcome-based filtering
(Section 5.2). We then examine how THINK PRM ’s performance varies with problem difficulty,
highlighting its effectiveness on more challenging reasoning tasks (Section 5.3). Finally, we discuss
some inherent limitations of the current generative PRM approach (Section 5.4).
9
1 2 3 4 5
Problems binned by difficulty020406080100Accuracy (%)Best-of-32: Math-500
1 2 3 4
Problems binned by difficulty020406080100Accuracy (%)Best-of-32: GPQA-Physics
ThinkPRM-14B DiscPRM-14BFigure 12: THINK PRM helps with challenging reasoning problems compared to DiscPRM. The generator model
here is Qwen-2.5-14B for MATH-500 and Qwen-2.5-32B-Instruct for GPQA.
5.1 Training data efficiency
A major strength of THINK PRM is training data efficiency compared to discriminative versions. We
study the training scaling behavior by training THINK PRM -14B using 500 and 1K examples in total
collected using the process in Section 3.2, which roughly corresponds to 4K and 8K process labels
from PRM800K in total. We compare that to DiscPRM-14B trained with 1K, 10K, 50K and 98K
examples in total, where 98K corresponds to training on the full PRM800K train set, which includes
about 712K step labels. Fig. 1 (Left) contrasts the training data scaling behavior of THINK PRM -14B
with that of DiscPRM-14B, where THINK PRM -14B’s performance scales substantially better with
two orders of magnitude fewer process labels. This primarily stems from THINK PRM ’s utilization of
text generation and reasoning abilities of the underlying models.
While we train THINK PRM using only 1K data points, we investigate whether it will benefit from
training on more data. Using the same data collection process described in Section 3.2, we collect
and filter additional verification CoTs and obtain a total of 65K chains. We then finetune R1-
Distill-Qwen-1.5B and R1-Distill-Qwen-14B on these for a single epoch while keeping all other
training hyperparameters fixed. We then compare the resulting models to the 1K-trained version of
THINK PRM under best-of-N selection on MATH-500. Figs. 22 and 23 in Appendix E.7 show clear
performance boost from training on the 65K examples, showing that THINK PRM can further benefit
from training on more data.
5.2 Filtering based on process vs. outcome labels
Section 3.2 explains how we filter verification CoTs based on the match between the generated
step-level decisions with the gold process labels. To validate this choice in yielding high-quality
synthetic CoTs for training THINK PRM , we compare this process-based filtering to a version using
final answer correctness, as done in GenRM (Zheng et al., 2024), where we ignore step-level labels,
and only consider the matching between the final step decision and the solution correctness. Precisely,
this version only keeps chains if the solution final answer is correct and the final step label is
\boxed{correct }or the final answer is incorrect and the final step label is \boxed{incorrect },
thereby ignoring intermediate step labels when filtering the generated data.
In total, we got 65K and 128K chains from process- and outcome-based filtering, respectively. Fig. 10
shows beam search performance of the resulting PRM from finetuning R1-Distill-Qwen-1.5B under
the two filtering strategies. Clearly, data filtered with outcome only yields substantially worse verifier
compared to data filtered based on process labels, even when containing almost 2x more examples.
This emphasizes the quality of the synthetic data needed to train generative PRMs and the benefit of
process labels in the filtering of data.
5.3 T HINK PRM helps with difficult reasoning problems
The hypothesis is that THINK PRM ’s reasoning process enables it to tackle verification of harder
problems. To check if this is the case, we analyze performance of THINK PRM vs. DiscPRM in
10
light of problem difficulty over MATH-500 and GPQA-physics (details on how we estimate problem
difficulty for GPQA-Physics are in Appendix E.8), shown in Fig. 12. The generators here are Qwen-
2.5-14B for MATH-500 and Qwen-2.5-32B-Instruct for GPQA-Physics. Primarily, THINK PRM
improves reasoning on the difficult (levels 3, 4, 5 in MATH-500 and 2, 3, 4 in GPQA-Physics), with
which DiscPRM struggles. This is likely due to the reasoning power of THINK PRM , which allows it
to reason more effectively about the harder problems and their solutions. This could potentially hint
at a compute-optimal verifier scaling strategy, where verification compute is allocated based on the
query difficulty.
5.4 Limitations of Generative PRMs
We note a few limitations with generative PRMs that may impact their usage in different reasoning
setups. First, overconfidence is a known issue in LLMs (Liu et al., 2023a; Stechly et al., 2023; Zhou
et al., 2024) and, in the case of PRMs, it can cause the predicted PRM scores to cluster near extremes:
close to either 0 or 1. One reason is that we are using probabilities of certain tokens such as “yes” or
“no”, which by nature will be either very high or very low. Future work should explore better ways to
extract better calibrated scores from generative reward models. Another limitation is a result of the
autoregressive nature of their reasoning, which can lead them to prematurely commit to an earlier
judgment. For example, we observe a phenomenon we term step label interference , where verification
errors in earlier solution steps could impact verification of later steps. For example, we noted that if
the PRM judges a particular step as incorrect, it becomes more likely to label subsequent steps as
incorrect. Lastly, generating a verification chains-of-thought introduces additional computational
overhead compared to discriminative PRMs. However, the performance gains offered by generative
PRMs compared to the baselines justifies this extra cost.
Conclusion
This work introduced THINK PRM , a generative process reward model trained with minimal supervi-
sion on synthetic data, enabling efficient and scalable verification of step-by-step reasoning. Through
extensive evaluation, we demonstrated that lightweight fine-tuning of generative PRMs on as few as
8K process labels can substantially improve upon zero-shot LLM-as-a-judge baselines. Importantly,
our approach also surpasses discriminative PRMs trained with orders of magnitude more process
labels, highlighting the advantages of utilizing generative language-modeling objectives for inter-
pretability, scalability, and data efficiency. Our results underscore the potential of generative PRMs to
effectively scale verification compute at test-time, particularly benefiting challenging domains such
as mathematical and scientific reasoning.
Acknowledgements
This research is supported by LG AI Research. In addition, we would like to thank Yunxiang Zhang,
Xin Liu, Sean Welleck, and Lewis Tunstall for very useful discussions.
References
Ekin Aky ¨urek, Mehul Damani, Linlu Qiu, Han Guo, Yoon Kim, and Jacob Andreas. The surprising
effectiveness of test-time training for abstract reasoning. arXiv preprint arXiv:2411.07279 , 2024.
Zachary Ankner, Mansheej Paul, Brandon Cui, Jonathan D Chang, and Prithviraj Ammanabrolu.
Critique-out-loud reward models. arXiv preprint arXiv:2408.11791 , 2024.
Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fern ´andez,
Albert Gatt, Esam Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, et al. Llms instead
of human judges? a large scale empirical study across 20 nlp evaluation tasks. arXiv preprint
arXiv:2406.18403 , 2024.
Edward Beeching, Lewis Tunstall, and Sasha Rush. Scaling test-time compute
with open models. URL https://huggingface.co/spaces/HuggingFaceH4/
blogpost-scaling-test-time-compute .
11
Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V Le, Christopher R ´e, and
Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling.
arXiv preprint arXiv:2407.21787 , 2024.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020.
Eugene Charniak and Mark Johnson. Coarse-to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual Meeting of the Association for Computational
Linguistics (ACL’05) , pp. 173–180, 2005.
Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. Alphamath almost zero: process supervision
without process. arXiv preprint arXiv:2405.03553 , 2024a.
Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu,
Mengfei Zhou, Zhuosheng Zhang, et al. Do not think that much for 2+ 3=? on the overthinking of
o1-like llms. arXiv preprint arXiv:2412.21187 , 2024b.
Ziru Chen, Michael White, Raymond Mooney, Ali Payani, Yu Su, and Huan Sun. When is tree search
useful for llm planning? it depends on the discriminator. arXiv preprint arXiv:2402.10890 , 2024c.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher
Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint
arXiv:2110.14168 , 2021.
Alejandro Cuadron, Dacheng Li, Wenjie Ma, Xingyao Wang, Yichuan Wang, Siyuan Zhuang, Shu
Liu, Luis Gaspar Schroeder, Tian Xia, Huanzhi Mao, et al. The danger of overthinking: Examining
the reasoning-action dilemma in agentic tasks. arXiv preprint arXiv:2502.08235 , 2025.
Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao,
Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative
foundation model alignment. arXiv preprint arXiv:2304.06767 , 2023.
Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma,
Liang Chen, Runxin Xu, et al. Omni-math: A universal olympiad level mathematic benchmark for
large language models. arXiv preprint arXiv:2410.07985 , 2024.
Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,
Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms
via reinforcement learning. arXiv preprint arXiv:2501.12948 , 2025.
Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu,
Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: A challenging benchmark for
promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint
arXiv:2402.14008 , 2024.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,
and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv
preprint arXiv:2103.03874 , 2021.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR , 1(2):3, 2022.
Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song,
and Denny Zhou. Large language models cannot self-correct reasoning yet. arXiv preprint
arXiv:2310.01798 , 2023.
Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang,
Bowen Yu, Kai Dang, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186 ,
2024.
Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec
Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint
arXiv:2412.16720 , 2024.
12
Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando
Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free
evaluation of large language models for code. arXiv preprint arXiv:2403.07974 , 2024.
Nimit Kalra and Leonard Tang. Verdict: A library for scaling judge-time compute. arXiv preprint
arXiv:2502.18018 , 2025.
Yu Kang, Xianghui Sun, Liangyu Chen, and Wei Zou. C3ot: Generating shorter chain-of-thought
without compromising effectiveness. arXiv preprint arXiv:2412.11664 , 2024.
Muhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, and Lu Wang. Grace:
Discriminator-guided chain-of-thought reasoning. arXiv preprint arXiv:2305.14934 , 2023.
Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun,
Seongjin Shin, Sungdong Kim, James Thorne, et al. Prometheus: Inducing fine-grained eval-
uation capability in language models. In The Twelfth International Conference on Learning
Representations , 2023.
Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes, Avi Singh, Kate Baumli,
Shariq Iqbal, Colton Bishop, Rebecca Roelofs, et al. Training language models to self-correct via
reinforcement learning. arXiv preprint arXiv:2409.12917 , 2024.
Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu,
Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluating reward models
for language modeling. arXiv preprint arXiv:2403.13787 , 2024.
Dacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi Mo, Shishir G Patil, Matei Zaharia, Joseph E
Gonzalez, and Ion Stoica. Llms can easily learn to reason from demonstrations structure, not
content, is what matters! arXiv preprint arXiv:2502.07374 , 2025.
Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making
language models better reasoners with step-aware verifier. In Proceedings of the 61st Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 5315–5333,
2023.
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan
Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. arXiv preprint
arXiv:2305.20050 , 2023.
Xin Liu, Muhammad Khalifa, and Lu Wang. Litcab: Lightweight language model calibration over
short-and long-form responses. arXiv preprint arXiv:2310.19208 , 2023a.
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: Nlg
evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634 , 2023b.
Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun
Zhu, Lei Meng, Jiao Sun, et al. Improve mathematical reasoning in language models by automated
process supervision. arXiv preprint arXiv:2406.06592 , 2, 2024.
Dakota Mahan, Duy Van Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato,
Jan-Philipp Fr ¨anken, Chelsea Finn, and Alon Albalak. Generative reward models. arXiv preprint
arXiv:2410.12832 , 2024.
Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke
Zettlemoyer, Percy Liang, Emmanuel Cand `es, and Tatsunori Hashimoto. s1: Simple test-time
scaling. arXiv preprint arXiv:2501.19393 , 2025.
David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani,
Julian Michael, and Samuel R Bowman. Gpqa: A graduate-level google-proof q&a benchmark. In
First Conference on Language Modeling , 2024.
Avi Singh, John D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter J
Liu, James Harrison, Jaehoon Lee, Kelvin Xu, et al. Beyond human data: Scaling self-training for
problem-solving with language models. arXiv preprint arXiv:2312.06585 , 2023.
13
Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally
can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314 , 2024.
Kaya Stechly, Matthew Marquez, and Subbarao Kambhampati. Gpt-4 doesn’t know it’s wrong: An
analysis of iterative prompting for reasoning problems. arXiv preprint arXiv:2310.12397 , 2023.
Zhiqing Sun, Longhui Yu, Yikang Shen, Weiyang Liu, Yiming Yang, Sean Welleck, and Chuang
Gan. Easy-to-hard generalization: Scalable alignment beyond human supervision. arXiv preprint
arXiv:2403.09472 , 2024.
Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown, November 2024. URL
https://qwenlm.github.io/blog/qwq-32b-preview/ .
Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia
Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and
outcome-based feedback. arXiv preprint arXiv:2211.14275 , 2022.
Ziyu Wan, Xidong Feng, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and
Jun Wang. Alphazero-like tree-search can guide large language model decoding and training. In
Forty-first International Conference on Machine Learning , 2024.
Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and
Jie Zhou. Is chatgpt a good nlg evaluator? a preliminary study. arXiv preprint arXiv:2303.04048 ,
2023a.
Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Y Wu, and Zhifang
Sui. Math-shepherd: A label-free step-by-step verifier for llms in mathematical reasoning. arXiv
preprint arXiv:2312.08935 , 2023b.
Tianlu Wang, Ilia Kulikov, Olga Golovneva, Ping Yu, Weizhe Yuan, Jane Dwivedi-Yu,
Richard Yuanzhe Pang, Maryam Fazel-Zarandi, Jason Weston, and Xian Li. Self-taught evaluators.
arXiv preprint arXiv:2408.02666 , 2024.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency
improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171 , 2022.
Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference scaling laws: An
empirical analysis of compute-optimal inference for problem-solving with language models. arXiv
preprint arXiv:2408.00724 , 2024.
Wei Xiong, Hanning Zhang, Nan Jiang, and Tong Zhang. An implementation of generative prm.
https://github.com/RLHFlow/RLHF-Reward-Modeling , 2024.
Wei Xiong, Hanning Zhang, Chenlu Ye, Lichang Chen, Nan Jiang, and Tong Zhang. Self-rewarding
correction for mathematical reasoning. arXiv preprint arXiv:2502.19613 , 2025.
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan.
Tree of thoughts: Deliberate problem solving with large language models. Advances in neural
information processing systems , 36:11809–11822, 2023.
Tian Ye, Zicheng Xu, Yuanzhi Li, and Zeyuan Allen-Zhu. Physics of language models: Part 2.2, how
to learn from mistakes on grade-school math problems. arXiv preprint arXiv:2408.16293 , 2024.
Ping Yu, Jing Xu, Jason Weston, and Ilia Kulikov. Distilling system 2 into system 1. arXiv preprint
arXiv:2407.06023 , 2024.
Lifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan
Liu, and Hao Peng. Free process rewards without process labels. arXiv preprint arXiv:2412.01981 ,
2024.
Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with
reasoning. Advances in Neural Information Processing Systems , 35:15476–15488, 2022.
14
Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal.
Generative verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240 ,
2024a.
Yunxiang Zhang, Muhammad Khalifa, Lajanugen Logeswaran, Jaekyeom Kim, Moontae Lee,
Honglak Lee, and Lu Wang. Small language models need strong verifiers to self-correct rea-
soning. arXiv preprint arXiv:2404.17140 , 2024b.
Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu,
Jingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical
reasoning. arXiv preprint arXiv:2501.07301 , 2025.
Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jin-
gren Zhou, and Junyang Lin. Processbench: Identifying process errors in mathematical reasoning.
arXiv preprint arXiv:2412.06559 , 2024.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,
Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and
chatbot arena. Advances in Neural Information Processing Systems , 36:46595–46623, 2023.
Kaitlyn Zhou, Jena D Hwang, Xiang Ren, and Maarten Sap. Relying on the unreliable: The impact
of language models’ reluctance to express uncertainty. arXiv preprint arXiv:2401.06730 , 2024.
Lianghui Zhu, Xinggang Wang, and Xinlong Wang. Judgelm: Fine-tuned large language models are
scalable judges. arXiv preprint arXiv:2310.17631 , 2023.
15
A Training data
A.1 Sampling
Fig. 5 shows the prompt used with QwQ-32B-Preview to sample verification chains for training. We
use the problem-prefix pairs from PRM800K train split (Lightman et al., 2023), which is based on
MATH (Hendrycks et al., 2021). We sample 4 verification chains for each prefix in the dataset with
temperature T= 0.1, until we are left with 1K verification CoTs that (1) follow the desired format
i.e., the model generated an extractable label for every step in the prefix and (2) the verbalized step
labels match the gold labels in the PRM800K dataset annotation, (3) at most 4096 tokens long, and
are (4) roughly balanced with respect to final answer correctness. Table 1 shows the statistics of the
final dataset. We also show an example of 1K training CoTs in Fig. 13.
A.2 Preprocessing
From the sampled verification CoTs, we extract boxed predictions (e.g., \boxed{correct} or
\boxed{incorrect} ) for each reasoning step. Then we filter data as follows:
• CoTs with missing or malformed verification decisions are discarded.
•CoTs where verification decisions don’t match ground truth labels are filtered out. Precisely, we
require all step-by-step verification decisions to match the ground truth labels.
Then we clean the data as follows:
• We standardize boxed notation formats.
• Content after the last verification decision is removed.
• Special tokens (e.g., <think> and</think> ) are added to designate verification reasoning.
Correctness Distribution
# correct 486 (48.6%)
# incorrect 514 (51.4%)
Step label distribution
# correct 7474 (92.3%)
# Incorrect 625 (7.7%)
Prefix statistics
# unique questions 869
average steps per prefix 9.5
min # steps 1
max # steps 53
Verification Chain Statistics
Average verification chain length (tokens) 1037.0
Min verification chain length (tokens) 207
Max verification chain length (tokens) 3669
Table 1: Statistics of the 1K finetuning dataset.
16
Synthetic training sample
Problem: Find the area in square feet of a square with a perimeter of 32ft.
Solution:
Step 1: A square has four equal sides, so the perimeter is four times the length of one side.
Step 2: I can write an equation to relate the perimeter and the side length: 4s = 32, where s is the side
length in feet.
Step 3: To solve for s, I can divide both sides by 4: s = 32/4 = 8.
Step 4: Now I know the side length of the square is 8 feet.
Step 5: The area of a square is the side length squared, so the area is s² = 8² = 64.
Step 6: The area is in square feet, so the units are ft².
Step 7: The answer is 64 ft²
<think>
Alright, let’s verify step by step the solution provided for finding the area of a square with a perimeter
of 32 feet.
Step 1: “A square has four equal sides, so the perimeter is four times the length of one side.”
Critique: This statement is accurate. A square indeed has four sides of equal length, and the perimeter
is the sum of all sides. Therefore, perimeter (P) equals 4 times the side length (s), or P = 4s.
The step is \\boxed{correct }
Step 2: “I can write an equation to relate the perimeter and the side length: 4s = 32, where s is the side
length in feet.”
Critique: Given that the perimeter is 32 feet, setting up the equation 4s = 32 is correct. It properly
represents the relationship between the perimeter and the side length of the square.
The step is \\boxed{correct }
Step 3: “To solve for s, I can divide both sides by 4: s = 32/4 = 8.”
Critique: This step correctly solves for s by dividing both sides of the equation by 4. The calculation 32
divided by 4 equals 8 is accurate.
The step is \\boxed{correct }
Step 4: “Now I know the side length of the square is 8 feet.”
Critique: Based on the previous step, s = 8 feet, which is correctly stated here.
The step is \\boxed{correct }
Step 5: “The area of a square is the side length squared, so the area is s² = 8² = 64.”
Critique: The formula for the area of a square is indeed s squared. Calculating 8 squared equals 64 is
correct.
The step is \\boxed{correct }
Step 6: “The area is in square feet, so the units are ft².”
Critique: Since the side length is in feet, the area will be in square feet (ft ²). This is the correct unit for
area.
The step is \\boxed{correct }
Step 7: “The answer is 64 ft²”
Critique: This is the final answer, correctly stating the area as 64 square feet.
The step is \\boxed{correct }
</think>
Is the solution correct? Yes
Figure 13: Training example from our sampled 1K synthetic chains.
17
Data generation prompt
You are given a math problem and a proposed multiple-step solution (with a step on each line):
[Math Problem]
{problem }
[Solution]
{solution }
Review and critique the proposed solution steps and determine whether each step is correct. If the solu-
tion is incomplete, only critique the steps that are provided. Your output must be in the following format:
Let’s verify step by step:
Step 1: <critique >...The step is \boxed{correct/incorrect }
Step 2: <critique >...The step is \boxed{correct/incorrect }
. . .
Step n: <critique >...The step is \boxed{correct/incorrect }
Once you find an incorrect step, you should stop since you don’t need to analyze the remaining steps.
Figure 14: Prompt used to generate synthetic verification chains as described in Section 3.2.
B LLM-as-a-Judge Limitations
We show examples of failures from QwQ-32B-Preview and Deepseek-R1-Distill-Qwen-14B when
used for LLM-as-judge verification in Figs. 16 and 18.
C Training details
C.1 Training T HINK PRM
We train our models on the filtered 1K chains using LoRA (Hu et al., 2022) with rank r= 32 and
α= 16 . Specifically, we finetune Qwen-14B-Instruct and R1-Deepseek (14B) for 3 epochs, and
QwQ-32B-Preview for 5 epochs using LoRA. We use an effective batch size of 16 and a fixed learning
rate of 4×10−4without warmup is used. Training took 1.5 hours for the 14B models and 4.5 hours
for QwQ-32B-Preview on a single A100 GPU. Without particularly found QwQ to be hard to train
with LoRA and still generates a relatively high percentage of invalid judgments after training. Full
training of the model will likely resolve these issues but that would require more compute than we
have.
The R1-Distill-Qwen {1.5B,7B }models use fulltraining with the following parameters. The 1.5B
model We trained for 3 epochs with an effective batch size of 32, using a constant learning rate of
6×10−5without decay or warmup. We train both models using four RTX A6000 GPU using data
parallel. Training the 1.5B model on the 1K chains took about half an hour and the 7B model about
two hours.
C.2 Training Discriminative Verifiers
We train R1-Qwen-14B for 1 epoch over the entire PRM800K dataset using two A100 80GB GPUs
with a batch size of 8 and a learning rate of 6×10−5. We use a constant learning rate scheduler with
no warmup. Following prior work (Wang et al., 2023b; Zhang et al., 2025) We train the model using
binary cross-entropy to maximize the probability of the tokens “+” and “-” for correct and incorrect
steps, respectively. The R1-Qwen-1.5B model is trained with the same infrastructure with a batch
size of 64 and a learning rate of 1×10−4with a warm up ratio of 10%.
18
QwQ-32B-Preview stuck in a loop
Okay, so I need to analyze each step of the proposed solution to the math problem. The problem is: If f(x) =3x−2
x−2, what is the
value of f(−2) +f(−1) +f(0)? The proposed solution is given, and I have to check if each step is correct.
First, let me read through the problem and the solution carefully.
The function is f(x) =3x−2
x−2. They want the sum of f(−2) +f(−1) +f(0).
Looking at the solution steps:
Step 0 : They plug in x=−2into the function and calculate3(−2)−2
−2−2=−8
−4= 2.
Step 1 : They mention that plugging x=−2into the denominator gives zero, which would be division by zero.
Step 2 : They generalize that we should avoid plugging in values that make the denominator zero.
Step 3 : Similarly, plugging x=−1into the denominator also gives zero.
Step 4 : Plugging x= 0 into the denominator also results in division by zero.
. . .
Wait no! The original function is given as f(x) =numerator
denominator=?
No! The problem says: If f(x) =numerator
denominator=?
No! The problem says: If f(x) =numerator
denominator=?
No! The problem says: If f(x) =numerator
denominator=?
No! The problem says: If f(x) =numerator
denominator=?
No! The problem says: If f(x) =numerator
denominator=?
No! The problem says: If f(x) =numerator
denominator=?
No! The problem says: If f(x) =numerator
denominator=?
No! The problem says: If f(x) =numerator
denominator=?
No! The problem says: If f(x) =numerator
denominator=?
...
Figure 15: Degeneration behavior from QwQ-32B-Preview with LLM-as-judge verification. Problem and
solution are omitted for brevity.
D Results on ProcessBench before and after finetuning
Table 2 shows the performance numbers of LLM-as-a-Judge and T HINK PRM on ProcessBench.
E Evaluation details
This section includes exact details on the test-time scaling shown in Section 4.2
E.1 Predicting verification labels
Following prior work (Snell et al., 2024; Beeching et al.), we aggregate scores from DiscPRM
by using the score of the laststep. For THINK PRM , we first prompt the model to generate the
verification chain up to a maximum of 8192 tokens, then we force decode the string “Is the solution
correct?” and useP(“yes”)
P(“yes”)+P(“no”)as the solution score.
E.2 Best-of-N selection
We sample solutions using a temperature of T= 0.8for Llama-3.2-3B-Instruct and T= 0.4for
Qwen-2.5-14B. We instruct all models to think step by step and put the final answer in \boxed{} . All
19
Deepseek-R1-Distill-Qwen-14B stuck in a loop
Okay, so I need to review and critique the solution step by step for the given math problem. The problem is to find the exact value of
the continued fraction expression:
1 + 1/(1 + 2/(1 + 1/(1 + 2/(1 + ...))))
The solution provided is split into 16 steps, each enclosed in a tag. I need to go through each step, analyze it, and see if there are any
errors or if the logic holds. After that, I’ll determine if the solution is correct by outputting ”yes” or ”no” in a box.
Let me start by understanding the problem. It’s a continued fraction that alternates between 1 and 2 in the numerators. The expression
is:
x = 1 + 1/(1 + 2/(1 + 1/(1 + 2/(1 + ...))))
So, the pattern repeats every two steps: 1, then 2, then 1, then 2, etc. The solution starts by denoting the expression as x and then tries
to find a relationship by substituting the inner expression back into x.
Wait, the original expression is x = 1 + 1/(1 + 2/(1 + 1/(1 + 2/(1 + ...)))). So, the part after the first 1 is 1 + 2/(1 + 1/(1 + 2/(1 + ...))).
But that part isn’t x, because x starts with 1 + 1/(...). So, maybe the substitution isn’t directly x. Let me think.
Wait, perhaps the inner part is similar but not exactly x. Let me denote the inner part as y. So, y = 1 + 2/(1 + 1/(1 + 2/(1 + ...))). But
then, the expression for y would involve another similar structure. Hmm, maybe the solution is oversimplifying here.
...
Wait, actually, when you multiply both sides by (1 + 2/x), the left side is x*(1 + 2/x) = x + 2.
The right side is [1 + 1/(1 + 2/x)]*(1 + 2/x) = 1*(1 + 2/x) + [1/(1 + 2/x)]*(1 + 2/x) = (1 + 2/x) + 1 = 2 + 2/x.
So, the correct equation after multiplying both sides by (1 + 2/x) is:
x + 2 = 2 + 2/x.
But in step 6, they have:
x + 2 = 1 + 1/(1 + 2/x) + 2/x + 4/x²
Which seems to be incorrect. They seem to have distributed the multiplication incorrectly.
Wait, let me see. If you have x*(1 + 2/x) = [1 + 1/(1 + 2/x)]*(1 + 2/x) + [2/x*(1 + 2/x)]
But that doesn’t make sense because when you multiply both sides by (1 + 2/x), you should only have:
Left side: x*(1 + 2/x)
Right side: [1 + 1/(1 + 2/x)]*(1 + 2/x)
Which simplifies to x + 2 = 2 + 2/x.
But in the solution, step 6 says:
x*(1 + 2/x) = 1 + 1/(1 + 2/x) + 2/x*(1 + 2/x)
Which seems to be adding an extra term, 2/x*(1 + 2/x), which is not correct. So, step 6 is incorrect.
Therefore, the solution has an error in step 6. Instead of correctly multiplying both sides, they incorrectly expanded the right-hand
side, leading to an incorrect equation.
Because of this mistake, the subsequent steps are likely incorrect as well. So, the entire solution is flawed from step 6 onwards.
...
Wait, let me compute the continued fraction numerically to check.
Let me compute the continued fraction up to a certain depth.
Let me compute it step by step:
Start with the innermost part and work outwards.
...
So, it seems to converge to approximately 2.414, which is 1 + sqrt(2) ≈2.4142.
Wait, but according to my earlier solving, x should be sqrt(2) ≈1.414, but the numerical computation suggests it converges to
approximately 2.414, which is 1 + sqrt(2). So, which one is correct?
Wait, perhaps I made a mistake in setting up the equation.
Wait, let’s re-examine the original expression:
...
But when I tried to solve it earlier, I set x = 1 + 1/(1 + 2/x), which led to x = sqrt(2). But the numerical computation suggests it
converges to 1 + sqrt(2). So, perhaps my initial substitution was incorrect.
Wait, perhaps I need to define it differently. Let me try to define the continued fraction properly.
Wait, let’s solve the system:
From x = 1 + 1/y and y = 1 + 2/x.
Substitute y = 1 + 2/x into x = 1 + 1/y:
x = 1 + 1/(1 + 2/x)
Multiply both sides by (1 + 2/x):
...
Alternatively, perhaps I need to define y differently. Let me try to define y as the expression starting with 1 + 1/(...):
y = 1 + 1/(1 + 2/(1 + 1/(1 + 2/(1 + ...))))
But then, y = x, which brings us back to the original substitution.
...
Which is the same equation as before, leading to x = sqrt(2). But the numerical computation suggests it’s approaching 1 + sqrt(2). So,
perhaps my initial substitution is incorrect.
Alternatively, perhaps the continued fraction is structured such that after the first 1 + 1/(...), the denominator is another continued
fraction similar to x but shifted.
...
Figure 16: Degeneration behavior from Deepseek-R1-Distill-Qwen-14B with LLM-as-judge verification. Prob-
lem and solution are omitted for brevity.
20
Problem and solution from ProcessBench
Problem: Leta, b be positive reals with a > b >1
2a. Place two squares of side lengths a, b next to each other, such that the
larger square has lower left corner at (0,0)and the smaller square has lower left corner at (a,0). Draw the line passing through
(0, a)and(a+b,0). The region in the two squares lying above the line has area 2013. If (a, b)is the unique pair maximizing
a+b, computea
b.
Solution:
We begin by finding the equation of the line through (0, a)and(a+b,0). The slope is:
m=0−a
(a+b)−0=−a
a+b
Using point-slope form from (0, a):
y=a−a
a+bx
Area above the line in the larger square:
Atx=a, the height is:
y=a−a2
a+b=ab
a+b
So the area is a trapezoid:
Area 1=1
2
a+ab
a+b
a=a3+ 2a2b
2(a+b)
Area above the line in the smaller square:
This is a triangle with heightab
a+band base b:
Area 2=1
2·ab
a+b·b=ab2
2(a+b)
Total Area:
Total =Area 1+Area 2=a3+ 2a2b+ab2
2(a+b)=a(a2+ 2ab+b2)
2(a+b)=a(a+b)2
2(a+b)=a(a+b)
2
We are given:
a(a+b)
2= 2013 ⇒a(a+b) = 4026
Express bin terms of a:
b=4026
a−a
Apply the constraints a > b >1
2a:
•a > b⇒a >4026
a−a⇒a2>2013⇒a >√
2013
•b >1
2a⇒4026
a−a >1
2a⇒a2<2684⇒a <√
2684
So√
2013 < a <√
2684 . To maximize a+b=4026
a, we minimize a, so let a=√
2013 . Then b=√
2013 , satisfying the
constraints.
a
b= 2
Figure 17: An example of problem and solution from OmniMath subset of ProcessBench.
21
Incorrect verification
Okay, so I need to review and critique the given solution step by step. Let me go through each step carefully.
Step 0: The solution starts by identifying key points and properties. It mentions that Jis the excenter opposite to A, which is correct.
A1is where the excircle touches BC, andB1, C1are where it touches the extensions of AC andAB. Then, it says A1B1is
perpendicular to AB and intersects at D.Eis the foot of the perpendicular from C1toDJ.
Hmm, this seems okay, but I wonder if they’ve correctly identified all the properties. Maybe they should have mentioned more about
the excenter’s properties, like the fact that it’s the intersection of external angle bisectors. But maybe that’s assumed knowledge.
Step 1: They note that since A1B1is perpendicular to AB,Dis the projection of A1ontoAB. So,Dlies on AB andA1Dis
perpendicular to AB.
Wait, is that necessarily true? If A1B1is perpendicular to AB, and they intersect at D, then yes, Dis the foot of the perpendicular
fromA1toAB. That seems correct.
Step 2: They examine the excircle and the triangle, mentioning that the excircle is tangent to BC atA1, and the tangents from A1to
B1andC1form right angles with the radii. Then, DJ is a line through the excenter and the intersection point of the perpendiculars
fromA1andB1toAB.
Hmm, I’m not sure about this part. The line DJ is through the excenter Jand point D, which is the foot from A1. But does
it involve perpendiculars from A1andB1? I think the solution might be conflating different properties here. The excenter is
equidistant from all sides, but DJ is just a line from DtoJ, not necessarily involving perpendiculars from both A1andB1. Maybe
this is a misstatement.
Step 3: They state that since Eis the foot of the perpendicular from C1toDJ, angle C1ED is 90 degrees. Then, they argue that
angles BEA 1andAEB 1are equal due to symmetry and properties of the cyclic quadrilateral.
Wait, why is Esymmetrically placed? The solution says it’s because of the properties of the excircle and right angles, but I’m not
entirely convinced. Just because Eis a foot of a perpendicular doesn’t automatically make it symmetric with respect to A1B1.
Maybe there’s a better reason, like cyclic quadrilaterals or similar triangles, but this step feels a bit hand-wavy.
Step 4: They conclude both angles are 45 degrees because of symmetry and right angles, forming a cyclic quadrilateral. But I’m not
sure if the cyclic quadrilateral part is clearly established. Also, 45 degrees seems specific—does the problem necessarily lead to that?
Wait, maybe I should draw a diagram to visualize. If A1B1is perpendicular to AB, andEis the foot from C1toDJ, which
is connected to the excenter. Hmm, maybe the angles are indeed 45 degrees because of the right angles and the properties of the
excentral triangle, but I’m not entirely certain.
Overall Thoughts: The solution seems to have some correct steps but might be making assumptions without sufficient justification.
For example, in Step 3, the symmetry argument isn’t fully explained. Also, in Step 2, the description of DJ might not be accurate.
The conclusion of 45 degrees might be correct, but the reasoning feels a bit rushed.
Final Answer After considering the steps, I think the solution might have some gaps in justification but arrives at the correct answer.
So, I’ll say yes, but with some reservations.
yes
Figure 18: Incorrect verification by Deepseek-R1-Distill-Qwen-14B for the problem-solution pair in Fig. 17.
ModelLLM-as-a-Judge T HINK PRM
OlympiadBench OmniMath OlympiadBench OmniMath
Random baseline 39.1 32.7 39.1 32.7
R1-Qwen-1.5B 5.0 (51.4 %) 5.4 (55.1 %) 76.3 (1.4 %) 75.7 (2.4 %)
R1-Qwen-7B 44.8 (18.2 %) 45.7 (20.9 %) 73.4 (1.1 %) 74.0 (1.4 %)
R1-Qwen-14B 72.8 (13.3 %) 67.8 (18.6 %) 87.3 (2.3 %) 85.7 (2.3 %)
QwQ-32B-preview 50.6 (7.9 %) 55.5 (10.9 %) 73.1 (15.1 %) 73.2 (7.9 %)
Table 2: Average F1-score on OlympiadBench and OmniMath subsets of ProcessBench (Zheng et al., 2024)
comparing LLM-as-a-Judge to THINK PRM finetuned on 1K examples. Random baseline for OlympiadBench is
39.1% and for OmniMath is 32.7%. Percentage of bad outputs (repetitions, invalid label formatting, overthinking,
etc.) are shown in red. LLM-as-a-judge with reasoning models suffer from issues that limits their utility as
generative verifiers.
our Best-of-N experiments use weighted majority voting (Sun et al., 2024) except for our experiments
on AIME ’24, where we use the verifier score directly to rank the solution, as we found this to
perform better for all verifiers.
E.3 Verifier-guided beam search
Under verifier-guidedbeam search, we sample candidate next steps and score them with the process
verifier, then selects top- Kout of these to further expand and so on. Our implementation is based on
(Snell et al., 2024; Beeching et al.), which maintains Nbeams in total, and samples Mcandidate
next steps per beam. We set M= 4for all experiments and run search for a maximum of 20 steps
per beam. To sample next steps, we use T= 0.6and use double newlines as the step delimiter.
22
E.4 Sequential scaling of verifier compute
We achieve budget forcing (Muennighoff et al., 2025) by triggering the model to think again for R
rounds, where each round uses a unique trigger phrase that incites the model to revisit or double-check
its earlier verification. We use different trigger phrases for each round since we found that using the
same phrase causes the model to repeat what it did in the last round.
We do a maximum of R= 4thinking rounds, and use the phrases “Let me double check”, “Let’s
verify again”, and “Did I miss something?”, for rounds 2, 3, and 4 respectively. We do not investigate
deeply into optimizing the trigger phrase, but we note that performance may depend on these and we
use the same phrases for both T HINK PRM and LLM-as-a-judge to ensure fair comparison.
E.5 MATH-500 test set
As running on all 500 examples from MATH-500 will require a lot of compute, we run all our
experiments on 100 randomly sampled subsets from MATH-500 (Hendrycks et al., 2021). We pick
the 100 problems such that they cover different difficulty levels, as shown in Fig. 19.
1 2 3 4 5
Problem Level0510152025CountDifficulty levels in the MATH-500 split we use
Figure 19: Histogram of difficulty levels in our 100-
problem subset from MATH-500.
202122232425
Number of solutions5055606570758085Accuracy (%)
Parallel scaling of verifier compute: MATH-500
ThinkPRM-14B
ThinkPRM-14B@4
ThinkPRM-14B@8DiscPRM-14B
MajorityFigure 20: Scaling of verifier compute by parallel
sampling of multiple verification CoTs and aver-
aging their scores. Parallel scaling ( THINK PRM -
14B@4 and THINK PRM -14B@8) further boosts
performance curve compared to scoring based on a
single CoT (T HINK PRM-14B).
E.6 Additional results on scaling verifier compute
Fig. 20 shows results of THINK PRM -14B when parallely scaling verifier compute by sampling
K= 4andK= 8CoTs with temperature T= 0.6and aggregating their scores. Parallel scaling
indeed lifts up the accuracy curve of THINK PRM -14B compared to standard K= 1with greedy
decoding. However, performance plateaus rather quickly and K= 8remains comparable to K= 4,
while slightly better at smaller sampling budgets. Fig. 21 compares parallel to sequential scaling
under the same token budget. While there is no clear winner, parallel scaling seems to perform
slightly better at best-of-8.
E.7 Scaling training data of T HINK PRM
Here, we show results when training THINK PRM -14B and THINK PRM -1.5B using synthetic data
from all PRM800K. The goal is to show that THINK PRM can still benefit from training on more
synthetic data. Here, we train both R1-Distill-Qwen-1.5B and R1-Distill-Qwen-14B on a total of
65K verification CoTs we obtained by sampling and filtering as explained in Section 3.2. Figs. 22
and 23 show best-of-N performance with THINK PRM -1.5B and THINK PRM -14B respectively, but
when trained on 65K and compares it to training on 1K examples—this is the version we use for
all other results in the paper. Interstingly, THINK PRM benefits from additional training, and can
further improves the accuracy curve compared to the 1K-trained version on MATH-500. We note,
23
2021222324
Number of solutions50556065707580Accuracy (%)
Parallel vs. Sequential Scaling: MATH-500
ThinkPRM-14B
ThinkPRM-14B@2ThinkPRM-14B (2 thinking rounds)
2021222324
Number of solutions50556065707580Accuracy (%)
Parallel vs. Sequential Scaling: MATH-500
ThinkPRM-14B
ThinkPRM-14B@4ThinkPRM-14B (4 thinking rounds)Figure 21: Parallel vs. sequential scaling of THINK PRM compute under the same generation budget with
Qwen-2.5-14B generator. Parallel scaling (model@K) is done by independently sampling Kverification CoTs
and aggregating their scores. Sequential scaling is done by prompting the model Ktimes to revise its own
verification for Kthinking rounds. Both setups generate up until 8192 tokens per generation. We do not observe
a clear winner although parallel scaling seems slightly better especially at larger sampling budgets.
however, that while training on more math data boosts performance on MATH-500, we observe some
performance drop on out-of-domain tasks due to the distribution shift.
21232527
Number of solutions455055606570Accuracy (%)
Generator: Llama 3.2-3B-InstructScaling training data: MATH-500
ThinkPRM-1.5B@4 (1K)
ThinkPRM-1.5B@4 (65K)DiscPRM-1.5B
RLHFFlow-8B-Deepseek
Figure 22: Best-of-N results with THINK PRM -1.5B
comparing the version trained on 1K examples (used
throughout the paper) and a version trained on 65K
examples. THINK PRM benefits from training on
more synthetic data as the performance can further
improve with more training.
202122232425
Number of solutions5055606570758085Accuracy (%)
Generator: Qwen2.5-14BScaling training data: MATH-500
ThinkPRM-14B (1K)
ThinkPRM-14B (65K)DiscPRM-14B
MajorityFigure 23: Best-of-N results with THINK PRM -
14B comparing the version trained on 1K examples
(used throughout the paper) and a version trained
on 65K examples. THINK PRM benefits from train-
ing on more synthetic data as the performance can
further improve with more training.
E.8 Verifier performance in terms of problem difficulty
We the difficulty We do not estimate the difficulty over MATH problem since each problem in MATH
is annotated based on 1 of 5 difficulty levels. For GPQA-Physics problems, we first compute the
pass@1 rate of Qwen2.5-32B-Instruct for every problem by sampling 32 solutions for each problem.
Then, we bin the pass@1 rates into four different bins and use the bin assignment for each problem
as the difficulty level. We discuss how the verifier accuracy changes with problem difficulty in
Section 5.3.
24
Prompt OlympiadBench OmniMath
QwQ-32BSingle Yes/No 50.6 (7.9 %) 55.5 (3.2 %)
Bad-step-index 65.4 (18.3 %) 69.0 (16.8 %)
Decision-for-each-step 40.1 (6.4 %) 36.6 (9.2 %)
R1-Qwen-14BSingle Yes/No 72.8 (13.3 %) 67.8 (18.6 %)
Bad step index 77.7 (17.0 %) 76.6 (17.9 %)
Decision-for-each-step 73.3 (14.6 %) 71.4 (14.7 %)
Table 3: F1-score on OlympiadBench and OmniMath subsets from ProcessBench with different prompts. Ratio
of invalid judgments is shown in red.
F More details on LLM-as-judge verification
The instruction used for LLM-as-judge verification in Section 3.1 is shown in Fig. 26, which we refer
to as Single-label. Table 3 shows performance of QwQ-32B-Preview and Qwen-Deepseek-R1-14B
with two more instructions. Bad-step-index refers to instructing the model to find the index of the
first incorrect step and then if a step is found generate a single yes/no label regarding the solution
correctness. Decision-for-each-step instructs the model to critique and produce a boxed label for each
step. This instruction is shown in Fig. 14. We observe high ha variance in verification performance
and invalid judgment ratio depending on the prompt.
We also an example from ProcessBench in Fig. 24 and the generated verification chain from QwQ-
32B-Preview in Fig. 25.
25
Example problem-solution pair from ProcessBench
Problem:
In quadrilateral ABCD ,m∠B+ m∠D= 270◦. The circumcircle of △ABD intersects CD at point E, distinct from D.
Given that BC= 4,CE= 5, andDE= 7, compute the diameter of the circumcircle of △ABD .
Solution:
Step 0: To solve this problem, we will use several properties of geometry, particularly related to circles and quadrilaterals. Let’s
break down the steps: First, we understand the given information. We have a quadrilateral ABCD where∠B+∠D= 270◦.
The circumcircle of △ABD intersects CD at point E, distinct from D. Additionally, BC= 4,CE= 5, andDE= 7.
Step 1: Next, we apply properties of cyclic quadrilaterals and circles. Since Elies on the circumcircle of △ABD ,Eis also part of
the circumcircle of □ABCD (assuming ABCD is cyclic, which will be justified later). In a cyclic quadrilateral, opposite angles
sum up to 180◦. Given ∠B+∠D= 270◦, it suggests an error in our initial understanding or misinterpretation, since angles
cannot sum to more than 360◦in a quadrilateral. However, if we consider the context, let’s assume a correction was intended for the
sake of explanation: Correct interpretation: If we adjust for logical consistency, perhaps the intention was to imply that one of the
angles at BorDis90◦(since this would fit the property of cyclic quadrilaterals better, aligning with the geometry principles
involved). For this explanation, let’s proceed with an assumption that either ∠Bor∠Dis90◦, aiming for a coherent solution path.
Step 2: We then use the Power of a Point Theorem. The Power of a Point Theorem states that for a point Poutside a circle, the
product of the lengths of the two segments from Pto the circle equals the product of the lengths of the other two segments when
considering tangents or secants from that point. However, in this case, Eis not outside but on the circumference of the circle, and we
apply it differently: Given BC= 4 andCE= 5,DE= 7, and knowing Eis part of the circumcircle, we can apply the property
of intersecting chords in a circle, which states that the product of the lengths of the segments of one chord is equal to the product
of the lengths of the segments of the other chord. Thus, in this scenario, we consider BC andCD as intersecting chords, where
CD=BC+CE= 4 + 5 = 9 .
BC·CD=CE·DE
Substituting the known values:
4·9 = 5·DE
36 = 5 ·DE
Thus,
DE=36
5= 7.2.
Step 3: This discrepancy suggests a reevaluation of the given values or a misunderstanding in their application within the context
provided. However, for the purpose of demonstrating the method:
Step 4: Finally, we find the diameter of the circumcircle of △ABD . The key to finding the diameter involves recognizing that the
circumcircle’s radius or diameter is crucial for calculating distances involving points A,B, andD. However, without specific details
about how Erelates to these points geometrically beyond being on the circle, or additional information such as the length of another
segment or angle measures, directly calculating the diameter becomes complex.
Step 5: The question’s focus on Eand its relation to BC,CE, andDE might suggest looking at the properties of chords in a
circle or applying trigonometric relationships based on the angles given initially. However, due to the discrepancy noted and the
complexity introduced by the misinterpretation of the angle condition, a direct calculation for the diameter of the circumcircle of
△ABD isn’t straightforward with the information provided.
Step 6: Given the complexities and discrepancies in the original problem statement, particularly with the angles summing to 270◦in
a quadrilateral, the solution path becomes less clear. The explanation provided aimed to creatively interpret and apply geometric
principles under the constraints of the given information. However, the direct calculation for the diameter of the circumcircle of
△ABD requires additional, precise information about the positions and relationships of the points A,B,C,D, andEwithin the
quadrilateral and its circumcircle.
Figure 24: An example problem and solution from the OlympiadBench (He et al., 2024) in ProcessBench (Zheng
et al., 2024).
26
Example from QwQ-32B-Preview
Let’s verify step by step:
Step 0: The initial step seems to correctly identify the given information in the problem. It mentions the quadrilateral ABCD
with angles at BandDsumming to 270◦, and the circumcircle of △ABD intersecting CD atE, distinct from D. The lengths
BC= 4,CE= 5, andDE= 7 are also noted. This step is accurate in summarizing the problem statement.
Step 1: This step attempts to apply properties of cyclic quadrilaterals but seems to get confused about the sum of angles in
quadrilateral ABCD . It is mentioned that in a cyclic quadrilateral, opposite angles sum to 180◦, but the given sum of angles ∠B
and∠Dis270◦, which indeed seems inconsistent. However, the step then suggests assuming that one of the angles is 90◦for the
sake of explanation, which is an adjustment to proceed with the solution. This part is a bit problematic because it alters the given
information, but perhaps there was a misinterpretation in the problem statement.
Step 2: Here, the Power of a Point Theorem is applied to points related to the circle. However, there is confusion in calculating the
length of CD. It states that CD=BC+CE= 4 + 5 = 9 , but since Eis a point on CD distinct from D, andDE is given
as7, this suggests that CD=CE+DE= 5 + 7 = 12 , not9. This is an error in calculation. Furthermore, the application
of the Power of a Point Theorem seems misapplied because Eis on the circle, not outside it, so the theorem should be applied
differently, possibly considering intersecting chords.
Step 3: Recognizing the discrepancy in the previous step, the solution acknowledges the need for reevaluation, which is a good
practice. However, without correcting the earlier mistake in calculating CD, the subsequent steps may be built on incorrect
foundations.
Step 4: This step aims to find the diameter of the circumcircle of △ABD , which is the main goal of the problem. It notes that the
diameter is related to the circumradius, but without specific details about the triangle’s sides or angles, it’s challenging to compute
directly. The solution hints at the need for more information, which is accurate.
Step 5: The insight here is that the relationship between Eand the other points might provide the necessary information through
chord properties or trigonometric relationships. However, due to the earlier discrepancies, this step does not lead to a concrete
calculation.
Step 6: The conclusion reflects on the complexities and discrepancies in the problem and the solution approach. It acknowledges that
the direct calculation for the diameter is not straightforward with the given information, which is a fair assessment.
Overall, while the solution attempt shows an understanding of relevant geometric principles, there are significant errors in calculation
and application of theorems, particularly in Step 2. These errors propagate through the subsequent steps, leading to an inconclusive
solution. Therefore, the solution is not correct.
\boxed{no}
Figure 25: Example verification chain generated by QwQ-32B-Preview in response to the problem-solution pair
in Fig. 24.
LLM-as-a-judge prompt
The following is a math problem and a solution (split into steps, enclosed with tags and indexed from
0):
[Math Problem]
{problem }
[Solution]
{prefix}
Your task is to review and critique the solution step by step. Your output must be in the following format:
Step 0: <critique >
Step 1: <critique >
. . .
Step n: <critique >
After your analysis, return yes/no depending on whether the solution is correct.
Please put your final answer (i.e., yes/no) in \boxed{}
Figure 26: Prompt used for LLM-as-a-judge verification in Section 3.1
27