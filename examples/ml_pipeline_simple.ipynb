{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff573559-aa7a-4300-bab7-fa27c442f70b",
   "metadata": {},
   "source": [
    "# SKLEARN PIPELINE OPTIMIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25e6606-ac30-48f3-851c-299c301dc807",
   "metadata": {},
   "source": [
    "### This notebook demonstrates how to use a simple ThoughtAction object to significantly improve a sklearn pipeline. This notebook serves as an introduction of how to actually use the ThoughtAction abstraction in a dedicated example. This serves as a foundation for the CoTAEngine where we build chain-of-thought-actions (CoTA) which just chains together ThoughtAction objects for complex use cases. In this notebook, we will be taking the digits dataset and training an sklearn pipeline which does PCA -> Random Forest classification and optimizing it. \n",
    "\n",
    "### Various hyperparameter optimization techniques exist (ex. Bayesian optimization, grid search, genetic algorithms etc.), but those techniques are computationally expensive and have no inherit mechanism for understanding problem context. \n",
    "\n",
    "### Specifically, consider trying to optimize hyperparameters for models trained on synthetic datasets vs. \"real world\" datasets. The classic HP optimizers above only undestand floats, they have no understanding of problem context. In this example, we will show how we can incorporate context into the optimization. We will use prompting to optimize - a single LLM API call to increase performance. We can extend this by incorporating domain expertise itself into an error signal. \n",
    "\n",
    "### LLMs often hallucinate code, this can be controlled by having them operating on fixed templates. When implemented correctly, we never have to worry about hallucinated code again. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2009225a-b6fe-45ff-a1ef-7aac4864b93e",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "### This notebook uses the getpass module - this keeps your keys secure and we can all run this notebook without exposing secrets\n",
    "### Remember, if you wish you not use Claude, you will simply need to use the OpenAIEngine OR subclass QueryEngine to use another LLM! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12e034ce-514d-4c3c-b4cc-7e46fa4686cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/boole/shannon_backup/AcceleRAG/cotarag_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import sys \n",
    "# Add the project root to Python path\n",
    "# Get the current working directory of the notebook\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(project_root)\n",
    "from contextlib import redirect_stdout\n",
    "from cotarag.cota_engine.thought_actions import LLMThoughtAction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0255ef-3059-4d9f-9f37-366f7fb21550",
   "metadata": {},
   "source": [
    "## MLPipeline as a LLMThoughtAction\n",
    "\n",
    "#### Here, we will subclass LLMThoughtAction, a familar pattern to those familiar with Pytorch where all neural nets subclass torch.nn.Module \n",
    "#### We must implement the \"thought\" method and the \"action\" method. \n",
    "#### All ThoughtActions (LLMThoughtActions subclass this base class), have a built in __call__ method which executes a (thought -> action) process. \n",
    "#### Note this pattern is not **explicitly** forced on the user, but i a convenience when building ThoughtAction chains. \n",
    "#### The key distinction between ThoughtAction and LLMThoughtAction is the former does not explicitly require an API call to an LLM while the latter expects it. It also keeps reasoning behind the code more clear. This also means we have *full* control over what parts of our pipeline will use an LLM and which will not, a limitation LangChain fails to address adequately for reasons beyond the scope of this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9130a9b2-c880-4ba2-a29d-3f8830c742c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CLAUDE_API_KEY\"] = \"<API-KEY>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a6cca35-6118-4c54-bfd5-9e846e247893",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPipeline(LLMThoughtAction):\n",
    "    def __init__(self, api_key=None, query_engine=None):\n",
    "        # Initialize parent class for LLM capabilities\n",
    "        super().__init__(api_key=api_key, query_engine=query_engine)\n",
    "        \n",
    "    def thought(self, input_data):\n",
    "        # Extract parameters from input_data (this will be a program treated as a parameterized string) \n",
    "        if not isinstance(input_data, dict):\n",
    "            raise ValueError(\"input_data must be a dictionary with parameters\")\n",
    "            \n",
    "        n_components = input_data.get('n_components')\n",
    "        num_trees = input_data.get('num_trees')\n",
    "        max_depth = input_data.get('max_depth')\n",
    "        \n",
    "        if any(x is None for x in [n_components, num_trees, max_depth]):\n",
    "            raise ValueError(\"Missing required parameters: n_components, num_trees, max_depth\")\n",
    "            # NOTE: This means that if we use LLMs to generate code itself - we can catch errors where there are missing template parameters early\n",
    "            # TIP: Have LLMs generate controlled code templates and use ThoughtActions to populate them. This contraint reduces hallucinatory outputs\n",
    "            \n",
    "        # Note that code templates are just python files as strings. We can use f = open(my_python_script.py,'r').read() \n",
    "        # The code template here as a raw string  is provided for illustration \n",
    "        # Note that the code becomes parameterized and so it changes only in predictable ways defined by the user. \n",
    "        code_template = \"\"\"\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import json\n",
    "\n",
    "# Reasoning: Load and prepare data\n",
    "data = load_digits()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reasoning: Create and execute pipeline\n",
    "scaler = StandardScaler()\n",
    "pca = PCA(n_components={n_components})\n",
    "rf = RandomForestClassifier(n_estimators={num_trees}, max_depth={max_depth}, random_state=42)\n",
    "\n",
    "# Reasoning: Fit and transform\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Reasoning: Train and predict\n",
    "rf.fit(X_train_pca, y_train)\n",
    "y_pred = rf.predict(X_test_pca)\n",
    "\n",
    "# Reasoning: Calculate metrics\n",
    "metrics = {{\n",
    "    'accuracy': accuracy_score(y_test, y_pred),\n",
    "    'precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "    'recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "    'f1': f1_score(y_test, y_pred, average='weighted'),\n",
    "    'confusion_matrix': confusion_matrix(y_test, y_pred).tolist(),\n",
    "    'feature_importance': rf.feature_importances_.tolist(),\n",
    "    'explained_variance_ratio': pca.explained_variance_ratio_.tolist(),\n",
    "    'current_params': {{\n",
    "        'n_components': {n_components},\n",
    "        'num_trees': {num_trees},\n",
    "        'max_depth': {max_depth}\n",
    "    }}\n",
    "}}\n",
    "\n",
    "# Reasoning: Store results for analysis\n",
    "result = json.dumps(metrics, indent=2)\n",
    "\"\"\"\n",
    "        # Format the template with provided arguments\n",
    "        # NOTE: This logic extends to any python program, and can be adapted to any other program executable via the subprocess module. \n",
    "        try:\n",
    "            formatted_code = code_template.format(\n",
    "                n_components=n_components,\n",
    "                num_trees=num_trees,\n",
    "                max_depth=max_depth\n",
    "            )\n",
    "        except KeyError as e:\n",
    "            raise ValueError(f\"Missing required argument: {e}\")\n",
    "            \n",
    "        return formatted_code\n",
    "        \n",
    "    def action(self, code):\n",
    "        # Execute the code and capture output\n",
    "        try:\n",
    "            # Reasoning: Create a string buffer to capture stdout\n",
    "            output_buffer = io.StringIO()\n",
    "            \n",
    "            # Reasoning: Execute code and capture output\n",
    "            with redirect_stdout(output_buffer):\n",
    "                # Reasoning: Execute in a new namespace to avoid pollution\n",
    "                namespace = {}\n",
    "                exec(code, namespace)\n",
    "                \n",
    "            # Reasoning: Get metrics as JSON string\n",
    "            metrics_str = namespace.get('result', '{}')\n",
    "            \n",
    "            # Reasoning: Analyze results using LLM\n",
    "            # NOTE: Actions can be LLM evaluations, this allows actually meta-reasoning which will be covered in a later notebook. \n",
    "            analysis_prompt = f\"\"\"Analyze the following ML pipeline results and suggest improvements:\n",
    "\n",
    "Current Results:\n",
    "{metrics_str}\n",
    "\n",
    "Please provide your analysis in two parts:\n",
    "\n",
    "PART 1 - Performance Analysis:\n",
    "1. Performance Summary:\n",
    "   - Overall accuracy and key metrics\n",
    "   - Strengths and weaknesses\n",
    "   - Class-wise performance (if applicable)\n",
    "\n",
    "2. PCA Analysis:\n",
    "   - Impact of current n_components\n",
    "   - Explained variance insights\n",
    "\n",
    "3. Random Forest Analysis:\n",
    "   - Current tree configuration effectiveness\n",
    "   - Feature importance insights\n",
    "\n",
    "4. General Recommendations:\n",
    "   - Potential model architecture changes\n",
    "   - Additional features or preprocessing to consider\n",
    "\n",
    "PART 2 - Hyperparameter Recommendations:\n",
    "Format your hyperparameter recommendations exactly as follows:\n",
    "max_depth = [suggested_value]\n",
    "num_trees = [suggested_value]\n",
    "n_components = [suggested_value]\n",
    "\n",
    "Analysis:\"\"\"\n",
    "\n",
    "            # Reasoning: Get LLM analysis\n",
    "            analysis = self.query_engine.generate_response(analysis_prompt)\n",
    "            \n",
    "            # Reasoning: Extract hyperparameter recommendations\n",
    "            hp_recommendations = {}\n",
    "            for line in analysis.split('\\n'):\n",
    "                if '=' in line:\n",
    "                    param, value = line.split('=')\n",
    "                    param = param.strip()\n",
    "                    value = value.strip()\n",
    "                    if param in ['max_depth', 'num_trees', 'n_components']:\n",
    "                        try:\n",
    "                            hp_recommendations[param] = int(value)\n",
    "                        except ValueError:\n",
    "                            hp_recommendations[param] = value\n",
    "            \n",
    "            # Reasoning: Return metrics, analysis, and structured recommendations\n",
    "            return {\n",
    "                'metrics': json.loads(metrics_str),\n",
    "                'analysis': analysis,\n",
    "                'hp_recommendations': hp_recommendations\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Reasoning: Handle execution errors\n",
    "            error_msg = f\"Error executing ML pipeline: {str(e)}\"\n",
    "            return {\n",
    "                'error': error_msg,\n",
    "                'analysis': None,\n",
    "                'hp_recommendations': None\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae699f21-dae6-40e9-b6b9-1e0dba87dc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ML Pipeline Thought-Action Demo ===\n",
      "\n",
      "1. Initializing ML Pipeline...\n",
      "\n",
      "2. Running first iteration...\n",
      "   Parameters:\n",
      "   - n_components: 10\n",
      "   - num_trees: 100\n",
      "   - max_depth: 5\n",
      "\n",
      "3. First Iteration Results:\n",
      "   Metrics:\n",
      "   - Accuracy: 0.8778\n",
      "   - F1 Score: 0.8788\n",
      "\n",
      "   Analysis:\n",
      "# Analysis of ML Pipeline Results\n",
      "\n",
      "## PART 1 - Performance Analysis\n",
      "\n",
      "### 1. Performance Summary\n",
      "\n",
      "**Overall Metrics:**\n",
      "- Accuracy: 87.78%\n",
      "- Precision: 88.31%\n",
      "- Recall: 87.78%\n",
      "- F1 Score: 87.88%\n",
      "\n",
      "**Strengths:**\n",
      "- Overall good performance with balanced precision and recall\n",
      "- Most classes show strong classification performance, particularly classes 0, 4, 5, 6, and 7\n",
      "- Model achieves above 85% on all key metrics\n",
      "\n",
      "**Weaknesses:**\n",
      "- Confusion between specific class pairs, particularly:\n",
      "  - Class 3 and Class 9 (5 instances of class 3 predicted as 9)\n",
      "  - Class 9 and Class 3 (6 instances of class 9 predicted as 3)\n",
      "  - Class 5 and Class 9 (4 instances of class 5 predicted as 9)\n",
      "  - Class 8 has relatively lower performance with misclassifications spread across classes 2, 9\n",
      "\n",
      "### 2. PCA Analysis\n",
      "\n",
      "- Current n_components: 10\n",
      "- The explained variance ratio shows these 10 components capture approximately 60.8% of total variance\n",
      "- The first component explains 12.04% variance, with subsequent components showing a gradual decrease\n",
      "- The drop-off in explained variance is relatively smooth, suggesting no clear cutoff point\n",
      "- Components beyond the 10th likely contribute minimal additional information\n",
      "\n",
      "### 3. Random Forest Analysis\n",
      "\n",
      "**Current Configuration:**\n",
      "- 100 trees with max_depth of 5\n",
      "- This moderate depth helps prevent overfitting while capturing important patterns\n",
      "\n",
      "**Feature Importance:**\n",
      "- Features 0, 1, and 2 are the most important (22.6%, 20.2%, and 19.1% respectively)\n",
      "- These three features account for over 60% of the model's predictive power\n",
      "- Features 4, 5, 8, and 9 have minimal importance (<5% each)\n",
      "\n",
      "### 4. General Recommendations\n",
      "\n",
      "- Address class confusion issues between classes 3 and 9, and 5 and 9\n",
      "- Consider feature engineering to better distinguish commonly confused classes\n",
      "- The model may benefit from class-specific data augmentation for classes with higher error rates\n",
      "- Implement ensemble methods beyond random forest (e.g., gradient boosting or voting classifiers)\n",
      "- Explore non-linear dimensionality reduction techniques like t-SNE or UMAP as alternatives to PCA\n",
      "- Consider implementing a more sophisticated model architecture for the final classifier\n",
      "\n",
      "## PART 2 - Hyperparameter Recommendations\n",
      "\n",
      "max_depth = 8\n",
      "num_trees = 150\n",
      "n_components = 12\n",
      "\n",
      "   Recommended Parameters for Next Iteration:\n",
      "   - n_components: 12\n",
      "   - num_trees: 150\n",
      "   - max_depth: 8\n",
      "\n",
      "4. Running second iteration with recommended parameters...\n",
      "\n",
      "5. Second Iteration Results:\n",
      "   Metrics:\n",
      "   - Accuracy: 0.9167\n",
      "   - F1 Score: 0.9167\n",
      "\n",
      "   Analysis:\n",
      "# ML Pipeline Analysis and Recommendations\n",
      "\n",
      "## PART 1 - Performance Analysis\n",
      "\n",
      "### 1. Performance Summary\n",
      "\n",
      "**Overall Accuracy and Key Metrics:**\n",
      "- Accuracy: 91.67%\n",
      "- Precision: 91.73%\n",
      "- Recall: 91.67%\n",
      "- F1 Score: 91.67%\n",
      "\n",
      "**Strengths:**\n",
      "- Overall performance is quite good with >91% across all metrics\n",
      "- Class 4 has perfect recall (46/46 correct)\n",
      "- Most classes show strong diagonal values in the confusion matrix\n",
      "\n",
      "**Weaknesses:**\n",
      "- Classes 3, 8, and 9 show the most misclassifications\n",
      "- There's confusion between classes 3 and 9 (bidirectional misclassifications)\n",
      "- Class 8 has lower precision with misclassifications from several other classes\n",
      "\n",
      "**Class-wise Performance:**\n",
      "- Class 4 shows perfect performance (46/46)\n",
      "- Classes 5 and 6 perform very well\n",
      "- Classes 3, 8, and 9 have the most room for improvement\n",
      "\n",
      "### 2. PCA Analysis\n",
      "\n",
      "**Impact of Current n_components:**\n",
      "- Currently using 12 components\n",
      "- The cumulative explained variance for 12 components is approximately 56.7%\n",
      "- The first few components carry significantly more information (12%, 9.7%, 8.6%)\n",
      "- The contribution diminishes for later components\n",
      "\n",
      "**Explained Variance Insights:**\n",
      "- The first 5 components explain about 41.7% of the variance\n",
      "- Additional components provide diminishing returns\n",
      "- There may be room to reduce dimensionality without significant performance loss\n",
      "\n",
      "### 3. Random Forest Analysis\n",
      "\n",
      "**Current Tree Configuration Effectiveness:**\n",
      "- 150 trees with max_depth of 8 is delivering strong performance\n",
      "- The model isn't showing obvious signs of underfitting\n",
      "- Given the high accuracy, there's no clear evidence of overfitting\n",
      "\n",
      "**Feature Importance Insights:**\n",
      "- Features 0, 1, and 2 are the most important (approximately 18.6%, 15.4%, and 15.9%)\n",
      "- Features 3 and 6 have moderate importance (9.8% and 10.8%)\n",
      "- Features 4, 5, 8, 9, 10, and 11 have relatively low importance (<5%)\n",
      "- This suggests potential for feature selection to simplify the model\n",
      "\n",
      "### 4. General Recommendations\n",
      "\n",
      "**Potential Model Architecture Changes:**\n",
      "- Consider class-specific approaches to address the confusion between classes 3, 8, and 9\n",
      "- Experiment with ensemble methods that might better handle the problematic classes\n",
      "- Test a simpler model with fewer PCA components to improve interpretability\n",
      "\n",
      "**Additional Features or Preprocessing:**\n",
      "- Apply more focused feature engineering for classes with lower performance\n",
      "- Consider oversampling for classes with higher error rates\n",
      "- Implement a feature selection step to focus on the most important features\n",
      "- Consider normalized or standardized features if not already done\n",
      "\n",
      "## PART 2 - Hyperparameter Recommendations\n",
      "\n",
      "max_depth = 10\n",
      "num_trees = 200\n",
      "n_components = 8\n",
      "\n",
      "6. Performance Comparison:\n",
      "   First Iteration Accuracy: 0.8778\n",
      "   Second Iteration Accuracy: 0.9167\n",
      "   Improvement: +3.89%\n",
      "\n",
      "=== Demo Complete ===\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Reasoning: Check for API key\n",
    "    api_key = os.environ.get(\"CLAUDE_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"CLAUDE_API_KEY environment variable not set\")\n",
    "    \n",
    "    print(\"\\n=== ML Pipeline Thought-Action Demo ===\")\n",
    "    \n",
    "    # Reasoning: Create pipeline with initial parameters\n",
    "    print(\"\\n1. Initializing ML Pipeline...\")\n",
    "    pipeline = MLPipeline(api_key=api_key)\n",
    "    \n",
    "    # Reasoning: First iteration with default parameters\n",
    "    print(\"\\n2. Running first iteration...\")\n",
    "    print(\"   Parameters:\")\n",
    "    print(\"   - n_components: 10\")\n",
    "    print(\"   - num_trees: 100\")\n",
    "    print(\"   - max_depth: 5\")\n",
    "    \n",
    "    result1 = pipeline(input_data={\n",
    "        'n_components': 10,\n",
    "        'num_trees': 100,\n",
    "        'max_depth': 5\n",
    "    })\n",
    "    # Reasoning: Print first iteration results\n",
    "    print(\"\\n3. First Iteration Results:\")\n",
    "    print(\"   Metrics:\")\n",
    "    metrics = result1['metrics']\n",
    "    print(f\"   - Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"   - F1 Score: {metrics['f1']:.4f}\")\n",
    "    \n",
    "    print(\"\\n   Analysis:\")\n",
    "    print(result1['analysis'])\n",
    "    \n",
    "    # Reasoning: Get hyperparameter recommendations\n",
    "    hp_recs = result1['hp_recommendations']\n",
    "    print(\"\\n   Recommended Parameters for Next Iteration:\")\n",
    "    print(f\"   - n_components: {hp_recs['n_components']}\")\n",
    "    print(f\"   - num_trees: {hp_recs['num_trees']}\")\n",
    "    print(f\"   - max_depth: {hp_recs['max_depth']}\")\n",
    "    \n",
    "    # Reasoning: Second iteration with recommended parameters\n",
    "    print(\"\\n4. Running second iteration with recommended parameters...\")\n",
    "    result2 = pipeline(input_data={\n",
    "        'n_components': hp_recs['n_components'],\n",
    "        'num_trees': hp_recs['num_trees'],\n",
    "        'max_depth': hp_recs['max_depth']\n",
    "    })\n",
    "    \n",
    "    # Reasoning: Print second iteration results\n",
    "    print(\"\\n5. Second Iteration Results:\")\n",
    "    print(\"   Metrics:\")\n",
    "    metrics = result2['metrics']\n",
    "    print(f\"   - Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"   - F1 Score: {metrics['f1']:.4f}\")\n",
    "    \n",
    "    print(\"\\n   Analysis:\")\n",
    "    print(result2['analysis'])\n",
    "    \n",
    "    # Reasoning: Compare iterations\n",
    "    print(\"\\n6. Performance Comparison:\")\n",
    "    print(f\"   First Iteration Accuracy: {result1['metrics']['accuracy']:.4f}\")\n",
    "    print(f\"   Second Iteration Accuracy: {result2['metrics']['accuracy']:.4f}\")\n",
    "    improvement = (result2['metrics']['accuracy'] - result1['metrics']['accuracy']) * 100\n",
    "    print(f\"   Improvement: {improvement:+.2f}%\")\n",
    "    \n",
    "    print(\"\\n=== Demo Complete ===\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e2f550-a3a7-4ffd-a1a1-f186705ffcbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
