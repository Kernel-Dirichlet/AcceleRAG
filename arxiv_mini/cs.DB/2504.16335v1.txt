MPAD: A New Dimension-Reduction Method for Preserving
Nearest Neighbors in High-Dimensional Vector Search
Jiuzhou Fu
University of Washington
jiuzhou@uw.eduDongfang Zhao
University of Washington
dzhao@cs.washington.edu
ABSTRACT
High-dimensional vector embeddings are widely used in retrieval
systems, yet dimensionality reduction (DR) is seldom applied due to
its tendency to distort nearest-neighbor (NN) structure critical for
search. Existing DR techniques such as PCA and UMAP optimize
global or manifold-preserving criteria, rather than retrieval-specific
objectives. We present MPAD â€”Maximum Pairwise Absolute Differ-
ence, an unsupervised DR method that explicitly preserves approx-
imate NN relations by maximizing the margin between ğ‘˜-NNs and
non-ğ‘˜-NNs under a soft orthogonality constraint. This design en-
ables MPAD to retain ANN-relevant geometry without supervision
or changes to the original embedding model. Experiments across
multiple domains show that MPAD consistently outperforms stan-
dard DR methods in preserving neighborhood structure, enabling
more accurate search in reduced dimensions.
1 INTRODUCTION
1.1 Background and Motivation
Vector embeddings are foundational to modern retrieval systems,
underpinning applications in language models, image retrieval,
and recommendation. These embeddings typically live in high-
dimensional spaces (e.g., 768 or 1024 dimensions), where similarity
is computed via Euclidean or cosine distance. However, operating
in such high-dimensional spaces presents several challenges: (i)
nearest-neighbor search becomes computationally expensive; (ii)
storage overhead grows linearly with dimensionality; and (iii) due to
the curse of dimensionality, distances tend to concentrate, making it
harder to distinguish truly similar items from dissimilar ones. These
issues motivate the need for effective dimensionality reduction (DR)
to compress embeddings without sacrificing retrieval performance.
While classical DR techniques like Principal Component Analy-
sis (PCA) [ 24], t-SNE [ 46], and UMAP [ 35] offer ways to project high-
dimensional data into lower dimensions, they are not designed with
nearest neighbors in mind. These methods prioritize global vari-
ance, local manifold continuity, or visualization qualityâ€”objectives
that do not align with preserving nearest-neighbor (NN) struc-
ture crucial for vector search. As a result, when used in retrieval
pipelines, they often degrade search accuracy by distorting the
fine-grained geometry around each point. This disconnect has dis-
couraged the use of DR in vector databases, despite its potential
benefits in efficiency and scalability.
This gap between the potential of dimensionality reduction and
its practical limitations in search motivates the need for neighborhood-
aware DR methods. Ideally, such a method should preserve the
relative similarity between a point and its true ğ‘˜-nearest neigh-
bors (k-NNs), even after projection into a low-dimensional space.
Moreover, the method should be compatible with precomputedembeddings, operate in an unsupervised setting, and avoid retrain-
ing or model-specific dependencies. While some supervised tech-
niques attempt to enforce neighborhood structure using labeled
data or contrastive objectives, they are not applicable in general-
purpose retrieval where labeled neighbors are unavailable. Thus,
an unsupervised, geometry-sensitive DR technique that aligns with
approximate nearest neighbor (ANN) principles remains largely
missing.
Our work addresses this need by proposing a retrieval-centric
DR method that aligns directly with the structure of ANN queries.
Rather than preserving global distance metrics, our goal is to pre-
serve relative neighborhood rankingsâ€”namely, ensuring that true
ğ‘˜-nearest neighbors remain closer than non-neighbors after pro-
jection. This shift in objective calls for a fundamentally different
formulation than traditional DR methods, and forms the core moti-
vation for our proposed solution.
1.2 Proposed Work
To bridge the gap between practical efficiency and semantic fi-
delity in multi-vector retrieval, we propose a new dimensionality
reduction framework called MPAD (Maximum Pairwise Absolute
Difference). Unlike traditional DR techniques that aim to preserve
global variance or pairwise distances, MPAD explicitly targets ap-
proximate nearest neighbor (ANN) fidelity. It formulates an unsuper-
vised optimization objective that prioritizes the relative ordering
of each vectorâ€™s true ğ‘˜-nearest neighbors over its non-neighbors
in the low-dimensional space. By encouraging margin-based sep-
aration between neighbors and distractors, MPAD ensures that
ANN-sensitive geometry is preserved during projection.
From a system design perspective, MPAD is lightweight and
deployment-friendly. It does not require access to the original em-
bedding model or labeled training data. Instead, it operates directly
on a given high-dimensional dataset and produces a projection
matrix that can be applied to new data without retraining. To avoid
collapsed or degenerate projections, MPAD incorporates a soft or-
thogonality constraint, ensuring that the projected space remains
expressive and well-structured. This makes MPAD compatible with
a broad class of pre-trained encoders and easily integrable into
existing vector search pipelines as a post-processing step.
Beyond empirical improvements, MPAD enjoys favorable geo-
metric and theoretical properties. We show that it preserves topo-
logical neighborhood information under affine transformations
such as translation and rotation. For non-uniform scaling, we intro-
duce a novel condition-number-based bound that quantifies how
distortion accumulates during projection. This allows us to formally
characterize the approximation stability of MPADâ€”a property that
is rarely analyzed in prior DR literature. Together, these propertiesarXiv:2504.16335v1  [cs.IR]  23 Apr 2025
establish MPAD not only as a practical DR tool for ANN but also
as a theoretically grounded algorithm.
1.3 Contributions
This paper makes the following key contributions:
â€¢We propose MPAD , a new unsupervised dimensionality re-
duction method designed specifically for preserving nearest-
neighbor structure in approximate vector search, contrast-
ing with traditional DR techniques that focus on global
geometry or visualization.
â€¢We provide a rigorous theoretical analysis of MPAD, includ-
ing its robustness under geometric transformations and a
novel condition-number-based bound under non-uniform
scaling.
â€¢We empirically evaluate MPAD across multiple real-world
datasets, including text and image embeddings, and show
that it consistently outperforms standard DR baselines (e.g.,
PCA, UMAP, random projections) in maintaining top- ğ‘˜
neighbor recall in the reduced space.
In the remainder of this paper, we systematically develop and
evaluate our proposed dimensionality reduction framework. Sec-
tion 2 reviews classical and modern dimensionality reduction tech-
niques, with emphasis on their limitations in preserving local neigh-
borhood structures for approximate nearest neighbor (ANN) tasks.
Section 3 introduces the Metric-Preserving Asymmetric Dimension-
ality reduction (MPAD) method, including its notation, algorithmic
formulation, theoretical foundations, and computational analysis.
In Section 4, we conduct extensive empirical evaluations of MPAD
against state-of-the-art baselines across diverse datasets, examining
both accuracy and robustness under varying parameters. Finally,
Section 5 surveys additional literature in manifold learning, high-
dimensional indexing, and neighbor-preserving reduction, situating
our work within broader research trends, and Section 6 concludes
with a discussion of future directions.
2 PRELIMINARIES OF DIMENSIONALITY
REDUCTION
Dimensionality reduction (DR) is a long-standing challenge in ma-
chine learning, statistics, and data management [ 23,28,47]. A wide
range of methods have been proposed to reduce the dimensionality
of data while attempting to preserve its structure. However, few ex-
isting DR techniques explicitly focus on preserving local neighbor-
hood information, especially in the context of approximate nearest
neighbor (ANN) search [ 3,5,22,37]. This section categorizes and
reviews key DR methods, their relationship to ANN preservation,
and related advances in neighbor-aware dimensionality reduction.
2.1 Principal Component Analysis (PCA)
[2,24] remains one of the most foundational and widely-used
linear dimensionality reduction (DR) techniques across scientific
and engineering domains. PCA seeks a set of orthogonal direc-
tionsâ€”known as principal componentsâ€”along which the data ex-
hibits maximum variance. By projecting high-dimensional data
onto a subspace spanned by the top ğ‘šprincipal components, PCA
provides a compact representation that retains as much of the dataâ€™sglobal structure as possible. Its solution is closed-form and can be
efficiently computed via singular value decomposition (SVD), mak-
ing it particularly appealing for large-scale datasets where both
accuracy and computational tractability are important.
Despite its widespread use, PCA is inherently a global method: it
optimizes for directions that explain the largest variance across the
entire dataset, without regard to local geometry or neighborhood
preservation. This characteristic makes PCA ill-suited for tasks
that rely on maintaining ğ‘˜-nearest neighbor (k-NN) relationsâ€”such
as approximate nearest neighbor (ANN) search and information
retrievalâ€”since small distances between semantically similar points
can be distorted during projection. In such contexts, preserving
local topology is often more important than capturing dominant
global trends, which PCA overlooks.
Over the years, numerous variants and generalizations of PCA
have been proposed to address its limitations or adapt it to specific
settings. For instance, Sparse PCA [53] imposes sparsity constraints
on the principal components to improve interpretability and vari-
able selection. Principal Curves [20] extend PCA by fitting smooth
one-dimensional curves that pass through the middle of the data,
capturing non-linear structure. Multilinear PCA (MPCA) [30] adapts
PCA to tensor-valued data by preserving multi-way structure. Re-
fined PCA [43] improves projection quality by iteratively updating
principal directions, and Robust PCA [9] decomposes data into
low-rank and sparse components to resist outliers. Kernel PCA,
discussed separately, is a further extension to non-linear settings
via kernel methods.
2.2 Kernel PCA
[33,42] extends the classical Principal Component Analysis (PCA)
by first mapping the input data into a high-dimensional feature
space via a non-linear kernel function, and then performing linear
PCA in that transformed space. This approach enables the discovery
of non-linear patterns and manifold structures in the original data
that standard PCAâ€”restricted to linear projectionsâ€”cannot capture.
Commonly used kernels include the Gaussian (RBF) and polynomial
kernels, which introduce flexibility in modeling complex geometric
relationships between data points.
However, this expressiveness comes at a substantial computa-
tional cost. Kernel PCA requires constructing an ğ‘Ã—ğ‘kernel
matrixâ€”where ğ‘is the number of samplesâ€”and computing its
eigenvalue decomposition, resulting in a time complexity of O(ğ‘3)
and space complexity of O(ğ‘2). These scaling limitations make
Kernel PCA impractical for large-scale or streaming datasets, partic-
ularly in applications such as approximate nearest neighbor (ANN)
search, where efficiency is critical. Moreover, the implicit mapping
complicates downstream interpretability and integration into vector
database systems that favor explicit linear transformations.
2.3 Multidimensional Scaling (MDS)
[36] is a classical dimensionality reduction technique that aims
to preserve the pairwise Euclidean distances or dissimilarities be-
tween data points when projecting them into a lower-dimensional
space. Given a distance matrix computed over the input space, MDS
seeks a configuration of points in the target space such that their
mutual distances approximate the original dissimilarities as closely
2
as possible. This makes MDS especially effective at uncovering the
global geometric structure of data manifolds, and it has been widely
used in applications such as psychometrics, bioinformatics, and
visualization.
Classical MDS is typically implemented via eigendecomposition
of the doubly centered distance matrix, which yields a set of co-
ordinates corresponding to the principal axes of variation in the
distance space. This spectral formulation ensures that MDS retains
an interpretable, globally consistent embedding. However, it also
introduces significant computational overhead: constructing and
decomposing the full pairwise distance matrix incurs O(ğ‘2)space
andO(ğ‘3)time complexity, making classical MDS unsuitable for
large-scale datasets.
To address scalability and generalization, several out-of-sample
extensions have been proposed. Regression-based methods [ 10,
45] allow new data points to be embedded in the learned low-
dimensional space without recomputing the full distance matrix.
These techniques typically fit a linear or non-linear model that maps
high-dimensional inputs to their corresponding MDS coordinates,
making it possible to apply MDS-style projections to streaming or
test-time data.
Despite its theoretical elegance and historical significance, MDS
suffers from practical limitations. Its reliance on exact distance
preservation makes it highly sensitive to noise and outliers in the
input space. Even small perturbations in distance values can sig-
nificantly distort the resulting embeddings. Furthermore, for high-
dimensional ANN or retrieval tasks, MDS does not explicitly opti-
mize for neighborhood preservation, and the emphasis on global fi-
delity can misrepresent fine-grained local relationshipsâ€”rendering
it suboptimal for similarity search applications.
2.4 Random projections
[1,48] are a class of lightweight and computationally efficient
dimensionality reduction techniques based on the celebrated John-
sonâ€“Lindenstrauss (JL) lemma [ 14,34]. The JL lemma asserts that
any set ofğ‘points in high-dimensional Euclidean space can be em-
bedded into a lower-dimensional space of dimension ğ‘‚(logğ‘/ğœ€2)
such that all pairwise distances are preserved up to a small dis-
tortion factor ğœ€. Random projection methods operationalize this
principle by multiplying the input data with a randomly gener-
ated matrix, typically populated with sub-Gaussian entries such as
standard normal or sparsified Â±1values [ 1]. The resulting transfor-
mation is linear, fast to compute, and requires no data-dependent
training, making it highly scalable to large datasets.
Despite their simplicity and theoretical guarantees, random pro-
jections suffer from several practical limitations when applied to
tasks such as approximate nearest neighbor (ANN) search. Their
data-independence implies that the projection does not adapt to the
structure or distribution of the input space, which can lead to subop-
timal preservation of fine-grained geometric relationshipsâ€”especially
in datasets with strong clustering, anisotropic features, or mani-
fold structure [ 7]. In particular, while inter-point distances may be
roughly maintained, the relative neighborhood rankingsâ€”critical
for retrieval performanceâ€”can be significantly distorted. Conse-
quently, although random projections are valuable as a baseline
or preprocessing step, they often underperform compared to moresophisticated DR methods that incorporate task-aware or geometry-
aware optimization.
3 MAXIMUM PAIRWISE ABSOLUTE
DIFFERENCE
3.1 Overview
We propose a new dimensionality reduction method called Max-
imum Pairwise Absolute Difference (MPAD), designed to preserve
local neighbor relationships in high-dimensional vector databases.
Unlike traditional techniques such as PCA, which emphasize global
variance, MPAD is tailored to maintain the local order structure
that underlies ğ‘˜-Nearest Neighbor (k-NN) queries. The method de-
fines a mapping ğ‘“:Rğ‘›â†’Rğ‘šthat seeks to maximize the smallest
pairwise differences in scalar projections while discouraging redun-
dancy among projection directions via a soft orthogonality penalty.
This balance ensures that important local geometry is preserved
without discarding meaningful directional overlap.
We will begin by formalizing domain and codomain notation
(Table 1), along with evaluation metrics based on test sets and ğ‘˜-NN
accuracy (Table 2). The core algorithm iteratively selects ğ‘špro-
jection directions by optimizing a signed objective function ğœ™that
balances informativeness and orthogonality. A key insight of MPAD
is that preserving the smallest pairwise absolute differences among
data projections helps maintain local structure in the reduced space.
We analyze the methodâ€™s intuition, algorithmic formulation, and
computational complexity in detail, and show that MPAD satisfies
desirable mathematical properties such as boundedness, continuity,
and monotonicity. These properties ensure the stability and inter-
pretability of the approach and provide a rigorous foundation for
its convergence guarantees.
To enable scalable application, we also provide a complexity
analysis of both the sequential and parallel execution of MPAD,
highlighting its suitability for large datasets under modern compu-
tational infrastructure. These features establish MPAD as a prin-
cipled, flexible, and locally structure-preserving framework for
dimensionality reduction in vector-based machine learning and
database applications.
3.2 Notation
We consider a vector database of ğ‘›-dimensional vectors, containing
ğ‘data points. Our goal is to find a mapping:
ğ‘“:Rğ‘›â†’Rğ‘š
whereğ‘‹âŠ†Rğ‘›,ğ‘“(ğ‘‹)=ğ‘‹â€²âŠ†Rğ‘š, andğ‘“is order-preserving.
We summarize the primary mathematical notations used in our
dimensionality reduction framework in Table 1. The table defines
both the domain and codomain of the transformation, the structure
of the input and output vector sets, and their respective matrix rep-
resentations. This formalism will be used consistently to describe
the projection process, algorithmic objectives, and theoretical anal-
ysis in the sections that follow.
After defining the Order-Preserving Map ğ‘“, we assess its perfor-
mance using nearest neighbor preservation metrics. Table 2 intro-
duces the notation used for evaluating the quality of the learned
projection on unseen data. Specifically, the test set ğ‘ŒâŠ‚Rğ‘›consists
ofğ‘‘high-dimensional vectors that are not part of the training set.
3
Notation Description
Domain Rğ‘›Theğ‘›-dimensional Euclidean space (in-
put space).
Codomain Rğ‘šThe target space after dimensionality
reduction, where ğ‘š<ğ‘›.
ğ‘–-th input vec-
torğ’™ğ‘–Theğ‘–-th vector in the domain, ğ’™ğ‘–âˆˆRğ‘›.
Input setğ‘‹=
{ğ’™ğ‘–}ğ‘
ğ‘–=1Collection of vectors to be reduced in
dimensionality. Can also be treated as
anğ‘›Ã—ğ‘matrix (each column is ğ’™ğ‘‡
ğ‘–).
ğ‘–-th output
vector ğ’™â€²
ğ‘–The transformed vector in the
codomain, where ğ‘“(ğ’™ğ‘–)=ğ’™â€²
ğ‘–.
Output set
ğ‘‹â€²=ğ‘“(ğ‘‹)=
{ğ’™â€²
ğ‘–}ğ‘
ğ‘–=1Collection of transformed vectors in the
codomain. Can also be represented as
anğ‘šÃ—ğ‘matrix (each column is ğ’™â€²ğ‘‡
ğ‘–).
Table 1: Notation and Definitions
These vectors are projected into the reduced space via the same map-
pingğ‘“learned from ğ‘‹, yielding the transformed test set ğ‘Œâ€²âŠ‚Rğ‘š.
Each test vector ğ’šğ‘–is used to evaluate neighborhood preservation
in the reduced space. We first compute the ğ‘˜-nearest neighbors of
ğ’šğ‘–in the original space ğ‘‹, and then compare this neighborhood
with theğ‘˜-nearest neighbors of ğ‘“(ğ’šğ‘–)in the reduced set ğ‘‹â€². This
comparison provides the basis for our quantitative evaluation of
local structure preservation under dimensionality reduction.
Notation Description
Test setğ‘Œ=
{ğ’šğ‘–}ğ‘‘
ğ‘–=1A set of test vectors in the domain Rğ‘›
of sizeğ‘‘.
Mapped test
setğ‘Œâ€²=ğ‘“(ğ‘Œ)The transformed test set in the
codomain.
ğ‘–-th test vector
ğ’šğ‘–A test vector in ğ‘Œ, for which we find its
ğ‘˜-NN inğ‘‹, then map it to ğ‘Œâ€²and find
itsğ‘˜-NN inğ‘‹â€².
Table 2: Test Set Notation
For a sufficiently large test set ğ‘Œ, theğ‘˜-NN test accuracy is
defined as:
Ağ‘š(ğ‘˜)=Ãğ‘‘
ğ‘–=1# ofğ‘˜nearest neighbors of ğ’šğ‘–in the intersection of ğ‘‹andğ‘‹â€²
ğ‘˜ğ‘‘.
â€¢ Ağ‘š(ğ‘˜)âˆˆ[ 0,1].
â€¢ Ağ‘š(ğ‘˜)=1if and only if ğ‘“is order-preserving for ğ‘˜.
â€¢Lower values indicate loss of original neighbors or intro-
duction of spurious neighbors in ğ‘‹â€².
We useAğ‘š(ğ‘˜)to evaluate the performance of ğ‘“.
3.3 Intuition
Dimensionality reduction techniques serve various objectives: some
are designed to enhance classification performance, while someothers aim to capture the most significant variations in the data
(e.g. PCA). Since any mapping from a high-dimensional space to a
lower-dimensional one must inevitably discard some information
in general, the challenge is to determine which aspects of the data
can be approximated and which must be preserved.
Motivated by the need to retain critical structural information,
we focus on preserving the local order of data pointsâ€”specifically,
theğ‘˜-Nearest Neighbor relationships under the ğ¿ğ‘norm.
Our approach begins with a fundamental question: if we are to
select a single axis (or dimension) in Rğ‘›on which to project the
input setğ‘‹, which axis should we choose? A natural answer is to
select the axis along which the data exhibits the greatest variance,
as in Principal Component Analysis (PCA), thereby maximizing
the separation between data points. However, three key limitations
of PCA in preserving local order become evident:
Information Trade-offs: Emphasizing variance may obscure
subtle but important details.
Local versus Global Structure: In many applications, main-
taining the integrity of local neighborhoods (i.e., preserving the
nearestğ‘˜neighbors) is more crucial than capturing global variance.
Emphasis on Pairwise Differences: Theğ¿ğ‘distance between
two vectors,
âˆ¥ğ’™ğ‘–âˆ’ğ’™ğ‘—âˆ¥ğ‘= ğ‘›âˆ‘ï¸
ğ‘˜=1|ğ‘¥ğ‘–ğ‘˜âˆ’ğ‘¥ğ‘—ğ‘˜|ğ‘!1
ğ‘
,
suggests that examining the dimension-wise absolute differences
can reveal a more fundamental structure than variance alone.
In light of these insights, our method selects the dimension that
maximizes the average of the smallest ğ‘%of the pairwise absolute
differences. Denote by ğ’†ğ‘˜the standard basis vectors in Rğ‘›. The
projection of a point ğ’™ğ‘–âˆˆğ‘‹onto the axis defined by ğ’†ğ‘˜is given by
ğ‘“ğ’†ğ‘˜(ğ’™ğ‘–)=(ğ’†ğ‘˜Â·ğ’™ğ‘–)ğ’†ğ‘˜.
For each pair 1â‰¤ğ‘–<ğ‘—â‰¤ğ‘, we define the projected difference as
ğ‘‘ğ‘–ğ‘—=ğ’†ğ‘˜Â·(ğ’™ğ‘–âˆ’ğ’™ğ‘—).
Letğ·ğ‘be the set of the smallest ğ‘%of these differences:
ğ·ğ‘={ğ‘‘ğ‘–ğ‘—|ğ‘‘ğ‘–ğ‘—is among the smallest ğ‘%of all pairwise differences }.
Our objective is to choose the basis vector ğ’†ğ‘˜that maximizes
maxğ’†ğ‘˜1
|ğ·ğ‘|âˆ‘ï¸
ğ‘‘ğ‘– ğ‘—âˆˆğ·ğ‘ğ‘‘ğ‘–ğ‘—.
This criterion is designed to preserve the local order structure that
is essential for accurate ğ‘˜-Nearest Neighbor results in the reduced
space.
High-dimensional datasets often exhibit characteristics that both
complicate dimensionality reduction and highlight the advantages
of our MPAD method:
Concentration of Measure: In high dimensions, pairwise dis-
tances tend to become nearly uniform, complicating the differenti-
ation between close and distant points.
Redundancy: Typically, only a few dimensions contain most of
the discriminative information, while the remainder may contribute
noise or redundancy.
Dominance of Local Structure: The intrinsic organization of
the data is frequently governed by local relationships rather than by
4
global variance, making it imperative to preserve the finer details
of the nearest neighbor topology.
3.4 Algorithm
We first propose our method here, with more rigorous mathemati-
cal analysis in the next section.
Givenğ‘›as the dimension of the domain, ğ‘šas the dimension of
the codomain, and ğ‘‹as the set of vectors whose dimensionality we
want to reduce, we want to find ğ‘švectors in Rğ‘›and projectğ‘‹onto
each of theğ‘šselected vectors. Then ğ‘‹â€²lies in the space generated
by theseğ‘švectors, i.e. Rğ‘š.
(1)Generate the first basis vector of Rğ‘š. Calculateğ‘%MPAD(ğ’˜1)=
ğœ‡ğ‘(ğ’˜1). Let ğ’˜1be a random unit vector in Rğ‘›, and letğ‘‹â€²
be the projection of ğ‘‹onto ğ’˜1(i.e.,
ğ‘“ğ’˜1(ğ’™ğ‘–)=projğ’˜1(ğ’™ğ‘–)=âŸ¨ğ’™ğ‘–,ğ’˜1âŸ©ğ’˜1,
which projects each ğ’™ğ‘–âˆˆğ‘‹onto ğ’˜1). For all 1â‰¤ğ‘–<ğ‘—â‰¤ğ‘,
define
ğ‘‘1,ğ‘–ğ‘—=ğ‘“ğ’˜1(ğ’™ğ‘–)âˆ’ğ‘“ğ’˜1(ğ’™ğ‘—).
Letğ‘âˆˆ(0,100]and define the set
ğ·1,ğ‘={ğ‘‘1,ğ‘–ğ‘—|Smallestğ‘%ofğ‘‘1,ğ‘–ğ‘—}.
We then define
ğœ‡ğ‘(ğ’˜1)=1
|ğ·1,ğ‘|âˆ‘ï¸
ğ‘‘1,ğ‘– ğ‘—âˆˆğ·1,ğ‘ğ‘‘1,ğ‘–ğ‘—.
We want to select
arg max
ğ’˜1ğœ‡ğ‘(ğ’˜1)=arg max
ğ’˜11
|ğ·1,ğ‘|âˆ‘ï¸
ğ‘‘1,ğ‘– ğ‘—âˆˆğ·1,ğ‘ğ‘‘1,ğ‘–ğ‘—
as the first basis vector of Rğ‘š.
(2)For the second basis vector ğ’˜2, we assign an orthogonality
penalty to its direction, as we wish to separate its direction
from ğ’˜1to preserve as much information as possible. Notice
that although we assign an orthogonality penalty, we do
not force ğ’˜2to be orthogonal to ğ’˜1like in PCA. This is
because we believe that in many real-life scenarios some
directions indeed convey more information than others,
and there is no reason to force them to be orthogonal. We
assign the penalty for ğ’˜2as
ğ‘ƒorth,2=ğ›¼(ğ’˜1Â·ğ’˜2)2.
The parameter ğ›¼is a penalizing factor in (0,âˆ)used to
adjust the strength of the penalty.
The process of calculating ğœ‡ğ‘(ğ’˜2)is similar to Step 1. Then
we want to find
arg max
ğ’˜2 ğœ‡ğ‘(ğ’˜2)âˆ’ğ‘ƒorth,2
=arg max
ğ’˜2Â©Â­
Â«1
|ğ·2,ğ‘|âˆ‘ï¸
ğ‘‘2,ğ‘– ğ‘—âˆˆğ·2,ğ‘ğ‘‘2,ğ‘–ğ‘—âˆ’ğ›¼(ğ’˜1Â·ğ’˜2)2ÂªÂ®
Â¬.(3) For theğ‘˜-th basis vector ğ’˜ğ‘˜, we assign the penalty
ğ‘ƒorth,ğ‘˜=ğ›¼ğ‘˜âˆ’1âˆ‘ï¸
ğ‘–=1(ğ’˜ğ‘–Â·ğ’˜ğ‘˜)2.
And we want to find
arg max
ğ’˜ğ‘˜ ğœ‡ğ‘(ğ’˜ğ‘˜)âˆ’ğ‘ƒorth,ğ‘˜
=arg max
ğ’˜ğ‘˜Â©Â­
Â«1
|ğ·ğ‘˜,ğ‘|âˆ‘ï¸
ğ‘‘ğ‘˜,ğ‘– ğ‘—âˆˆğ·ğ‘˜,ğ‘ğ‘‘ğ‘˜,ğ‘–ğ‘—âˆ’ğ›¼ğ‘˜âˆ’1âˆ‘ï¸
ğ‘–=1(ğ’˜ğ‘–Â·ğ’˜ğ‘˜)2ÂªÂ®
Â¬.
(4)Repeat until all ğ‘šbasis vectors have been selected. They
will generate the codomain Rğ‘š. Then we define ğ‘“to be the
projection map:
ğ‘“:Rğ‘›â†’Rğ‘š, ğ‘“(ğ’™)=(âŸ¨ğ’™,ğ’˜1âŸ©,âŸ¨ğ’™,ğ’˜2âŸ©,...,âŸ¨ğ’™,ğ’˜ğ‘šâŸ©),âˆ€ğ’™âˆˆRğ‘›.
In matrix terms, if we let ğ‘€ğ‘šÃ—ğ‘›be anğ‘šÃ—ğ‘›matrix whose
ğ‘–-th row is ğ’˜ğ‘‡
ğ‘–, thenğ‘“(ğ’™)=ğ‘€ğ’™, andğ‘“(ğ‘‹)=ğ‘€ğ‘‹.
3.5 Analysis
Time Complexity. We now analyze the computational complexity
of the proposed algorithm and examine opportunities for parallel
computation. Let us revisit each primary computational step in the
method and quantify its complexity. Assume:
â€¢ğ‘: number of data points (vectors).
â€¢ğ‘›: dimensionality of the domain (Input set).
â€¢ğ‘š: dimensionality of the codomain (Output set).
â€¢ğ‘‡: number of iterations required for convergence in opti-
mization.
â€¢ğ‘: fraction (percentage) of smallest pairwise distances con-
sidered.
We examine each step individually:
Generation of Pairwise Projections and Distances: For each
candidate vector ğ’˜ğ‘˜, the projection of all ğ‘vectors onto ğ’˜ğ‘˜involves
a dot product for each vector, yielding complexity ğ‘‚(ğ‘Â·ğ‘›). After
projection, computing all pairwise distances has complexity ğ‘‚(ğ‘2).
Selecting the smallest ğ‘%of these distances involves sorting, which
takesğ‘‚(ğ‘2logğ‘2)â†’ğ‘‚(ğ‘2logğ‘). Since we perform this step
for each of the ğ‘šbasis vectors, the complexity is:
ğ‘‚(ğ‘šÂ·(ğ‘Â·ğ‘›+ğ‘2logğ‘)).
Orthogonality Penalty Calculation: For theğ‘˜-th vector, calcu-
lating the orthogonality penalty with respect to previously chosen
(ğ‘˜âˆ’1)vectors involves(ğ‘˜âˆ’1)dot products, each with complexity
ğ‘‚(ğ‘›). Across allğ‘švectors, this yields complexity ğ‘‚(ğ‘š2Â·ğ‘›). Typi-
cally, sinceğ‘šâ‰ªğ‘, this complexity is negligible compared to the
previous step.
Optimization Iterations: Optimization to determine each ğ’˜ğ‘˜
is iterative. Suppose each optimization step involves a constant
number of candidate evaluations. Denoting the average iterations
per vector as ğ‘‡, the complexity multiplies by ğ‘‡:
ğ‘‚(ğ‘šÂ·ğ‘‡Â·(ğ‘Â·ğ‘›+ğ‘2logğ‘)).
Thus, the overall complexity of our algorithm is dominated by
ğ‘‚(ğ‘2logğ‘), given that we would usually expect that ğ‘â‰«ğ‘›,
ğ‘â‰«ğ‘š, andğ‘â‰«ğ‘‡, which may become computationally intensive
for large datasets.
5
Algorithm 1 MPAD
Input: Data setğ‘‹={ğ’™ğ‘–}ğ‘
ğ‘–=1âŠ‚Rğ‘›, target dimension ğ‘š, orthogo-
nality penalty factor ğ›¼, fractionğ‘âˆˆ(0,100], and number of
optimization iterations limit ğ‘‡
Output: Projection matrix ğ‘€âˆˆRğ‘šÃ—ğ‘›and mapping ğ‘“(ğ’™)=ğ‘€ğ’™
1:Initializeğ‘€â†[]
2:forğ‘˜=1toğ‘šdo
3: Initialize candidate basis vector ğ’˜ğ‘˜as a random unit vector
inRğ‘›
4:forğ‘¡=1toğ‘‡do
5: Projection: For each ğ’™ğ‘–âˆˆğ‘‹, compute scalar projection
ğ‘ğ‘–=âŸ¨ğ’™ğ‘–,ğ’˜ğ‘˜âŸ©
6: Pairwise Distances: For all 1â‰¤ğ‘–<ğ‘—â‰¤ğ‘, compute
ğ‘‘ğ‘–ğ‘—=|ğ‘ğ‘–âˆ’ğ‘ğ‘—|
7: Sort allğ‘‘ğ‘–ğ‘—and select the smallest ğ‘%to form the set ğ·ğ‘˜,ğ‘
8: Compute utility:
ğœ‡ğ‘(ğ’˜ğ‘˜)=1
|ğ·ğ‘˜,ğ‘|âˆ‘ï¸
ğ‘‘ğ‘– ğ‘—âˆˆğ·ğ‘˜,ğ‘ğ‘‘ğ‘–ğ‘—
9: ifğ‘˜>1then
10: Compute orthogonality penalty:
ğ‘ƒorth,ğ‘˜=ğ›¼ğ‘˜âˆ’1âˆ‘ï¸
ğ‘–=1(ğ’˜ğ‘–Â·ğ’˜ğ‘˜)2
11: else
12: Setğ‘ƒorth,ğ‘˜â†0
13: end if
14: Objective: Define
ğœ™(ğ’˜ğ‘˜)=ğœ‡ğ‘(ğ’˜ğ‘˜)âˆ’ğ‘ƒorth,ğ‘˜
15: Update ğ’˜ğ‘˜using an optimization method (e.g., gradient
ascent) to maximize ğœ™(ğ’˜ğ‘˜)
16: end for
17: Append the optimized ğ’˜ğ‘˜as theğ‘˜-th row ofğ‘€
18:end for
19:returnğ‘€, with mapping ğ‘“(ğ’™)=ğ‘€ğ’™
Space Complexity. The space complexity of the algorithm is primar-
ily determined by the storage required for computing and managing
the pairwise distances among the projected vectors. We analyze
the key components as follows:
Projection Storage: For each candidate basis vector ğ’˜ğ‘˜, project-
ing the input set ğ‘‹ofğ‘vectors yields an array of ğ‘scalar values.
This requires ğ‘‚(ğ‘)space.
Pairwise Distances: The most memory-intensive step involves
calculating the pairwise distances among the ğ‘projected values.
Since there are roughly ğ‘
2=ğ‘‚(ğ‘2)distinct pairs, storing these
distances requires ğ‘‚(ğ‘2)space.
Basis Vectors: The algorithm stores ğ‘šbasis vectors, each of
dimensionğ‘›. This adds an extra ğ‘‚(ğ‘šÂ·ğ‘›)space. Typically, since
ğ‘â‰«ğ‘šandğ‘›is moderate, this term is negligible compared to
ğ‘‚(ğ‘2).Thus, in the worst-case scenario, the overall space complexity
of the algorithm is dominated by the pairwise distance storage
ğ‘‚(ğ‘2). It is worth noting that if one employs streaming methods
or in-place selection techniques for determining the smallest ğ‘%
distances, the practical memory footprint can be reduced. However,
in the worst-case analysis, the space complexity remains ğ‘‚(ğ‘2).
Ideal Parallel Computation with ğ‘ƒProcessors. Assume we have ğ‘ƒ
processors available. Under the assumption of perfect load balanc-
ing, the heavy computations can be parallelized as follows:
Parallel Projection: The projection of ğ‘vectors onto a can-
didate basis vector, which requires ğ‘‚(ğ‘Â·ğ‘›)operations sequen-
tially, can be distributed over ğ‘ƒprocessors. This reduces the time
toğ‘‚
ğ‘Â·ğ‘›
ğ‘ƒ
.
Parallel Distance Calculation: Computing all ğ‘2pairwise
distances, originally costing ğ‘‚(ğ‘2)operations, can be carried out
inğ‘‚
ğ‘2
ğ‘ƒ
time when distributed evenly across ğ‘ƒprocessors.
Parallel Sorting: The time needed for sorting the ğ‘2pairwise
distances using an optimal parallel sorting algorithm is of
ğ‘‚ğ‘2
ğ‘ƒlog
ğ‘2
â†’ğ‘‚ğ‘2
ğ‘ƒlogğ‘
.
Thus, for each optimization iteration per candidate basis vector, the
dominant cost is due to sorting, requiring
ğ‘‚ğ‘2
ğ‘ƒlogğ‘
.
Ifğ‘‡iterations are needed per basis vector, the time per candidate
becomes
ğ‘‚
ğ‘‡Â·ğ‘2
ğ‘ƒlogğ‘
.
Since the selection of basis vectors is inherently sequential (due
to dependencies introduced by the orthogonality penalty), the over-
all ideal parallel time complexity of the algorithm is:
ğ‘‚
ğ‘šÂ·ğ‘‡Â·ğ‘2
ğ‘ƒlogğ‘
.
Again, we expect that ğ‘â‰«ğ‘šandğ‘â‰«ğ‘‡, so the complexity reduce
to
ğ‘‚ğ‘2
ğ‘ƒlogğ‘
.
Summary of Time Complexity. Table 3 summarizes the time com-
plexities of our proposed MPAD method under sequential and ideal
parallel conditions, alongside those of several standard dimension-
ality reduction methods.
The linear reduction in computational time under ideal paral-
lelization underscores the potential of leveraging modern parallel
computing architectures for large-scale applications.
3.6 Theoretical Properties
We will show that MPAD satisfies some desired mathematical prop-
erties and why those properties are meaningful. Notice that MPAD
is built upon the objective function, which is a signed measure, as
ğœ™ {ğ’˜ğ‘˜}ğ‘š
ğ‘˜=1=ğ‘šâˆ‘ï¸
ğ‘˜=1ï£®ï£¯ï£¯ï£¯ï£¯ï£°ğœ‡ğ‘(ğ’˜ğ‘˜)âˆ’ğ›¼ğ‘˜âˆ’1âˆ‘ï¸
ğ‘—=1
ğ’˜ğ‘—Â·ğ’˜ğ‘˜2ï£¹ï£ºï£ºï£ºï£ºï£»,
6
Method Time Complexity
MPAD (Sequential) ğ‘‚(ğ‘2logğ‘)
MPAD (Ideal Parallel, ğ‘ƒprocessors) ğ‘‚
ğ‘2
ğ‘ƒlogğ‘
PCA ğ‘‚(ğ‘Â·ğ‘›2)
Kernel PCA ğ‘‚(ğ‘3)
MDS ğ‘‚(ğ‘3)
Isomap ğ‘‚(ğ‘3)
UMAP ğ‘‚(ğ‘logğ‘)
Table 3: Comparison of time complexities for MPAD and
some mainstream DR methods.
where the utility function is defined as
ğœ‡ğ‘(ğ’˜ğ‘˜)=1
|ğ·ğ‘˜,ğ‘|âˆ‘ï¸
ğ‘‘ğ‘˜,ğ‘– ğ‘—âˆˆğ·ğ‘˜,ğ‘ğ‘‘ğ‘˜,ğ‘–ğ‘—, ğ‘‘ğ‘˜,ğ‘–ğ‘—=ğ’˜ğ‘˜Â·(ğ’™ğ‘–âˆ’ğ’™ğ‘—).
Objective Function ğœ™being a Signed Measure. Definingğœ™as a signed
measure is more than a mathematical formalityâ€”it provides a struc-
tured way to assess and decompose the quality of our basis vectors.
First, the measure ğœ™naturally splits into positive contributions, via
ğœ‡ğ‘(ğ’˜ğ‘˜)(which quantifies the preservation of local neighborhood
order), and negative contributions, via the orthogonality penalty.
This decomposition allows us to analyze how each basis vec-
tor improves or degrades the overall preservation of the ğ‘˜-NN
structure. Second, through the property of countable additivity
(detailed below), we can assess the effect of subsets of basis
vectors independently . This modularity is valuable when design-
ing algorithms, as it enables parallel optimization and local
performance analysis that aggregates to a global measure. Be-
sides, a measure-theoretic foundation ensures that every part of our
objective is derived from well-defined mathematical principles. In
this way, we avoid arbitrary choices in the design of the method,
thereby ensuring consistency and reproducibility .
Other Mathematical Properties: Boundedness, Compactness, Con-
tinuity, and Monotonicity. These mathematical properties are not
merely theoretical conveniencesâ€”they are essential to ensuring
the practical reliability of our proposed dimensionality reduction
algorithm. Boundedness guarantees that the optimization objective
remains well-defined andnumerically stable across iterations .
Compactness ensures the existence of optima , preventing diver-
gence or undefined behavior. Continuity allows gradient-based
or continuous optimization strategies to be applicable, as small
changes in the parameters yield predictable changes in the objec-
tive. Finally, monotonicity provides a formal assurance that the
algorithm will improve or maintain performance at every step,
thereby ensuring convergence . Together, these properties lay a
robust theoretical foundation for the correctness and stability of the
approach. Below we elaborate on these key properties and provide
supporting equations and reasoning.
Proofs of the above properties.
Proof of Boundedness. Each basis vector ğ’˜ğ‘˜is constrained
to lie on the unit sphere in Rğ‘›, i.e.,âˆ¥ğ’˜ğ‘˜âˆ¥=1. For any two vectorsğ’™ğ‘–,ğ’™ğ‘—âˆˆğ‘‹, by the Cauchyâ€“Schwarz inequality,
ğ’˜ğ‘˜Â·(ğ’™ğ‘–âˆ’ğ’™ğ‘—)â‰¤âˆ¥ğ’˜ğ‘˜âˆ¥Â·âˆ¥ğ’™ğ‘–âˆ’ğ’™ğ‘—âˆ¥=âˆ¥ğ’™ğ‘–âˆ’ğ’™ğ‘—âˆ¥.
Letğ·ğ‘šğ‘ğ‘¥=maxğ‘–,ğ‘—âˆ¥ğ’™ğ‘–âˆ’ğ’™ğ‘—âˆ¥.
Then, for every ğ’˜ğ‘˜, we haveğœ‡ğ‘(ğ’˜ğ‘˜)â‰¤ğ·ğ‘šğ‘ğ‘¥. Similarly, since
ğ’˜ğ‘—Â·ğ’˜ğ‘˜2
â‰¤1, the penalty term satisfies
ğ‘ƒorth,ğ‘˜=ğ›¼ğ‘˜âˆ’1âˆ‘ï¸
ğ‘—=1
ğ’˜ğ‘—Â·ğ’˜ğ‘˜2
â‰¤ğ›¼(ğ‘˜âˆ’1).
Thus, forğ‘šbasis vectors,
ğœ™ {ğ’˜ğ‘˜}ğ‘š
ğ‘˜=1â‰¤ğ‘šğ·ğ‘šğ‘ğ‘¥âˆ’ğ›¼ğ‘šâˆ‘ï¸
ğ‘˜=1(ğ‘˜âˆ’1)=ğ‘šğ·ğ‘šğ‘ğ‘¥âˆ’ğ›¼ğ‘š(ğ‘šâˆ’1)
2.
This upper bound (and a corresponding lower bound) ensures
thatğœ™is bounded. â–¡
Proof of Compactness. The set of candidate basis vectors is
the unit sphere ğ‘†ğ‘›âˆ’1, which is compact in Rğ‘›. Since continuous
functions on compact sets are bounded and attain their extrema, if
the operations defining ğœ™were entirely continuous, then ğœ™would be
guaranteed to have a maximum and minimum. Although the sorting
operation in the definition of ğœ‡ğ‘can introduce discontinuities, these
occur only at isolated points (a measure-zero set), thus preserving
the overall compact behavior of the domain. â–¡
Proof of Continuity. Consider the projection function ğ‘“ğ’˜(ğ’™)=
âŸ¨ğ’™,ğ’˜âŸ©.
This function is continuous in ğ’˜because the dot product is linear.
The absolute value function and finite sums are also continuous.
Hence, for almost every ğ’˜ğ‘˜,
ğœ‡ğ‘(ğ’˜ğ‘˜)=1
|ğ·ğ‘˜,ğ‘|âˆ‘ï¸
ğ‘‘ğ‘˜,ğ‘– ğ‘—âˆˆğ·ğ‘˜,ğ‘âŸ¨ğ’™ğ‘–âˆ’ğ’™ğ‘—,ğ’˜ğ‘˜âŸ©
is continuous except at those critical points where the order of
pairwise distances changes. The penalty term,
ğ‘ƒorth,ğ‘˜=ğ›¼ğ‘˜âˆ’1âˆ‘ï¸
ğ‘—=1
ğ’˜ğ‘—Â·ğ’˜ğ‘˜2
,
is a polynomial function in the entries of ğ’˜ğ‘˜and is therefore con-
tinuous. Overall, the function ğœ™is (almost everywhere) continuous
on the compact domain. â–¡
Proof of Monotonicity. In our algorithm, each update step
forğ’˜ğ‘˜is performed with the goal of increasing the objective
ğ½(ğ’˜ğ‘˜)=ğœ‡ğ‘(ğ’˜ğ‘˜)âˆ’ğ‘ƒorth,ğ‘˜.
Thus, at each iteration ğ‘¡, we haveğœ™(ğ‘¡+1)â‰¥ğœ™(ğ‘¡).
Sinceğœ™is bounded above (as shown under Boundedness), the
monotonic sequence {ğœ™(ğ‘¡)}must converge. This monotonicity is
crucial to guarantee algorithmic stability and convergence in prac-
tice.
â–¡
We then show that ğœ™behaves like a signed measure.
7
Proof of Signed Measure Properties. It is sufficient to prove
that:
Null Empty Set. For an empty collection of basis vectors, the
convention for empty sums gives
ğœ™(âˆ…)=âˆ‘ï¸
ğ‘˜âˆˆâˆ…ï£®ï£¯ï£¯ï£¯ï£¯ï£°ğœ‡ğ‘(ğ’˜ğ‘˜)âˆ’ğ›¼ğ‘˜âˆ’1âˆ‘ï¸
ğ‘—=1
ğ’˜ğ‘—Â·ğ’˜ğ‘˜2ï£¹ï£ºï£ºï£ºï£ºï£»=0.
Countable Additivity. Letğµ={ğ’˜ğ‘–}ğ‘–âˆˆğ¼be a countable set of
basis vectors. Suppose we partition ğµinto disjoint subsets {ğµğ‘˜}ğ‘˜âˆˆN
such that
ğµ=Ã˜
ğ‘˜âˆˆNğµğ‘˜, ğµğ‘˜âˆ©ğµâ„“=âˆ…forğ‘˜â‰ â„“.
For each subset ğµğ‘˜, define
ğœ™(ğµğ‘˜)=âˆ‘ï¸
ğ’˜âˆˆğµğ‘˜ğœ‡ğ‘(ğ’˜)âˆ’ğ›¼âˆ‘ï¸
ğ’˜,ğ’—âˆˆğµğ‘˜
ğ’—precedes ğ’˜
ğ’—Â·ğ’˜2
.
By choosing an ordering where vectors in each ğµğ‘˜appear con-
secutively, we have
ğœ™Ã˜
ğ‘˜âˆˆNğµğ‘˜
=âˆ‘ï¸
ğ‘˜âˆˆNğœ™(ğµğ‘˜).
This confirms the countable additivity property of ğœ™. â–¡
4 EVALUATION
In this section, we assess the performance of our proposed MPAD
method against several established dimensionality reduction tech-
niques on datasets in various areas.
4.1 Datasets and Setup
Dataset. We evaluate our method on four diverse datasets span-
ning different domains: Fasttext [ 8,25,26] (text), Isolet [ 12] (voice),
Arcene [ 18] (human health/mass-spectrometry), and PBMC3k [ 40]
(biology/genomics). The Fasttext dataset consists of pre-trained
word vectors; in our experiments, we use the wiki-news-300d-1M
model, which comprises one million vectors trained on Wikipedia
2017, the UMBC webbase corpus, and the statmt.org news dataset
(16B tokens). The Isolet dataset includes recordings from 150 sub-
jects, each pronouncing the name of every letter of the alphabet
twice. We use the designated training set (isolet1+2+3+4), mak-
ing it well-suited for noisy, perceptual tasks and for testing the
scaling abilities of various algorithms. The Arcene dataset, cre-
ated for the NIPS 2003 feature selection challenge, comprises mass-
spectrometry data used to distinguish between cancerous and healthy
patients; we employ its training, validation, and test splits. The
PBMC3k dataset contains single-cell RNA sequencing data from
approximately 3,000 peripheral blood mononuclear cells (PBMCs)
from a healthy human donor. It is commonly used to benchmark
and investigate immune cell types and gene expression patterns
in scRNA-seq analyses. For this dataset, we utilize the non-empty
points from the pbmc3k_processed data provided by Scanpy. Ta-
ble 4 summarizes the key characteristics of these four datasets.
Parameters. Our experiments consider four parameters. Two global
parameters affect all baseline methods: the Target Ratio (defined
as the ratio of the target dimension to the input dimension) and
the neighborhood size ğ‘˜. Throughout our experiments, we useTarget Ratios of[0.05,0.1,0.2,0.4,0.6]and neighborhood sizes of
[1,3,6,10,15]. Additionally, the MPAD method introduces two spe-
cific parameters: ğ›¼, which controls the penalty on non-orthogonality,
andğ‘, which governs the preservation of the local data manifold. We
sweepğ›¼over[1,6,12,18,25,35,50,10000]andğ‘over[60,70,80,90,100].
It is worth noting that assigning ğ›¼=10000 essentially enforce the
select projection vectors to be orthogonal. Consequently, for each
dataset, we conduct exactly 1000 accuracy tests across different
parameter combinations.
Metrics. The primary performance metric is the ğ‘˜-NN test accuracy,
denoted asAğ‘š(ğ‘˜), which quantifies how well local neighborhood
relationships are preserved in the reduced space for an external
test set. We use a different seed to randomly select 600 points (297
points for Arcene, limited by the dataset size) from each dataset
as the test set ğ‘Œ, and calculateAğ‘š(ğ‘˜)as described in the section
3.2. Additionally, we record the frequency with which each method
ranks as the best or second-best performer, serving as an auxiliary
measure of the robustness of MPAD and the baseline methods.
Baseline. We compare MPAD against several baseline dimensional-
ity reduction methods: UMAP [ 35], Isomap [ 44], Kernel PCA [ 42],
and Multidimensional Scaling (MDS) with a linear regression-based
out-of-sample extension [ 10,45]. Although Kernel PCA is more
computationally expensive than PCA, its ability to capture non-
linear structures makes it a more accurate and robust baselineâ€”hence
its inclusion instead of standard PCA. Furthermore, because MDS
does not inherently support out-of-sample dimensionality reduc-
tion, we first apply MDS to the input set and then learn the projec-
tion mapping using a linear regression model.
Environment. All experiments are implemented in Python and ex-
ecuted on CloudLab using machines running Ubuntu 22.04.2 LTS
(GNU/Linux 5.15.0-131-generic x86_64). Each node is equipped
with two AMD EPYC 7543 32-core processors running at 2.80GHz,
256GB of ECC memory (16 Ã—16GB 3200MHz DDR4), a 480GB SATA
SSD, and a 1.6TB NVMe SSD (PCIe v4.0). MPAD computations and
nearest neighbor searches are accelerated through parallel process-
ing across CPU cores.
4.2 Evaluation on Overall Effectiveness
We want to first evaluate the overall performance of the MPAD
method. We want to compare the average performance of MPAD
(using a single fixed combination of (ğ›¼,ğ‘)per dataset) against the
baseline methods for various target ratios (i.e., different reduced
dimensionalities) and different neighborhood sizes ğ‘˜. Specifically,
we choose a fixed pair of parameter values (ğ›¼,ğ‘)for MPAD for
each dataset. Then we compute the accuracy Ağ‘š(ğ‘˜)for each target
ratio andğ‘˜combination and take the average of different Ağ‘š(ğ‘˜)
to assess the average performance Ağ‘š(ğ‘˜)of different DR methods.
To assess the overall retrieval-oriented effectiveness of MPAD,
we evaluate its average performance across varying global parame-
tersâ€”specifically, target dimensionality ratios and neighborhood
sizesğ‘˜. For each dataset, we select a single (ğ›¼,ğ‘)configuration
for MPAD and compare its performance against four widely used
dimensionality reduction baselines: UMAP, Isomap, Kernel PCA
(KPCA), and MDS with linear-regression based out-of-sample pro-
jection.
8
Table 4: Dataset Characteristics
Dataset Domain Total Dimension Total Size Sample Dimension Sample Size Sampling
Fasttext Text 300 1,000,000 300 600 Random
Isolet Voice 617 7,797 200 600 Random
Arcene Health /Mass-spectrometry 10000 900 200 600 Random
PBMC3k_Processed Biology/Genomics 1838 2638 200 600 Random
Figure 1: Average ğ‘˜-NN accuracyAğ‘š(ğ‘˜)across all target ra-
tios and neighborhood sizes for each dataset. MPAD consis-
tently achieves the highest or second-highest performance
compared to baseline methods.
Figure 1 presents the averageğ‘˜-NN accuracyAğ‘š(ğ‘˜)across
combinations of ğ‘˜and target dimension ratios. The results clearly
show that MPAD consistently yields the highest or second-highest
accuracy on all four datasets. On Fasttext and Isolet, MPAD signifi-
cantly outperforms all baselines, reflecting its strong alignment with
neighborhood-preserving objectives in both text and auditory do-
mains. The selected parameter combinations (e.g., ğ›¼=50.0,ğ‘=80.0
for Fasttext) ensure robustness over a wide range of retrieval set-
tings without requiring per-instance tuning.
In the Arcene datasetâ€”representing high-dimensional biomedi-
cal dataâ€”MPAD maintains a clear lead over manifold-based meth-
ods like UMAP and Isomap, while performing on par with MDS.
Notably, MDS also shows competitive results across several datasets,
particularly Arcene and PBMC3k. This suggests that in settings lack-
ing strong nonlinear manifold structure, simpler global geometry-
preserving approaches like MDS may offer surprisingly strong
performance, likely due to their fewer assumptions and smooth
linear behavior.
The PBMC3k dataset poses a more challenging scenario due to
its sparse and noisy nature from single-cell RNA-seq data. Even in
this setting, MPAD remains competitive, closely matching MDS in
overall accuracy. KPCA, while occasionally strong (e.g., in Isolet),
suffers in noisier or less structured datasets due to its sensitivity to
kernel choice and parameterization.
Overall, MPAD achieves strong average neighborhood-preserving
accuracy in all domains tested. Its consistent performance undera fixed parameter configuration highlights its practicality in real-
world retrieval pipelines, where per-query or per-dataset tuning
is often infeasible. These findings validate our hypothesis that a
margin-based, order-preserving objective tailored for approximate
nearest-neighbor retrieval leads to better global OPDR performance
than traditional variance- or topology-driven approaches.
4.3 Evaluation on Robustness
While the previous subsection focused on MPADâ€™s average perfor-
mance using a single parameter configuration, we now investigate
how robust MPAD is across its entire parameter space. Recall that
MPAD introduces two additional hyperparameters: ğ›¼, which con-
trols the penalty for non-orthogonality, and ğ‘, which determines
the fraction of smallest pairwise distances used in the objective.
We sweepğ›¼over 8 values and ğ‘over 5 values, resulting in 40
MPAD configurations. Combined with 25 global parameter com-
binations (five target ratios and five ğ‘˜-NN sizes), we obtain a total
of 1000 experimental settings per dataset. For each setting, we fix
an MPAD configuration (ğ›¼,ğ‘), choose a target dimension ratio
and neighborhood size ğ‘˜, and compute the ğ‘˜-NN accuracyAğ‘š(ğ‘˜).
We compare the result against the four baseline methods (UMAP,
Isomap, KPCA, and MDS) and record which method achieves the
best and second-best accuracy. Repeating this process across all
parameter combinations allows us to count how often each method
ranks first or second.
Figure 2 shows the results. Each subplot corresponds to one
dataset, and each stacked bar reports the number of times a method
achieves the highest or second-highest accuracy. MPAD dominates
across most datasets, achieving the most first-place finishes in
nearly every scenario. It also consistently appears as the second-
best method in the remaining cases. In contrast, the other meth-
odsâ€”while occasionally competitiveâ€”lack this level of consistency
across the full parameter space.
These findings demonstrate that MPAD is not only effective
but also highly robust. Its performance does not hinge on finely
tuned parameter choices, which is critical in real-world deploy-
ments where exhaustive hyperparameter searches may be imprac-
tical. Even under diverse conditions, MPAD consistently preserves
nearest-neighbor structure better than existing baseline dimension-
ality reduction techniques.
4.4 Ablation Study
To further understand the behavior of MPAD, we conduct an ab-
lation study to examine how different parameter choices affect its
performance. Specifically, we isolate each parameter of interest and
systematically vary it while holding all other parameters fixed, in-
cluding the target dimension. We then record the changes in ğ‘˜-NN
9
Figure 2: Robustness of MPAD measured by the number of
times each method ranks first or second in accuracy across all
MPAD parameter combinations. MPAD dominates across all
datasets, indicating strong stability and broad effectiveness.
accuracyAğ‘š(ğ‘˜)and compare MPADâ€™s behavior against that of
baseline methods.
Figure 3 presents representative ablation results for each of the
four datasets. Each row corresponds to a dataset, and each column
isolates a different parameter. From left to right, the plots show
the effect of varying neighborhood size ğ‘˜, target dimension ratio,
the parameter ğ‘(which controls the fraction of pairwise distances
used in the MPAD objective), and the orthogonality penalty ğ›¼. For
each plot, the baseline parameter configuration is noted in the
title. Across all datasets, MPAD consistently outperforms baseline
methods across most parameter settings.
In the first column (varying ğ‘˜), MPAD demonstrates strong rela-
tive accuracy overall. It maintains high accuracy across all ğ‘˜values,
in contrast to methods like Isomap, whose performance fluctuates
significantly with different ğ‘˜. This stability suggests that one can
reasonably estimate performance at other ğ‘˜values in ANN search
tasks by evaluating just a single or only a few ğ‘˜values. Notably,
MPADâ€™s relative advantage is even more pronounced at small ğ‘˜
valuesâ€”a particularly important feature, as many real-world ANN
search tasks prioritize retrieving only the top-1 or top-3 neighbors.
Unlike some baselines that become competitive only at larger ğ‘˜,
MPAD preserves its performance edge even in these more demand-
ing settings.
In the second column (target ratio), MPAD demonstrates a clear
upward trend: its ğ‘˜-NN accuracy improves steadily as the target
dimension increases. This monotonicity is desirable in practice, as
it provides users with a stable and interpretable trade-off: increas-
ing the target dimension will consistently lead to better recall. In
contrast, UMAP sometimes experience decreasing accuracy as the
target ratio increases. This counterintuitive behavior is likely due
to overfitting or noise amplification in the manifold assumptions
used by those methods.
In the third and fourth columns, we vary MPADâ€™s internal param-
etersğ‘andğ›¼, respectively. While accuracy does fluctuate, MPADremains consistently strong across wide ranges of these values.
This suggests that MPAD is not overly sensitive to parameter tun-
ingâ€”users can obtain good performance without exhaustive search.
In particular, accuracy tends to plateau for ğ›¼beyond a moderate
threshold, indicating stability in the orthogonality penalty.
Overall, this ablation study shows that MPAD is not only ac-
curate but also interpretable and dependable. Its behavior aligns
with intuitive expectations: higher target dimensions lead to better
accuracy; small ğ‘˜-NN tasks remain tractable; and parameter ro-
bustness ensures ease of deployment. These properties distinguish
MPAD from more brittle alternatives whose performance may vary
significantly under minor parameter changes.
5 ADDITIONAL RELATED WORK
5.1 Manifold Learning and Visualization-Driven
Dimensionality Reduction
Several methods have been designed to uncover low-dimensional
manifolds embedded in high-dimensional space. Compared to classi-
cal linear methods, they are often more computationally expensive
but also more powerful in preserving topological structures and
visual coherence.
Isomap [6,44] extends MDS by preserving geodesic distances
rather than straight-line Euclidean distances. It constructs a neigh-
borhood graph and estimates pairwise geodesics using shortest path
algorithms. This allows the embedding to reflect the true geometry
of a non-linear manifold. However, its dependency on ğ‘˜-NN graph
construction and shortest-path computation renders it impractical
for large-scale datasets [4].
Locally Linear Embedding (LLE) [39] is based on the premise
that each data point and its neighbors lie on a locally linear patch
of the manifold. It reconstructs each point using a linear combina-
tion of its neighbors, and then finds a low-dimensional embedding
that preserves these reconstruction weights. Though theoretically
appealing, LLE assumes noise-free data and can exhibit instability
in the presence of sparse or high-dimensional samples[41].
t-SNE (t-distributed Stochastic Neighbor Embedding) [46]
maps high-dimensional data to a low-dimensional space by mini-
mizing the Kullbackâ€“Leibler divergence between probability distri-
butions that reflect neighborhood similarities. UMAP (Uniform
Manifold Approximation and Projection) [35] is a more recent
technique that uses a fuzzy topological structure to achieve sim-
ilar goals with better runtime scalability. Both methods excel at
visualization and cluster separation but significantly distort global
distances and are unsuitable for tasks requiring metric preservation
or nearest-neighbor retrieval.
5.2 High-Dimensional Indexing and ANN
Search
ANN search complements DR by enabling efficient retrieval in
high-dimensional settings. However, most indexing methods as-
sume fixed embeddings and do not alter dimensionality to preserve
neighbor structure.
Locality-Sensitive Hashing (LSH) [13,16,22] offers a prob-
abilistic solution to approximate nearest neighbor (ANN) search
by designing hash functions that map similar items to the same
10
Figure 3: Ablation Study. Each subplot visualizes the effect of varying one parameter (e.g., ğ‘˜, target ratio, ğ‘, orğ›¼), with all others
fixed. MPAD consistently maintains strong performance across a broad parameter range. The baseline parameter combinations
are listed for each dataset.
bucket with high probability. It provides theoretical guarantees
for sublinear query time and has been widely adopted in large-
scale vector retrieval systems. LSH is often combined with random
projections to reduce dimensionality prior to hashing, further im-
proving efficiency. Despite its practical success, LSH has notable
limitations: its hashing mechanism introduces quantization effects
that can distort fine-grained geometric relationships, making it
difficult to preserve nuanced similarity rankings. More importantly,
LSH operates in a data-independent manner and does not alter the
underlying embedding space to enhance neighborhood preserva-
tion. As a result, while effective for coarse retrieval tasks, it lacks
the ability to adaptively reshape embeddings to better retain ğ‘˜-
NN structures in lower dimensions [ 31,52], limiting its utility in
scenarios demanding high-fidelity local ordering.
Graph-based indexing methods, such as Hierarchical Naviga-
ble Small World (HNSW)[ 32] and Navigating Spreading-out Graph
(NSG)[ 15], construct sparse proximity graphs where each node links
to its approximate neighbors. These methods enable extremely fast
ANN retrieval with logarithmic search time. However, they operateon fixed input embeddings and do not inherently perform dimen-
sionality reduction, leaving the preservation of neighbor structure
to the quality of the input vectors.
Some recent VLDB surveys [ 38,50] emphasize the trade-offs
among query speed, accuracy, and memory footprint in ANN sys-
tems. Emerging research [ 51,52,54] has explored index struc-
ture refinements for efficiency, yet they stop short of integrating
dimensionality-aware neighborhood preservation strategies.
5.3 Neighbor- and Order-Preserving
Dimensionality Reduction
Efforts to directly preserve nearest-neighbor or similarity order
under DR are more recent and limited.
Order-preserving DR (OPDR) Evaluation [17] introduces a
framework to analyze how DR affects similarity ranking among
data points. By examining the functional relationship between tar-
get dimensionality and factors like variance retention or neighbor-
hood distortion, OPDR enables systematic evaluation and design
of dimensionality reduction techniques that better preserve neigh-
borhood ordering.
11
Similarity Order Preserving Discriminant Analysis (SOPDA) [21]
is a supervised DR method that uses class label information to en-
force a similarity hierarchy: samples within the same class are
embedded closer together in a manner that reflects their origi-
nal similarity rankings. Although effective for classification tasks,
SOPDAâ€™s reliance on labeled data limits its applicability in unsu-
pervised contexts like general-purpose ANN search.
Order-Preserving Hashing [49] addresses the preservation of
similarity rankings in the binary hash space. It aims to maintain rela-
tive distances in the original space after hashing, facilitating ranked
retrieval in hash-based indexing systems. Nevertheless, it outputs
discrete binary codes and cannot be directly applied in scenarios
requiring real-valued embeddings or differentiable mappings.
Earlier methods such as dominance analysis in biomedical
statistics [29] implicitly used order-preserving principles to ana-
lyze time-series curves. However, they were domain-specific and
not intended as general DR techniques.
Recent contrastive learning approaches [11,19] learn em-
beddings that maintain semantic or local structure by contrast-
ing positive and negative sample pairs. These methods often yield
neighborhood-preserving features, albeit indirectly and at a high
computational cost. Moreover, they require supervised signals or
complex augmentations, posing challenges for large-scale, unsu-
pervised retrieval tasks.
Graph sparsification techniques [27] and all-rangeğ‘˜-NN
graphs [54] complement dimensionality reduction (DR) by focus-
ing on compact yet accurate representations of neighborhood struc-
ture in graph form. These methods aim to reduce redundancy in
dense similarity graphs, enabling faster search and lower mem-
ory usage while preserving essential connectivity. In particular,
all-rangeğ‘˜-NN graphs extend conventional ğ‘˜-NN graphs by re-
taining top neighbors across a wide range of distances, improving
global navigation without losing local fidelity. However, these tech-
niques operate exclusively within the discrete graph domain and
are typically applied post hoc to fixed vector embeddings. They
do not modify or learn new low-dimensional representations of
the input data, and thus cannot replace DR methods that seek to
reshape continuous vector spaces in a way that preserves approxi-
mate nearest neighbor (ANN) relationships. As such, while valu-
able for graph-based indexing and retrieval, they are orthogonal to
embedding-based dimensionality reduction approaches.
6 CONCLUSION AND FUTURE WORK
We presented MPAD, an unsupervised dimension-reduction frame-
work that explicitly preserves local neighborhood structure for
nearest-neighbor retrieval. Unlike conventional DR methods such
as PCA and UMAPâ€”which prioritize global variance or manifold
embeddingâ€”MPAD promotes margin-based separation of true ğ‘˜-
NNs from non-neighbors, ensuring that fine-grained geometric
relationships remain intact. This design aligns with the practical
demands of high-dimensional vector search, where preserving top-
ğ‘˜neighbor consistency is often more critical than capturing broad
data variance. Through extensive evaluations on diverse real-world
datasets (text, speech, human health, and biological gene-expression
profiles), MPAD demonstrates consistent gains in ğ‘˜-NN recall whencompared to standard DR techniques. In addition, our ablation stud-
ies show that MPADâ€™s performance is robust to moderate variations
in its primary parameters, enabling smooth deployment across dif-
ferent domains without elaborate tuning. Moreover, the algorithmâ€™s
soft orthogonality constraint and margin-based formulation help
it avoid degenerate projections while still achieving strong reduc-
tion ratios. Together, these results affirm MPADâ€™s effectiveness for
retrieval-centric applications and highlight its ability to preserve se-
mantically relevant neighborhoods in reduced-dimensional spaces.
Potential directions for future work include investigating stochastic
variants of MPAD tailored to very large datasets, exploring adaptive
parameter schedules to further automate the projection process,
and integrating MPAD into existing approximate nearest neighbor
search pipelines for even more efficient retrieval.
REFERENCES
[1]Dimitris Achlioptas. 2003. Database-friendly random projections: John-
sonâ€“Lindenstrauss with binary coins. J. Comput. System Sci. 66, 4 (2003), 671â€“687.
[2] Karl Pearson and. 1901. LIII. On lines and planes of closest fit to systems of points
in space. The London, Edinburgh, and Dublin Philosophical Magazine and Journal
of Science 2, 11 (1901), 559â€“572. https://doi.org/10.1080/14786440109462720
arXiv:https://doi.org/10.1080/14786440109462720
[3] Alexandr Andoni, Piotr Indyk, and Ilya Razenshteyn. 2018. Approximate Nearest
Neighbor Search in High Dimensions. arXiv preprint arXiv:1806.09823 (2018).
[4]Farzana Anowar, Samira Sadaoui, and Bassant Selim. 2021. Conceptual and
empirical comparison of dimensionality reduction algorithms (pca, kpca, lda,
mds, svd, lle, isomap, le, ica, t-sne). Computer Science Review 40 (2021), 100378.
[5]Martin AumÃ¼ller, Erik Bernhardsson, and Alexander Faithfull. 2020. ANN-
Benchmarks: A Benchmarking Tool for Approximate Nearest Neighbor Algo-
rithms. Information Systems 87 (2020), 101374.
[6]Mukund Balasubramanian and Eric L. Schwartz. 2002. The
Isomap Algorithm and Topological Stability. Science 295,
5552 (2002), 7â€“7. https://doi.org/10.1126/science.295.5552.7a
arXiv:https://www.science.org/doi/pdf/10.1126/science.295.5552.7a
[7] Ella Bingham and Heikki Mannila. 2001. Random projection in dimensionality
reduction: applications to image and text data. In Proceedings of the seventh
ACM SIGKDD international conference on Knowledge discovery and data mining .
245â€“250.
[8] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2016. En-
riching Word Vectors with Subword Information. arXiv preprint arXiv:1607.04606
(2016).
[9] Emmanuel J CandÃ¨s, Xiaodong Li, Yi Ma, and John Wright. 2011. Robust principal
component analysis? Journal of the ACM (JACM) 58, 3 (2011), 1â€“37.
[10] Huazhou Chen, Qi-Qing Song, Kai Shi, and Zhen Jia. 2015. [Multidimensional
Scaling Linear Regression Applied to FTIR Spectral Quantitative Analysis of
Clinical Parameters of Human Blood Serum]. Guang pu xue yu guang pu fen xi =
Guang pu 35 (04 2015), 914â€“8. https://doi.org/10.3964/j.issn.1000-0593(2015)04-
0914-05
[11] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020.
A Simple Framework for Contrastive Learning of Visual Representations. In
Proceedings of the 37th International Conference on Machine Learning (ICML) .
1597â€“1607.
[12] Ron Cole and Mark Fanty. 1991. ISOLET. UCI Machine Learning Repository.
DOI: https://doi.org/10.24432/C51G69.
[13] Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab Mirrokni. 2004. Locality-
Sensitive Hashing Scheme Based on p-Stable Distributions. In Proceedings of the
20th Annual Symposium on Computational Geometry (SoCG) . 253â€“262.
[14] Peter Frankl and Hiroshi Maehara. 1988. The Johnson-Lindenstrauss lemma and
the sphericity of some graphs. Journal of Combinatorial Theory, Series B 44, 3
(1988), 355â€“362.
[15] Cong Fu, Changxu Wang, and Deng Cai. 2017. Fast Approximate Nearest Neigh-
bor Search With Navigating Spreading-out Graphs. CoRR abs/1707.00143 (2017).
arXiv:1707.00143 http://arxiv.org/abs/1707.00143
[16] Aristides Gionis, Piotr Indyk, and Rajeev Motwani. 1999. Similarity Search in
High Dimensions via Hashing. In Proceedings of the 25th International Conference
on Very Large Data Bases (VLDB) . 518â€“529.
[17] Chengyu Gong, Gefei Shen, Luanzheng Guo, Nathan Tallent, and Dongfang Zhao.
2024. OPDR: Order-Preserving Dimension Reduction for Semantic Embedding
of Multimodal Scientific Data. arXiv:2408.10264 [cs.LG] https://arxiv.org/abs/
2408.10264
[18] Isabelle Guyon, Steve Gunn, Asa Ben-Hur, and Gideon Dror. 2004. Arcene. UCI
Machine Learning Repository. DOI: https://doi.org/10.24432/C58P55.
12
[19] Raia Hadsell, Sumit Chopra, and Yann LeCun. 2006. Dimensionality Reduction
by Learning an Invariant Mapping. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) . 1735â€“1742.
[20] Trevor Hastie and Werner Stuetzle and. 1989. Principal Curves. J. Amer. Statist.
Assoc. 84, 406 (1989), 502â€“516. https://doi.org/10.1080/01621459.1989.10478797
arXiv:https://www.tandfonline.com/doi/pdf/10.1080/01621459.1989.10478797
[21] HaoShuang Hu, Da-Zheng Feng, and Qing-Yan Chen. 2021. A novel dimension-
ality reduction method: Similarity order preserving discriminant analysis. Signal
Processing 182 (2021), 107933. https://doi.org/10.1016/j.sigpro.2020.107933
[22] Piotr Indyk and Rajeev Motwani. 1998. Approximate Nearest Neighbors: To-
wards Removing the Curse of Dimensionality. In Proceedings of the 30th ACM
Symposium on Theory of Computing (STOC) . 604â€“613.
[23] Weikuan Jia, Meili Sun, Jian Lian, and Sujuan Hou. 2022. Feature dimensionality
reduction: a review. Complex & Intelligent Systems 8, 3 (2022), 2663â€“2693.
[24] Ian Jolliffe. 2002. Principal Component Analysis . Springer.
[25] Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, HÃ©rve JÃ©gou,
and Tomas Mikolov. 2016. FastText.zip: Compressing text classification models.
arXiv preprint arXiv:1612.03651 (2016).
[26] Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2016. Bag
of Tricks for Efficient Text Classification. arXiv preprint arXiv:1607.01759 (2016).
[27] Abd Errahmane Kiouche, Julien Baste, Mohammed Haddad, Hamida Seba, and
Angela Bonifati. 2024. Neighborhood-Preserving Graph Sparsification. Proc.
VLDB Endow. 17, 13 (2024), 4853â€“4866. https://www.vldb.org/pvldb/vol17/p4853-
seba.pdf
[28] Oliver Kramer. 2013. K-Nearest Neighbors . Springer Berlin Heidelberg, Berlin,
Heidelberg, 13â€“23. https://doi.org/10.1007/978-3-642-38652-7_2
[29] Sang Han Lee, Johan Lim, Marina Vannucci, Eva Petkova, Maurice Preter,
and Donald F. Klein. 2008. Order-Preserving Dimension Reduction Procedure
for the Dominance of Two Mean Curves with Application to Tidal Volume
Curves. Biometrics 64, 3 (01 2008), 931â€“939. https://doi.org/10.1111/j.1541-
0420.2007.00959.x arXiv:https://academic.oup.com/biometrics/article-
pdf/64/3/931/52729767/biometrics_64_3_931.pdf
[30] Haiping Lu, Konstantinos N Plataniotis, and Anastasios N Venetsanopoulos.
2008. MPCA: Multilinear principal component analysis of tensor objects. IEEE
transactions on Neural Networks 19, 1 (2008), 18â€“39.
[31] Qin Lv, William Josephson, Zhe Wang, Moses Charikar, and Kai Li. 2007. Multi-
Probe LSH: Efficient Indexing for High-Dimensional Similarity Search. In Pro-
ceedings of the 33rd International Conference on Very Large Data Bases (VLDB) .
950â€“961.
[32] Yu A Malkov and D A Yashunin. 2018. Efficient and robust approximate near-
est neighbor search using Hierarchical Navigable Small World graphs. IEEE
transactions on pattern analysis and machine intelligence 42, 4 (2018), 824â€“836.
[33] Sanparith Marukatat. 2023. Tutorial on PCA and approximate PCA and approxi-
mate kernel PCA. Artificial Intelligence Review 56, 6 (2023), 5445â€“5477.
[34] JiÅ™Ã­ MatouÅ¡ek. 2008. On variants of the Johnsonâ€“Lindenstrauss lemma. Random
Structures & Algorithms 33, 2 (2008), 142â€“156.
[35] Leland McInnes, John Healy, and James Melville. 2018. UMAP: Uniform man-
ifold approximation and projection for dimension reduction. arXiv preprint
arXiv:1802.03426 (2018).
[36] A. Mead. 1992. Review of the Development of Multidimensional Scaling Methods.
Journal of the Royal Statistical Society. Series D (The Statistician) 41, 1 (1992), 27â€“39.
http://www.jstor.org/stable/2348634
[37] Marius Muja and David G. Lowe. 2014. Scalable Nearest Neighbor Algorithms
for High Dimensional Data. IEEE Transactions on Pattern Analysis and Machine
Intelligence 36, 11 (2014), 2227â€“2240.
[38] Jianbin Qin, Wei Wang, Chuan Xiao, and Ying Zhang. 2020. Similarity Query
Processing for High-Dimensional Data. Proc. VLDB Endow. 13, 12 (2020), 3437â€“
3440. https://doi.org/10.14778/3415478.3415564
[39] Sam T. Roweis and Lawrence K. Saul. 2000. Nonlinear Dimension-
ality Reduction by Locally Linear Embedding. Science 290, 5500
(2000), 2323â€“2326. https://doi.org/10.1126/science.290.5500.2323
arXiv:https://www.science.org/doi/pdf/10.1126/science.290.5500.2323
[40] Rahul Satija, Jeffrey A Farrell, David Gennert, Alexander F Schier, and Aviv
Regev. 2015. Spatial reconstruction of single-cell gene expression data. Nature
Biotechnology 33, 5 (2015), 495â€“502.
[41] Lawrence K Saul and Sam T Roweis. 2000. An introduction to locally
linear embedding. unpublished. Available at: http://www. cs. toronto. edu/Ëœ
roweis/lle/publications. html (2000).
[42] Bernhard SchÃ¶lkopf, Alexander Smola, and Klaus-Robert MÃ¼ller.
1998. Nonlinear Component Analysis as a Kernel Eigenvalue
Problem. Neural Computation 10, 5 (07 1998), 1299â€“1319. https:
//doi.org/10.1162/089976698300017467 arXiv:https://direct.mit.edu/neco/article-
pdf/10/5/1299/813905/089976698300017467.pdf
[43] Kirill Simonov, Fedor Fomin, Petr Golovach, and Fahad Panolan. 2019. Re-
fined Complexity of PCA with Outliers. In Proceedings of the 36th International
Conference on Machine Learning (Proceedings of Machine Learning Research) ,
Kamalika Chaudhuri and Ruslan Salakhutdinov (Eds.), Vol. 97. PMLR, 5818â€“5826.
https://proceedings.mlr.press/v97/simonov19a.html[44] Joshua B. Tenenbaum, Vin de Silva, and John C. Langford. 2000. A Global
Geometric Framework for Nonlinear Dimensionality Reduction. Science
290, 5500 (2000), 2319â€“2323. https://doi.org/10.1126/science.290.5500.2319
arXiv:https://www.science.org/doi/pdf/10.1126/science.290.5500.2319
[45] Michael W. Trosset and Carey E. Priebe. 2008. The out-of-sample problem for
classical multidimensional scaling. Computational Statistics & Data Analysis 52,
10 (2008), 4635â€“4642. https://doi.org/10.1016/j.csda.2008.02.031
[46] Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.
Journal of machine learning research 9, Nov (2008), 2579â€“2605.
[47] Laurens Van Der Maaten, Eric O Postma, H Jaap Van Den Herik, et al .2009.
Dimensionality reduction: A comparative review. Journal of machine learning
research 10, 66-71 (2009), 13.
[48] Santosh S Vempala. 2005. The random projection method . Vol. 65. American
Mathematical Soc.
[49] Jianfeng Wang, Jingdong Wang, Nenghai Yu, and Shipeng Li. 2013. Order
preserving hashing for approximate nearest neighbor search. In Proceedings
of the 21st ACM International Conference on Multimedia (Barcelona, Spain)
(MM â€™13) . Association for Computing Machinery, New York, NY, USA, 133â€“142.
https://doi.org/10.1145/2502081.2502100
[50] Mengzhao Wang, Xiaoliang Xu, Qiang Yue, and Yuxiang Wang. 2021. A Com-
prehensive Survey and Experimental Comparison of Graph-Based Approxi-
mate Nearest Neighbor Search. Proc. VLDB Endow. 14, 11 (2021), 1964â€“1978.
https://doi.org/10.14778/3476249.3476255
[51] Xi Zhao, Yao Tian, Kai Huang, Bolong Zheng, and Xiaofang Zhou. 2023. Towards
Efficient Index Construction and Approximate Nearest Neighbor Search in High-
Dimensional Spaces. Proc. VLDB Endow. 16, 8 (2023), 1979â€“1991. https://doi.org/
10.14778/3594512.3594527
[52] Bolong Zheng, Xi Zhao, Lianggui Weng, Nguyen Quoc Viet Hung, Hang Liu,
and Christian S. Jensen. 2020. PM-LSH: A Fast and Accurate LSH Framework
for High-Dimensional Approximate NN Search. Proc. VLDB Endow. 13, 5 (2020),
643â€“655. https://doi.org/10.14778/3377369.3377374
[53] Hui Zou, Trevor Hastie, and Robert Tibshirani. 2006. Sparse principal component
analysis. Journal of computational and graphical statistics 15, 2 (2006), 265â€“286.
[54] Chaoji Zuo and Dong Deng. 2023. ARKGraph: All-Range Approximate K-Nearest-
Neighbor Graph. Proc. VLDB Endow. 16, 10 (2023), 2645â€“2658. https://doi.org/10.
14778/3603581.3603601
13