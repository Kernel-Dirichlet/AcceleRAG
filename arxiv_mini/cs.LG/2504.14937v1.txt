Causal DAG Summarization (Full Version)
Anna Zeng
annazeng@mit.edu
CSAIL, MIT
USAMichael Cafarella
michjc@csail.mit.edu
CSAIL, MIT
USABatya Kenig
batyak@technion.ac.il
Technion
Israel
Markos Markakis
markakis@mit.edu
CSAIL, MIT
USABrit Youngmann
brity@technion.ac.il
Technion
IsraelBabak Salimi
bsalimi@ucsd.edu
University of California, San Diego
USA
ABSTRACT
Causal inference aids researchers in discovering cause-and-effect
relationships, leading to scientific insights. Accurate causal esti-
mation requires identifying confounding variables to avoid false
discoveries. Pearlâ€™s causal model uses causal DAGs to identify con-
founding variables, but incorrect DAGs can lead to unreliable causal
conclusions. However, for high dimensional data, the causal DAGs
are often complex beyond human verifiability. Graph summariza-
tion is a logical next step, but current methods for general-purpose
graph summarization are inadequate for causal DAG summariza-
tion. This paper addresses these challenges by proposing a causal
graph summarization objective that balances graph simplification
for better understanding while retaining essential causal informa-
tion for reliable inference. We develop an efficient greedy algorithm
and show that summary causal DAGs can be directly used for in-
ference and are more robust to misspecification of assumptions,
enhancing robustness for causal inference. Experimenting with
six real-life datasets, we compared our algorithm to three existing
solutions, showing its effectiveness in handling high-dimensional
data and its ability to generate summary DAGs that ensure both
reliable causal inference and robustness against misspecifications.
1 INTRODUCTION
Causal inference is central to informed decision-making in econom-
ics, sociology, medicine, and in helping analysts unravel complex
cause-effect relationships [ 30,43,105]. It has become increasingly
critical in machine learning, where it supports algorithmic fair-
ness [ 90], data debiasing [ 117,117,118], explainable AI [ 11,28,
64,65], and enhanced robustness [ 53,91,102]. Causal inference
has also become a major theme in recent data management re-
search [ 13,59,61,84,89], integrating causality into data manage-
ment tasks such as finding input responsibilities toward query an-
swers [ 59â€“61], explaining for query results [ 85,88,111,112], data
discovery [ 27,113], data cleaning [ 79,90], hypothetical reasoning
[26], and large system diagnostics [7, 29, 36, 55, 56].
Drawing causal conclusions from data fundamentally hinges on
access to background knowledge and assumptions, as data alone
cannot establish causality [ 74,87]. A principled way to encode
such background knowledge is through Causal Directed Acyclic
Graphs (DAGs) [ 74]. These graphs explicitly represent assumed
causal relationships, enabling systematic reasoning about interven-
tions. Causal DAGs can be used together with graphical criteria
such as the backdoor criterion, or in general, Pearlâ€™s ğ‘‘ğ‘œ-calculus [ 74]
to determine whether the effect of interventions can be answeredusing data and available background knowledge. If so, they help
identify the right set of confounding variables to control for, ensur-
ing sound causal inference given the background knowledge.
However, the soundness and robustness of causal inference
hinges on the availability of high-quality causal DAGs, which are
often not readily available. These DAGs are typically constructed
using domain knowledge [ 16,57,106] or through causal discov-
ery methods [ 20,35,95,108,119]. This elicitation process is costly,
error-prone [69], and time-consuming. Causal discovery methods,
while useful, are fundamentally restrictive as they identify a class
of DAGs compatible with observed data rather than a singular, de-
finitive model [ 35]. Moreover, existing discovery methods often do
not perform well on real-world data and require significant human
intervention for verification [ 21,40,98]. The problem is even worse
for high-dimensional data, increasing the need for efficient meth-
ods to simplify and verify causal models while retaining essential
information [71]. We illustrate this with an example:
Example 1. Consider the application of performance diagnosis
for a cloud-based data warehouse service. Specifically, consider a
dataset collected from the monitoring views in Amazon Redshift
Serverless [ 8], including performance metrics and query-extracted
features, such as the number of unique tables and columns refer-
enced in the executed query. This dataset enables answering crucial
causal queries for optimizing performance. For example, under-
standing the impact of caching on latency (i.e., Result Cache Hit
onElapsed Time ) can help tune caching mechanisms, or analyzing
the effect of join complexity on the query plannerâ€™s performance
(i.e.,Num Joins onPlan Time ) can optimize query execution strate-
gies. However, the necessary causal DAG to answer such questions
is not readily available, and getting it right is non-trivial.
Figure 1 shows an example causal DAG covering variables from
just one monitoring view [ 6] and a few query features, chosen for
illustration. This is just a small part of the overall high dimensional
dataset. Edges in causal DAGs represent potential cause-effect rela-
tionships. In our example, for instance, the edge from Num Columns
toExec. Time suggests that the number of columns referenced in
a query may influence the queryâ€™s execution time.
To answer the above causal queries, Query Template is a critical
confounder that must be adjusted for because it influences both
the performance metrics (e.g., Elapsed Time ,Plan Time ) and the
analyzed mechanisms (e.g., Result Cache Hit ,Num Joins ). Failing
to adjust for this variable can lead to biased estimations and incor-
rect conclusions. Hence, any possible misspecification in the causal
DAG that would fail to identify this variable as a confounder wouldarXiv:2504.14937v1  [cs.LG]  21 Apr 2025
Anna Zeng, Michael Cafarella, Batya Kenig, Markos Markakis, Brit Youngmann, and Babak Salimi
result in incorrect effect estimations. Such sensitivity to graph er-
rors makes domain expert verification essential for each existing
or missing edge. This task can be overwhelming, even in this small
example with only 12nodes, as it involves inspecting 66potential
edges, one per pair of nodes. In the full dataset, the number of
variables would be much higher, further complicating the task. â–¡
Graph summarization is a logical next step, as it reduces the
number of nodes and edges, making it easier for users to verify and
inspect causal DAGs in high-dimensional datasets. Graph summa-
rization has been extensively studied, with state-of-the-art methods
designed to efficiently generate concise representations aimed at
minimizing reconstruction errors [ 47,110], or facilitating accurate
query answering [ 52,96]. However, we argue that while general-
purpose methods are adept at managing massive graphs, they are
inadequate for summarizing causal graphs, a task that demands the
preservation of causal information crucial for reliable inference.
In this paper, we propose a graph summarization technique tai-
lored for causal inference. It simplifies high-dimensional causal
DAGs into manageable forms without compromising essential
causal information, thereby improving interpretability. Our ap-
proach introduces a causal DAG summarization objective, which
balances simplifying the graph for enhanced comprehensibility and
retaining essential causal information. Using our technique, one
can summarize an initial causal DAG (constructed using partial
domain knowledge or causal discovery) for simpler verification and
elicitation. Additionally, the summary causal DAG can be directly
used for causal inference and is more robust to misspecification of
assumptions. Our approach thereby improves interpretability ,veri-
fiability , and robustness in causal inference, facilitating the adoption
of these techniques in practice. We illustrate this with an example:
Example 2. Consider Fig. 2a, which shows the summary graph
generated by SSumM [ 47] for the causal DAG of Fig. 1. SSumM is
a top-performing general-purpose graph summarization method
that effectively balances conciseness and reconstruction accuracy.
However, the generated graph can no longer be interpreted as a
causal DAG, since it exhibits cycles and self-loops. For example,
computing the causal effect of Num Joins onPlan time is impossi-
ble due to the bidirectional edge between their cluster nodes. Other
methods (e.g., [ 110]) exhibit similar weaknesses, making them un-
suitable for summarizing causal DAGs. An in-depth comparison
with another graph summarization method [ 103] is provided in
Section 8. We show that although this method can be adapted to
generate summary DAGs compatible with causal inference prin-
ciples, it does not optimally preserve critical causal information,
reducing the accuracy of the inference over the summary DAG.
In contrast, Fig.2b shows the 5-node summary DAG generated by
our approach, which preserves critical causal information, offering
a more interpretable summary that can be directly used for infer-
ence. This summary DAG makes it easier to verify the soundness of
assumptions it encodes. Furthermore, this summary DAG is inher-
ently more robust to misspecification, because our summarization
process creates a summary DAG compatible with a setof possi-
ble initial DAGs. Hence, even if the original causal DAG missed
an edge, our summarization algorithm can still create the neces-
sary connections and maintain causal integrity. Using the summary
DAG for inference intuitively leads to a more conservative set ofQuery
TemplateReturned
RowsReturned
Bytes
Num
JoinsNum
TablesNum
ColumnsResult
Cache
Hit
Compile
TimePlan
TimeLock
Wait
TimeExec.
Time
Elapsed
Time
Figure 1: Example causal DAG
Query
TemplateRet. Rows,
Ret. Bytes,
Num
Columns
Num Joins,
Num Tables,
Result
Cache Hit,
Elapsed
Time
Compile
TimePlan Time,
Exec. Time,
Lock Wait
Time
(a) Problematic Summary GraphQuery
Template,
Ret. Rows,
Ret. Bytes,
Num
Columns
Num
Joins,
Num
TablesResult
Cache Hit,
Exec. Time
Plan
Time,
Lock Wait
TimeCompile
Time,
Elapsed
Time
(b) Our Summary DAG
Figure 2: 5-node summary graphs for the DAG in Fig. 1.
confounders: it may lead to adjusting for redundant attributes, but
they will only be ones that do not hurt the analysis. â–¡
Our main contributions are summarized as follows.
Causal DAG Summarization . We introduce the problem of sum-
marizing causal DAGs in a way that preserves their utility for
reliable causal inference (Section 3). This necessitates preserving
the causal information encoded in the input DAG. Causal DAGs
encode information through missing edges, which imply Condi-
tional Independence (CI) constraints. We therefore formalize causal
DAG summarization as finding a summary DAG that preserves CI
statements to the greatest extent possible, while meeting a node
number constraint. We prove that this problem is NP-hard.
Summary Causal DAGs . We introduce the concept of summary
causal DAGs , derived by grouping nodes within the original DAG
vianode contractions . Despite inherently leading to information loss,
node contraction enables summary DAGs to compactly encapsulate
potential causal DAGs from which the summary DAG could have
originated. We show that contracting nodes is akin to adding edges
to the input causal DAG. Based on this connection, we develop a
sound and complete algorithm for identifying all CIsencoded by a
summary DAG. This connection is crucial for utilizing summary
causal DAGs for causal inference. (Section 4).
TheCaGreS Algorithm . We devise an efficient heuristic greedy
algorithm called CaGreS . A key feature of CaGreS is its approach
to choosing which node pair to contract. This process is informed
by the connection between node contraction and the addition of
edges to the input DAG, prioritizing node pairs that add the fewest
edges upon contraction. Additionally, CaGreS incorporates several
optimizations, including caching mechanisms, making it a practical
tool for generating summary causal DAGs (Section 5).
Causal DAG Summarization (Full Version)
ğ´
ğµ ğ¶
ğ·
ğ¸
(a)G1ğ´
ğµ ğ¶
ğ·
ğ¸
(b)G2ğ´
ğµ ğ¶
ğ·
ğ¸
(c)G3
Figure 3: Three causal DAGs over the same set of nodes.
Causal Inference over Summary Causal DAGs We show that
summary causal DAGs can be directly utilized for causal inference.
We establish that Pearlâ€™s do-calculus framework [ 74], which pro-
vides a set of sound and complete rules for reasoning about the
effects of interventions using causal DAGs, remains sound and com-
plete for summary DAGs. By examining the connection between
node contractions and the addition of edges, we offer clear insights
into how these modifications affect the soundness and completeness
of do-calculus within the framework of summary DAGs (Section 6).
Experimental Evaluation We demonstrate how summary DAGs
offer robustness against errors in the input causal DAG (Section 7).
We conduct extensive experiments over six datasets demonstrating
the effectiveness of CaGreS compared to existing solutions and two
variations of CaGreS . The results show the efficiency of CaGreS
in handling high-dimensional datasets and its ability to generate
summary DAGs that ensure reliable inference (Section 8).
2 BACKGROUND
We consider a single-relation database over a schema A. We use
upper case letters to denote a variable from Aand bold symbols for
sets of variables. The broad goal of causal inference is to estimate the
effect of an exposure variable ğ‘‡âˆˆAon an outcome variable ğ‘‚âˆˆA. We
use Pearlâ€™s model for causal inference on observational data [74].
To get an unbiased estimate for the causal effect of the exposure
ğ‘‡on the outcome ğ‘‚, one must mitigate the effect of confounding
variables , i.e., variables that can affect the exposure assignment and
outcome [ 74]. For instance, when estimating how query execution
time affects the elapsed time, one would avoid a source of confound-
ing bias by considering the number of columns and tables. Pearlâ€™s
model provides ways to account for confounding variables to get
an unbiased causal estimate using causal DAGs [74]. Causal DAGs
provide a simple way of representing causal relationships within a
set of variables. A causal DAG Gfor the variables in Ais a specific
type of a Bayesian network and is formally defined as follows:
Causal DAG . A Bayesian network is a DAG Gin which nodes
represent random variables and edges express direct dependence
between the variables. Each node ğ‘‹ğ‘–is associated with the condi-
tional distribution P(ğ‘‹ğ‘–|ğœ‹(ğ‘‹ğ‘–)), whereğœ‹(ğ‘‹ğ‘–)is the set of parents
ofğ‘‹ğ‘–inG. The joint distribution over all variables P(ğ‘‹1,...,ğ‘‹ğ‘›),
is given by the product of all conditional distributions. That is,
P(ğ‘‹1,...,ğ‘‹ğ‘›)=ğ‘›Ã–
ğ‘–=1P(ğ‘‹ğ‘–|ğœ‹(ğ‘‹ğ‘–)) (1)
A causal DAG is a Bayesian network where edges signify direct
causal influence rather than statistical dependence. We say that ğ‘‹
is a potential cause of ğ‘Œif there is a directed path from ğ‘‹toğ‘Œ.
ğ‘‘-Separation. ğ‘‘-separation is a criterion in a causal DAG that de-
termines whether two sets of nodes are conditionally independent,given a third set, by checking whether all paths between the sets
are â€œblockedâ€ based on specific structural rules. If two sets of nodes
areğ‘‘-separated, by definition it means that all paths connecting
them are blocked by other nodes. Formally, a trailğ‘¡=(ğ‘‹1,...,ğ‘‹ğ‘›)
is a sequence of nodes s.t. there is a a distinct edge between ğ‘‹ğ‘–and
ğ‘‹ğ‘–+1for everyğ‘–. That is,(ğ‘‹ğ‘–â†’ğ‘‹ğ‘–+1)âˆˆE(G)or(ğ‘‹ğ‘–â†ğ‘‹ğ‘–+1)âˆˆE(G)for
everyğ‘–. A nodeğ‘‹ğ‘–is said to be head-to-head with respect to ğ‘¡if
(ğ‘‹ğ‘–âˆ’1â†’ğ‘‹ğ‘–)âˆˆE(G)and(ğ‘‹ğ‘–â†ğ‘‹ğ‘–+1)âˆˆE(G). A trailğ‘¡=(ğ‘‹1,...,ğ‘‹ğ‘›)
isactive given ZâŠ†Xif (1) everyğ‘‹ğ‘–that is a head-to-head node with
respect toğ‘¡either belongs to Zor has a descendant in Z, and (2)
everyğ‘‹ğ‘–that is not a head-to-head node w.r.t. ğ‘¡does not belong to
Z. If a trailğ‘¡is not active given Z, then it is blocked given Z[74]. In
a DAG, two sets of nodes XandYareğ‘‘-separated by a third set of
nodes Zif all trails connecting XandYare blocked by Z.
Conditional Independence. Causal DAGs encode a set of Condi-
tional Independence statements ( CIs) that can be read off the graph
usingğ‘‘-separation [ 74]. These statements describe the absence of
an active trail between two sets of variables when conditioning on
other variables. If two sets of nodes XandYareğ‘‘-separated by Z,
thenXandYare conditionally independent given Z.
Example 3. Examples of CIsencoded in the causal DAG depicted
in Fig. 3(a) include: ( ğµâŠ¥âŠ¥ğ‘‘ğ¶|ğ´), and (ğ·âŠ¥âŠ¥ğ‘‘ğ´|ğµğ¶).â–¡
CIs & Missing Edges. In causal DAGs, the information encoded
by missing edges implies the set of CIsthe DAG represents. Namely,
removing edges can undermine the causal model as it implies CIs
that do not necessarily hold in the distribution. On the other hand,
existing edges indicate potential causal dependence. This implies
that adding edges to a causal DAG, provided acyclicity is maintained,
does not necessarily compromise validity [74].
The Recursive Basis. The Recursive Basis (RB) [ 33] for a causal
DAG comprises a set of at most ğ‘›CIs, signifying that each node
is conditionally independent of its non-descendants nodes given
its parents. This succinct set of CIsholds significance, as it can be
used for constructing the causal DAG, and all other CIsencoded
in the DAG can be deduced from it. Formally, given a causal DAG
G, letâŸ¨ğ‘‹1,...,ğ‘‹ğ‘›âŸ©denote a complete topological order over V(G).
Equation 1 implicitly encodes a set of ğ‘›CIs, called the RB for G,
defined as follows:
Î£RB(G)def={(ğ‘‹ğ‘–âŠ¥âŠ¥ğ‘‹1...ğ‘‹ğ‘–âˆ’1\ğœ‹(ğ‘‹ğ‘–)|ğœ‹(ğ‘‹ğ‘–)):ğ‘–âˆˆ[ğ‘›]} (2)
It has been shown [ 32,33,107] that both the semi-graphoid axioms
(see Appendix A) and ğ‘‘-separation are sound and complete for
inferring CIsfrom the RB, which matches the CIsencoded by the
causal DAG.
Example 4. Consider the causal DAG G1in Fig. 3(a). In the nodesâ€™
topological order, ğ´precedesğµandğ¶, which in turn, precedes
ğ·. The last node is ğ¸. The RB ofG1is given in Table 1. Given
the topological order over the nodes and the RB, G1can be fully
constructed. Further, any CI statement encoded in G1can be implied
from this RB by using the semi-graphoid axioms. â–¡
ATE& do-Calculus. Theğ‘‘o-operator, a fundamental concept in
causal inference, is used to denote interventions on variables in a
causal model. It represents the intervention on a variable to observe
the resulting change in an outcome variable while holding the
Anna Zeng, Michael Cafarella, Batya Kenig, Markos Markakis, Brit Youngmann, and Babak Salimi
external factors constant. In computing the Average Treatment Effect
(ATE) [ 74], a popular measure of causal estimate, the ğ‘‘ğ‘œ-operator
is applied to represent the treatment assignment for treatment and
control groups. The ATE quantifies the average causal effect of a
treatmentğ‘‡on an outcome variable ğ‘‚in a population:
ğ´ğ‘‡ğ¸(ğ‘‡,ğ‘‚)=E[ğ‘‚|ğ‘‘ğ‘œ(ğ‘‡=1)]âˆ’E[ğ‘‚|ğ‘‘ğ‘œ(ğ‘‡=0)] (3)
To compute the causal effect of ğ‘‡onğ‘‚, it is crucial to identify
and adjust for confounders. The backdoor criterion [ 74] provides a
sufficient condition by identifying a set of variables Zthat blocks all
backdoor paths between ğ‘‡andğ‘‚, enabling confounder adjustment
within the causal DAG framework. However, it is part of the ğ‘‘ğ‘œ-
calculus system, an axiomatic framework designed for reasoning
about interventions and their effects within causal models. The
ğ‘‘ğ‘œ-calculus comprises three rules that facilitate the substitution of
probability expressions containing the ğ‘‘ğ‘œ-operator with standard
conditional probabilities [ 74]. It provides a systematic method for
deriving causal relationships from observational data. Due to its
soundness and completeness, the framework offers a broad toolkit
for causal inference. Since these concepts are not directly used in
this paper, we omit a detailed review.
3 PROBLEM FORMULATION
Our goal is to distill a causal DAG1into an interpretable summary
by grouping nodes while preserving its utility for causal inference.
Thus, the summary DAG should meet the following criteria:
Size Constraint: The summary DAG should be concise to reduce
the cognitive load on analysts [ 14]. We therefore impose a size
constraint to enhance the summary DAGâ€™s intelligibility, ensuring
the core complexity of the original DAG is maintained.
Preserving Causal Information: The summary DAG must main-
tain the causal dependencies present in the original DAG: If variable
ğ´has a directed causal path to ğµin the original DAG, this rela-
tionship should be faithfully preserved in the summary DAG. The
summary DAG should also preserve the CIs represented in the orig-
inal DAG. If variables ğ´andğµare conditionally independent, this
lack of dependence should be reflected in the summary DAG. Lastly,
the summary DAG should not introduce any spurious conditional
independencies that the original causal DAG does not imply.
Our objective is to preserve the utility of the summary causal
DAG for causal inference. As mentioned, in causal DAGs, the infor-
mation encoded by missing edges implies the set of CIsthe DAG
represents. Therefore, removing edges can undermine the causal
model as it implies CIsthat do not necessarily hold in the original
DAG. On the other hand, existing edges indicate potential causal
dependence. This implies that adding edges to a causal DAG, pro-
vided acyclicity is maintained, does not necessarily compromise
validity. We, therefore, rigorously enforce conditions on the sum-
mary DAG to ensure that the directionality is faithfully preserved
and assert that the summary DAG should preserve, to the greatest
extent possible, a subset of the independence assumptions encoded
in the original DAG. We show that, with these considerations, the
summary causal DAG remains a viable tool for causal inference.
We first formalize the concept of a summary causal DAG, then
rigorously formalize the problem of causal DAG summarization.
1For a discussion of other causal graph formats like mixed graphs, see Appendix Cğ´
BC
ğ·
ğ¸
(a)H1ğ´
BDğ¶
ğ¸
(b)H2ABC
ğ·
ğ¸
(c)H3ğ´
BC
DE
(d)H4H1
H3H4
H2
(e) Order
Figure 4: Summary causal DAGs for G1and the partial order
among them.
3.1 Summary Causal DAGs
Asummary graph is obtained by applying node contraction oper-
ations [ 76]. The resulting graph retains the essential connectivity
information of the original graph with a reduced number of nodes.
Given a graphG, the contraction of a pair of nodes ğ‘ˆ,ğ‘‰âˆˆV(G)
is the operation that produces a graph Hin which the two nodes
ğ‘ˆandğ‘‰are replaced with a single node C={ğ‘ˆ,ğ‘‰}âˆˆV(H), where C
is now neighbors with nodes that ğ‘ˆandğ‘‰were originally adjacent
to (edge directionality is preserved). If ğ‘ˆandğ‘‰were connected by
an edge, the edge is removed upon contraction.
Definition 1 (Summary-DAG) .A summary DAG of a DAG Gis
a pair(H,ğ‘“), whereHis a DAG with nodes V(H), edges E(H),
andğ‘“:V(G)â†’ V(H) is a function that partitions the nodes V(G)
among the nodes V(H), such that: If(ğ‘ˆ,ğ‘‰)âˆˆE(G), thenğ‘“(ğ‘ˆ)=ğ‘“(ğ‘‰)
or(ğ‘“(ğ‘ˆ),ğ‘“(ğ‘‰))âˆˆE(H). We define the inverse ğ‘“âˆ’1:V(H)â†’ 2V(G)
as follows:ğ‘“âˆ’1(ğ‘‹)def={ğ‘‰âˆˆV(G) :ğ‘“(ğ‘‰)=ğ‘‹}
To simplify the notations, we omit ğ‘“whenever possible.
Example 5. Consider Fig. 3(a) which depicts a DAG G1. After
contracting ğµandğ¶, the resulting summary DAG H1is displayed
in Fig. 4(a). InH1, the nodesğµandğ¶have been contracted into the
nodeBC. Namely,ğ‘“(ğµ)=ğ‘“(ğ¶)=BC, andğ‘“âˆ’1(BC)={ğµ,ğ¶}.â–¡
A causal DAGGis said to be compatible with a summary DAG H,
if, there exists a function ğ‘“that partitions the nodes V(G)among
the nodes V(H), such that: If(ğ‘ˆ,ğ‘‰)âˆˆE(G), thenğ‘“(ğ‘ˆ)=ğ‘“(ğ‘‰)or
(ğ‘“(ğ‘ˆ),ğ‘“(ğ‘‰))âˆˆE(H). Namely,His a summary DAG of G.
Definition 2 (Compatibility) .Let(H,ğ‘“)be a summary DAG. A
DAGGiscompatible withHifHis a summary DAG for G. We
use{Gğ‘–}Hto denote the set of all causal DAGs compatible with H.
We also use the term compatibility to describe the relationship
between two causal DAGs sharing the same set of nodes, where
the edges of one are fully contained in the set of edges of another
graph. LetGbe a causal DAG and let Gâ€²be a causal DAG where
V(G)=V(Gâ€²). We say thatGâ€²is asupergraph ofGifE(G)âŠ† E(Gâ€²).
In this case, we also say that Giscompatible withGâ€².
Example 6. Consider again Fig. 3. Both G1andG2are compatible
with the summary DAG H1shown in Fig. 4(a) (achieved by con-
tractingğµandğ¶). However,G3is not compatible with H1, since
the edge between ğ·andğ´is not preserved. â–¡
We aim to find acyclic summary graphs. Thus, we prove a simple
lemma characterizing node contractions that preserve acyclicity.
Causal DAG Summarization (Full Version)
Table 1: The recursive bases of the summary DAGs in Figure 4
Graph Recursive Basis
G1(ğ¶âŠ¥âŠ¥ğµ|ğ´),(ğ·âŠ¥âŠ¥ğ´|ğµğ¶),(ğ¸âŠ¥âŠ¥ğ´ğµğ¶|ğ·)
H1(ğ·âŠ¥âŠ¥ğ´|ğµğ¶),(ğ¸âŠ¥âŠ¥ğ´ğµğ¶|ğ·)
H2 (ğ¸âŠ¥âŠ¥ğ´ğ¶|ğµğ·)
H3 (ğ¸âŠ¥âŠ¥ğ´ğµğ¶|ğ·)
H4 (ğ·ğ¸âŠ¥âŠ¥ğ´|ğµğ¶)
Lemma 3.1. LetGbe a DAG, and let ğ‘‰,ğ‘ˆâˆˆV(G). LetHğ‘‰ğ‘ˆdenote
the summary graph that results from Gby contracting ğ‘‰andğ‘ˆ. Then
Hğ‘‰ğ‘ˆcontains a directed cycle if and only if Gcontains a directed
pathğ‘ƒfromğ‘‰toğ‘ˆ(orğ‘ˆtoğ‘‰), where|ğ‘ƒ|â‰¥2.
Asummary causal DAG is a specific type of summary graph
obtained through node contraction operations over a causal DAG
Gand ensures acyclicity.
As mentioned, the RB of a causal DAG, as defined by Eq. (2),
comprises a set of at most ğ‘›CIs(whereğ‘›=|V(G)|), signifying that
each node is conditionally independent of its preceding nodes2
given its parents. This succinct set of CIsholds significance, because
it enables the derivation of all other CIsrepresented in the causal
DAG. The RB of a summary causal DAG is defined in a similar
manner. The only difference is that in a summary causal DAG, a
node may represent a subset of nodes of the original DAG.
Example 7. Figure 4 displays four summary graphs for the causal
DAG in Figure 3(a). Table 1 shows the RBs of these summary causal
DAGs. InH4, for instance, there are only three nodes and therefore
the RB includes a single CI statement. â–¡
3.2 The Causal DAG Summarization Problem
As mentioned, we aim to reduce an input causal DAG by contracting
its nodes, retaining maximal causal information. We covered the
two criteria of our problem before proceeding with formalizing it.
Size Constraint A size constraint is a key motivating constraint
for graph summarization work and may be imposed on the number
of nodes, storage space, minimum description length, etc. [ 51].
We focus on a node-based size constraint as limited-size graphs
are generally more accessible for inspection [14, 39]. Additionally,
setting and adjusting a limit on the number of nodes is shown to be
relatively straightforward for analysts [ 42,103]. We also observe
that other summarization problems share similar hyperparameters,
such as many clustering algorithms or k-nearest neighbors [ 45,49].
Causal Information Preservation As mentioned, if two variables
have a directed path between them in the original DAG, then this
relationship should be faithfully preserved in the summary DAG.
Indeed, this follows from the definition of a summary DAG (Def. 1).
Given two summary DAGs derived from the same causal DAG
G, both adhering to the size constraint, we prefer the one that
preserves, to a larger degree, the set of CIsrepresented inG. To
this end, we devise a measure to compare summary DAGs based on
their RBs. When comparing two summary DAGs H1andH2, we
assert thatH1issuperior toH2if the RB ofH2is implied by the RB
ofH1. Namely, all the CIsencoded byH2can also be deduced from
H1. We are searching for a maximal summary causal DAG, namely,
that its RB is not implied by any other valid summary DAG.
As mentioned, a summary DAG should not introduce any spuri-
ous CIs that the original causal DAG does not imply. However, it
2According to a given full topological order of the nodesmay overlook some CIs that are present in the original DAG. We
refer to this property as an I-Map. Formally, Let Î©def={ğ‘‹1,...,ğ‘‹ğ‘›}
be a set of jointly distributed random variables with distribution P
(i.e., nodes of the original DAG). Formally,
Definition 3 (I-Map) .A DAGGis an I-Map forPif for every
disjoint sets X,Y, and Zit holds that(XâŠ¥âŠ¥ğ‘‘Y|Z)Gonly if
(XâŠ¥âŠ¥PY|Z).
LetG1andG2be two DAGs that are I-Maps for P. We say that
G2is superior toG1, in notationG2â‰»G 1, if for every ğœâˆˆÎ£RB(G1),
it holds that Î£RB(G2)=â‡’ğœ. Note that the relation â‰»does not
necessarily form a complete order. We say that Gismaximal for P
ifGis an I-Map for P, and there does not exist any Gâ€²âˆˆG(P)such
thatGâ€²â‰»G. Our goal is to find a summary DAG that is an I-Map for
Pand maximal for P, given a constraint on the number of nodes.
Example 8. Consider the causal DAG G1in Fig. 3(a). Fig. 4(a)
presents a 4-size summary DAG H1forG1. The RBs of both DAGs
are shown in Table 1. Clearly, Î£RB(H1)âŠ‚Î£RB(G1), and henceH1is
an I-Map for P. Fig. 4(b) presentsH2, another 4-size summary DAG
forG1, where Î£RB(H2)={(ğ¸âŠ¥âŠ¥ğ´ğ¶|ğµğ·)}. From the semi-graphoid
axioms, it holds that (ğ¸âŠ¥âŠ¥ğ´ğµğ¶|ğ·)=â‡’(ğ¸âŠ¥âŠ¥ğ´ğ¶|ğµğ·). Thus,
H1â‰»H 2. Hence,H1is a superior summary DAG. Similarly, Fig-
ures 4(c) and 4(d) illustrate H3andH4,3-size summary DAGs for
G1. Their RBs are given in Table 1. The partial order among all sum-
mary DAGs is presented in Fig. 4(e). Despite H3having only three
nodes, it surpassesH2. However,H3andH4are incomparable, i.e.,
neither Î£RB(H3)=â‡’Î£RB(H4)norÎ£RB(H4)=â‡’Î£RB(H3).â–¡
Problem 1 (Causal DAG Summarization) .Given a causal DAG G
defined over a joint distribution P, and a bound ğ‘˜, find a summary
causal DAGHs.t. (i) the number of nodes in Hisâ‰¤ğ‘˜; (ii)Gis
compatible withH, is an I-Map for Pand is maximal for P.
Example 9. Consider again the causal DAG in Fig. 1. We set ğ‘˜=5.
Fig. 2b depicts an optimal summary causal DAG. Namely, the RB
of any other summary causal DAG with 5 or fewer nodes is not
superior to RB of this 5-node summary causal DAG. â–¡
We show that the causal DAG summarization problem is ğ‘ğ‘ƒ-
hard via a reduction from the ğ‘˜-Max-Cut problem [38].
Theorem 3.2. Causal DAG summarization is an NP-hard problem.
As the proof relies on the relationship between node contractions
and the addition of edges, established in the next section, we will
explain the intuition behind this theorem in Section 4.1.
4 NODE-CONTRACTION AS EDGE ADDITION
Next, we establish the connection between node contractions and
the addition of edges to the input causal DAG. This connection
will be used to read off, from a given summary causal DAG, all
theCIsit encodes. It also serves as a pivotal factor in guiding our
algorithm for selecting promising node pairs to merge. Additionally,
in Section 6, we will leverage this connection to demonstrate how
causal inference can be directly conducted over summary DAGs.
We note that the canonical causal DAG is not an objective of our
problem; rather, it serves as a tool to formally define causal inference
over summary DAGs and to guide our algorithm in identifying node
contractions that minimize information loss.
Anna Zeng, Michael Cafarella, Batya Kenig, Markos Markakis, Brit Youngmann, and Babak Salimi
ğ´
ğµ ğ¶
ğ·
ğ¸
(a) Causal DAGABC
ğ·
ğ¸
(b) Summary DAGğ´
ğµ ğ¶
ğ·
ğ¸
(c) Canonical causal DAG
Figure 5: A causal DAG, its summary DAG, and the corre-
sponding canonical causal DAG
.
4.1 The Canonical Causal DAG
Given a summary causal DAG H, we define its corresponding
canonical causal DAG , denoted asGH. In this causal DAG, cluster
nodes are decomposed into distinct nodes connected by edges. We
show that the RB of the canonical causal DAG isequivalent to that
ofH. We first define the notion of equivalence for sets of CIs.
Definition 4 (CI Sets Equivalence) .LetSandTdenote two sets
ofCIsover the variable-set {ğ‘‹1,...,ğ‘‹ğ‘›}. We say that S=â‡’Tif
S=â‡’ğœfor every CI ğœâˆˆT. We say that SandTareequivalent , in
notation Sâ‰¡T, ifS=â‡’TandT=â‡’S.
Next, we formally define the notion of the canonical causal DAG
for a given summary DAG.
Definition 5 (Canonical Causal DAG) .Let(H,ğ‘“)be a summary
DAG for a causal DAG G. LetâŸ¨ğ‘‹1,...,ğ‘‹ğ‘›âŸ©denote a complete topo-
logical order over V(G). We define the canonical causal DAG asso-
ciated with(H,ğ‘“), denotedGHas follows: V(GH)=V(G), and
(ğ‘‹ğ‘–,ğ‘‹ğ‘—)âˆˆE(GH)if and only if (ğ‘‹ğ‘–,ğ‘‹ğ‘—)âˆˆE(G)
or(ğ‘“(ğ‘‹ğ‘–),ğ‘“(ğ‘‹ğ‘—))âˆˆ E(H)
or ğ‘“(ğ‘‹ğ‘–)=ğ‘“(ğ‘‹ğ‘—)andğ‘–<ğ‘—
We observe that, by definition, GHis compatible with the sum-
mary DAG(H,ğ‘“).
Example 10. Consider Figures 5(a) and 5(b) that depict an input
causal DAG, and a 3-node summary. Fig. 5(c) depicts the correspond-
ingcanonical causal DAG . In the topological order ğ´precedesğµ
which in turn precedes ğ¶. All nodes within the node ABC are con-
nected by edges in the canonical causal DAG, according to the
topological order. Since ABC is the parent of ğ·in the summary
DAG, in the canonical causal DAG allğ´,ğµ andğ¶are parents of ğ·.
Note that Fig. 5(c) contains two more edges than Fig. 5(a), which
represents conditional independence relationships which are not
captured in the 3-node summary graph Figure 5(b). â–¡
We show that the RB of the canonical causal DAG GHis equiva-
lent to that of the summary DAG Hobtained by node contractions
to a causal DAGG. In other words, node contractions can be con-
ceptualized as the addition of edges to the input causal DAG.
Theorem 4.1. LetHbe a summary causal DAG, and GHis its cor-
responding canonical causal DAG. We have: Î£RB(H)â‰¡ Î£RB(GH).
Continuing with Example 10, the RB of GH3is(ğ¸âŠ¥âŠ¥ğ´ğµğ¶|ğ·),
which is identical to that of H3(see Table 1).
Proof Intuition for Theorem 3.2 . In our proof we rely on the
connection between a summary DAG and it canonical causal DAG .Specifically, Theorem 3.2 establishes that finding a summary DAG
(H,ğ‘“)whose canonical causal DAG GHresults in the smallest num-
ber of added edges |E(GH)|âˆ’| E(ğº)|is NP-Hard. Specifically, our
proof shows that finding a summary DAG (H,ğ‘“)where|V(H)|=ğ‘˜
and|E(GH)|âˆ’| E(ğº)| â‰¤ğœfor some threshold ğœ>0is an NP-
complete problem. In fact, we prove the stronger claim that finding
a summary DAG(H,ğ‘“)where|V(H)|=ğ‘˜and
{(ğ‘‹ğ‘–,ğ‘‹ğ‘—)âˆˆE(GH)\E(ğº):ğ‘“(ğ‘‹ğ‘–)=ğ‘“(ğ‘‹ğ‘—}â‰¤ğœ (4)
is NP-hard. If|E(GH)|âˆ’| E(ğº)|â‰¤ğœ, then (4)must hold as well. We
establish that finding a summary DAG where (4)holds is NP-Hard,
and hence finding a summary DAG where |E(GH)|âˆ’| E(ğº)|â‰¤ğœis
NP-Hard as well. The full proof is provided in the Appendix.
4.2ğ‘ -Separation
We introduce the notion of ğ‘ -separation , an extension of ğ‘‘-separation,
tailored to identify CIsencoded by a summary DAG. Intuitively,
a summary DAG represents a collection of causal DAGs that are
compatible with it, meaning that it could have been obtained from
any of those DAGs (similar to possible worlds [23]). Each of these
DAGs encodes a different set of CIs. The set of CIsencoded by a
summary DAG is the intersection of CIsthat holds in all compat-
ible DAGs. In this way, we can ensure we restrict ourselves only
toCIsthat are certainly present in a particular context and can
be reliably used for inference. ğ‘ -separation extends ğ‘‘-separation,
which allows the identification of valid CIs within a summary DAG.
We also introduce a sound and complete ğ‘ -separation algorithm
that leverages the standard ğ‘‘-separation algorithm.
The validity of a CI statement, as derived from summary DAG
H, is given by the following definition:
Definition 6 (Validity of a CI in a summary DAG) .A CI statement
is deemed valid in a summary causal DAG Hif and only if it is
implied by all causal DAGs within {Gğ‘–}H.
ğ‘ -separation captures all certain CIsthat hold across all DAGs
in{Gğ‘–}H. We propose the following criterion for ğ‘ -separation to
encapsulate this notion of validity.
Definition 7 (ğ‘ -separation) .Given a summary DAG (H,ğ‘“)and
disjoint subsets X,Y,ZâŠ†V(H), we say that XandYareğ‘ -separated
inHbyZ, denoted by(XâŠ¥âŠ¥ğ‘†Y|Z)H, iffğ‘“âˆ’1(X)andğ‘“âˆ’1(Y)are
ğ‘‘-separated by ğ‘“âˆ’1(Z)in every causal DAG within {Gğ‘–}H.
We say that XandYareğ‘ -connected in(H,ğ‘“)byZ, if there
exists a causal DAG Gâˆˆ{Gğ‘–}H, such thatğ‘“âˆ’1(X)andğ‘“âˆ’1(Y)are
ğ‘‘-connected inGbyğ‘“âˆ’1(Z).
4.2.1ğ‘ -separation Algorithm. Given a summary causal DAG
H, we aim to derive the set of CIsit encodes. A naive approach
would be to employ ğ‘‘-separation algorithms [ 74]. However,H
can potentially encompass more CIsthan those discerned through
ğ‘‘-separation alone, as demonstrated in the following example.
Example 11. Referring back to Fig. 3, (ğµâŠ¥âŠ¥ğ‘‘ğ¸|ğ·),(ğ¶âŠ¥âŠ¥ğ‘‘ğ¸
|ğµ,ğ·), and(ğµ,ğ¶âŠ¥âŠ¥ğ‘‘ğ¸|ğ·)all hold inG1andG2. Likewise,
(BCâŠ¥âŠ¥ğ‘‘ğ¸|ğ·)holds inH1(Fig. 4(a)). However, since H1does not
containğµorğ¶as separate nodes, we cannot establish (ğµâŠ¥âŠ¥ğ‘‘ğ¸|ğ·)
or(ğ¶âŠ¥âŠ¥ğ‘‘ğ¸|ğµ,ğ·)fromH1usingğ‘‘-separation. â–¡
Causal DAG Summarization (Full Version)
To address this, a simple solution is to find the set of CIsshared
across all DAGs compatible with H. However, this approach is
costly. We, therefore, present a simple algorithm for ğ‘ -separation
that leverages the connection between a summary DAG and its
canonical causal DAG. This algorithm operates as follows: Given
a summary DAGH, establish a topological order for its nodes.3
Using this order, construct the canonical causal DAG GH. Next,
applyğ‘‘-separation overGHand return the resulting CI set. We
demonstrate that this algorithm is sound and complete.
Theorem 4.2 (Soundness and Completeness of ğ‘ -separation).
In a summary DAG (H,ğ‘“), letX,Y,ZâŠ†V(H) be disjoint sets of nodes.
IfXandYareğ‘‘-separated by ZinH, then in any causal DAG
Gâˆˆ{Gğ‘–}H,ğ‘“âˆ’1(X)andğ‘“âˆ’1(Y)areğ‘‘-separated by ğ‘“âˆ’1(Z). That is:
(XâŠ¥âŠ¥ğ‘‘Y|Z)H=â‡’(ğ‘“âˆ’1(X)âŠ¥âŠ¥ğ‘‘ğ‘“âˆ’1(Y)|ğ‘“âˆ’1(Z))G=â‡’(XâŠ¥âŠ¥ğ‘ Y|Z)H
IfXandYareğ‘‘-connected by ZinH, then there exists a DAG Gâˆˆ{Gğ‘–}H,
s.tğ‘“âˆ’1(X)andğ‘“âˆ’1(Y)areğ‘‘-connected by ğ‘“âˆ’1(Z)inG.
5 THE CAGRES ALGORITHM
As demonstrated in Theorem 3.2, the causal DAG summarization
problem is NP-hard and therefore it is not trivial to devise an effi-
cient algorithm with theoretical guarantees. We, therefore, intro-
duce a heuristic algorithm, named CaGreS for the causal DAG
summarization problem. Although lacking theoretical guarantees,
CaGreS effectively meets the size constraint and produces summary
causal DAGs that can be directly used for sound causal inference. A
brute force approach explores all summary DAGs with up to ğ‘˜nodes.
It finds the optimal summary DAG, but runs in exponential time due
to the exponential number of potential graphs. CaGreS addresses
this by estimating the merging effect on the canonical causal DAG
rather than iterating over all possible summary DAGs.
Overview TheCaGreS algorithm follows a previous line of work
[34,103], where a bottom-up greedy approach is used to identify
promising node pairs for contraction. Its main contribution lies in
how it estimates merge costs, to preserve the causal interpretation
of the graph: It counts the number of edges to be added in the
canonical causal DAG for each node pair (a proxy for the RBâ€™s effect,
as discussed in Section 4). In each iteration, the algorithm contracts
the node pair resulting in the minimal number of additional edges.
We also introduce optimizations for runtime efficiency, such as
semantic constraint, fast low-cost merges, and caching mechanisms.
TheCaGreS algorithm is given in Algorithm 1. Given a bound
ğ‘˜and an input causal DAG, this algorithm iteratively seeks the
next-best pair of nodes to be merged, until the size constraint is met
(lines 4-15). The next-best pair of nodes to merge is the node pair
whose contraction has the lowest cost (lines 10-12). The algorithm
randomly breaks ties (lines 13-14). The GetCost procedure is shown
in Algorithm 2. The cost of merging two (clusters of) nodes Uand
Vis equal to the number of edges to be added in the corresponding
canonical causal DAG : (1) edges to be added between the nodes
within the combined cluster UÃV(lines 3-4), (2) new parents for
the nodes in UorVpost-merge (lines 6-11), and (3) new children
for the nodes in UorVafter the merge (lines 13-18).
We next propose three optimizations to improve runtime.
3The order of nodes within a cluster is considered arbitrary, or it may be determined
based on the topological order of the input causal DAG if such information is preserved.Algorithm 1: TheCaGreS Algorithm
input : A causal DAGGand a number ğ‘˜.
output: A summary causal DAG Hwithğ‘˜nodes.
1Hâ†G
2Hâ† LowCostMerges(H)
3while size(H.ğ‘›ğ‘œğ‘‘ğ‘’ğ‘ )>ğ‘˜do
4ğ‘šğ‘–ğ‘›_ğ‘ğ‘œğ‘ ğ‘¡â†âˆ
5(ğ‘¿,ğ’€)â†ğ‘ğ‘¢ğ‘™ğ‘™
6for(ğ‘¼,ğ‘½)âˆˆH.nodes do
7 ifIsValidPair (ğ‘¼,ğ‘½,H)then
8 costğ‘ˆğ‘‰â†GetCost(ğ‘¼,ğ‘½,H)
9 ifcostğ‘ˆğ‘‰<min_cost then
10 min_costâ†costğ‘ˆğ‘‰
11(ğ‘¿,ğ’€)â†(ğ‘¼,ğ‘½)
12 ifcostğ‘ˆğ‘‰==min_cost then
13 Randomly decide if to replace ğ‘¿andğ’€withğ‘¼andğ‘½
14H.Merge(ğ‘¿,ğ’€)
15returnH
Algorithm 2: The GetCost Procedure
input : A summary causal DAG Hand a pair of nodes ğ‘¼andğ‘½.
output: The cost of contracting ğ‘¼andğ‘½.
1costâ†0
2/* New edges among the nodes in the cluster */
3ifH.HasEdge(ğ‘¼,ğ‘½)== False then
4 costâ†cost+ğ‘ ğ‘–ğ‘§ğ‘’(ğ‘¼)Â·ğ‘ ğ‘–ğ‘§ğ‘’(ğ‘½)
5/* New parents */
6parentsğ‘ˆâ†H.GetPredecessors(ğ‘¼)
7parentsğ‘‰â†H.GetPredecessors(ğ‘½)
8parentsOnlyUâ†parentsğ‘ˆ\parentsğ‘‰
9costâ†cost+size(parentsOnlyU)Â·size(ğ‘½)
10parentsOnlyVâ†parentsğ‘‰\parentsğ‘ˆ
11costâ†cost+size(parentsOnlyV)Â·size(ğ‘¼)
12/* New children */
13childrenğ‘ˆâ†H.GetSuccessors(ğ‘¼)
14childrenğ‘‰â†H.GetSuccessors(ğ‘½)
15childrenOnlyUâ†childrenğ‘ˆ\childrenğ‘‰
16costâ†cost+size(childrenOnlyU)Â·size(ğ‘½)
17childrenOnlyVâ†childrenğ‘‰\childrenğ‘ˆ
18costâ†cost+size(childrenOnlyV)Â·size(ğ‘¼)
19return cost
Semantic Constraint : We can reduce the search space and ensure
that only semantically related variables are merged, thereby sup-
porting semantic coherence in the summary DAG. To achieve this,
the user may specify which node pairs are allowed to be merged
by providing a semantic similarity matrix and a threshold that indi-
cates the maximum distance between two nodes within a cluster.
The user can assess the semantic similarity using previous work
on semantic similarity [37, 63] or large language models [5].
Assume a semantic similarity measure ğ‘ ğ‘–ğ‘š(Â·,Â·)that assigns a
value between 0 and 1 to a pair of variables. For a summary DAG H
and a threshold ğœ, we say thatHsatisfies the semantic constraint
if, for every cluster CâˆˆV(H),ğ‘ ğ‘–ğ‘š(ğ‘‰ğ‘–,ğ‘‰ğ‘—)â‰¥ğœfor everyğ‘‰ğ‘–,ğ‘‰ğ‘—âˆˆC.
Anna Zeng, Michael Cafarella, Batya Kenig, Markos Markakis, Brit Youngmann, and Babak Salimi
This condition is checked in line 8 of the CaGreS algorithm when
validating whether a pair of nodes is suitable for contraction.
Caching : We use two caching mechanisms: one for storing invalid
node pairs and another for cost scores. We demonstrated in Section
8.5 that these caching mechanisms are effective in reducing runtime.
We initialize the invalid pairs cache during a preprocessing phase.
An invalid pair is a node pair with semantic similarity below the
threshold or connected by a directed path of length above 2(Lemma
3.1). During CaGreS â€™s run, invalid pairs are cached, and each itera-
tion checks the validity of node pairs before computing costs.
The cost of a node pair U,Vremains unchanged after merging
another node pair X,Yif neither UnorVare neighbors of Xor
Y. Following the merge of XandY, we update the cost cache by
removing the cost scores of all node pairs involving one of their
neighbors. When calculating the cost for a node pair, we check if
the score is in the cache. If not, we compute and add it, ensuring
the cache reflects node pair mergersâ€™ impact on neighboring pairs.
Low Cost Merges : As pre-processing, we contract node pairs with
low costs (line 4). This involves merging nodes that share identical
children and parents. Additionally, we merge nodes linked along
non-branching paths, each having at most one parent and one
child. In Section 8.5, we experimentally show that this optimization
benefits small or low-density causal DAGs.
Time Complexity A single cost computation with ğ‘›=|V(G)| takes
ğ‘‚(ğ‘›)due to the maximum number of neighbors a node can have.
The algorithm undergoes ğ‘›âˆ’ğ‘˜iterations, evaluating all node pairs
(O(ğ‘›2) such pairs) in the current summary DAG with no more than
ğ‘›neighbors. Thus, the overall time complexity is ğ‘‚((ğ‘›âˆ’ğ‘˜)Â·ğ‘›3).
6DO-CALCULUS IN SUMMARY CAUSAL DAGS
Next, we show that the rules of ğ‘‘ğ‘œ-calculus are sound and complete
in summary causal DAGs. This is vital to ensure that the summary
causal DAGs are effective formats that support causal inference
by enabling direct causal inference on the summary DAGs. Our
proof relies on the equivalence between the RB of a summary
DAG and its canonical causal DAG (Theorem 4.1). This result is
not surprising because the canonical causal DAG is a supergraph
of the input causal DAG. Pearl already observed in [ 74] that: â€œThe
addition of arcs to a causal diagram can impede, but never assist,
the identification of causal effects in nonparametric models. This
is because such addition reduces the set of ğ‘‘-separation conditions
carried by the diagram; hence, if causal effect derivation fails in the
original diagram, it is bound to fail in the augmented diagramâ€ .
Given a causal DAG G, for a set of nodes XâŠ†V(G), letGXdenote
the graph that results from Gby removing all incoming edges to
nodes in X, byGXthe graph that results from Gby removing all
outgoing edges from the nodes in X. For a set of nodes XâŠ†V(ğº)\Z,
we denote byGXZthe graph that results from Gby removing all
incoming edges into Xand all outgoing edges from Z.
Theorem 6.1 (Soundness of Do-Calculus in summary causal
DAGs). LetGbe a causal DAG encoding an interventional distribu-
tionğ‘ƒ(Â·|ğ‘‘ğ‘œ(Â·)), compatible with the summary causal DAG (H,ğ‘“).Query
TemplateReturned
RowsReturned
Bytes
Num
JoinsNum
TablesNum
ColumnsResult
Cache
Hit
Compile
TimePlan
TimeLock
Wait
TimeExec.
Time
Elapsed
Time
Figure 6: Modifications to the Redshift DAG (Fig. 1).
For any disjoint subsets X,Y,Z,WâŠ†V(H), the following rules hold:
R1:(YâŠ¥âŠ¥Z|X,W)HX=â‡’ğ‘ƒ(ğ’€|ğ‘‘ğ‘œ(ğ‘¿),ğ’,ğ‘¾)=ğ‘ƒ(ğ’€|ğ‘‘ğ‘œ(ğ‘¿),ğ‘¾)
R2:(ğ’€âŠ¥âŠ¥ğ’|ğ‘¿,ğ‘¾)Hğ‘¿ğ’=â‡’ğ‘ƒ(ğ’€|ğ‘‘ğ‘œ(ğ‘¿),ğ‘‘ğ‘œ(ğ’),ğ‘¾)=ğ‘ƒ(ğ’€|ğ‘‘ğ‘œ(ğ‘¿),ğ’,ğ‘¾)
R3:(ğ’€âŠ¥âŠ¥ğ’|ğ‘¿,ğ‘¾)Hğ‘¿ğ’(ğ‘¾)=â‡’ğ‘ƒ(ğ’€|ğ‘‘ğ‘œ(ğ‘¿),ğ‘‘ğ‘œ(ğ’),ğ‘¾)=ğ‘ƒ(ğ’€|ğ‘‘ğ‘œ(ğ‘¿),ğ‘¾)
where, ğ‘¼def=(ğ‘ˆ)for everyğ‘ˆâˆˆV(H), andğ‘(ğ‘Š)is the set of nodes
inğ‘that are not ancestors of any node in ğ‘Š.
Theorem 6.2 (Completeness of Do-Calculus in summary
causal DAGs). Let(H,ğ‘“)be a summary causal DAG for G, and let
ğ‘¿,ğ’€,ğ‘¾,ğ’âŠ†V(H) be disjoint sets of variables. If ğ’€isğ‘‘-connected to ğ’
inHğ‘¿w.r.t.ğ‘¿âˆªğ‘¾, then there exists a causal DAG Gâ€²compatible with
H, such thatğ‘“(ğ’€)isğ‘‘-connected to ğ‘“(ğ’)inGâ€²
ğ‘“(ğ‘¿)w.r.t.ğ‘“(ğ‘¿âˆªğ‘¾).
ATE Computation over Summary DAGs : We outline how to
compute ATE (see Section 2) directly on the summary DAG. If
the treatment or outcome is in a cluster node of H, we estimate
ğ´ğ‘‡ğ¸(ğ‘ˆ,ğ‘‰)over the canonical causal DAG GH. To minimize the
adjustment set, ğ‘ˆis ordered before all nodes in its cluster in GH.
Alternatively, an upper and lower bound can be derived by consid-
ering all subsets in ğ‘ˆâ€™s cluster inH.
7 ROBUSTNESS AGAINST DAG QUALITY
We evaluate the effectiveness of summary DAGs in providing ro-
bustness against a flawed input causal DAG. In a case study, we
demonstrate that the summary DAG facilitates the handling of er-
rors in the input DAG more effectively than directly examining the
causal DAG (which may be overwhelming to the user). This study
emphasizes that causal DAG summarization helps address quality
issues and increases robustness against misspecifications.
We revisit the Redshift causal DAG (Fig. 1). For each variable
pair, we consulted GPT-4 [ 70] about the edge presence and direction,
resulting in 55 detected edges. GPT-4 correctly identified 21 of the
23 original edges, inverted 1, and missed 1. It also generated 33
additional edges not in the original DAG. We will demonstrate how
causal DAG summarization can reduce the impact of these errors.
Missing Edges : Starting from the Redshift DAG (Fig. 1), we
remove the edge GPT-4 failed to detect ( Result Cache Hit â†’
Lock Wait Time ), marked in red in Fig. 6. As evident in Fig. 7a,
CaGreS produces the same summary DAG as in Fig. 2b. The in-
formation of which node in the cluster {Results Cache Hit ,
Exec. Time} has a directed edge to one of the nodes in the cluster
Causal DAG Summarization (Full Version)
Query
Template,
Ret. Rows,
Ret. Bytes,
Num
Columns
Num
Joins,
Num
TablesResult
Cache Hit,
Exec. Time
Plan
Time,
Lock Wait
TimeCompile
Time,
Elapsed
Time
(a) Summary After DeletionQuery
Template,
Num Joins,
Num Tables,
Num
ColumnsRet.
Rows,
Ret.
Bytes
Result
Cache Hit,
Lock Wait
Time
Plan
Time,
Compile
TimeExec.
Time,
Elapsed
Time
(b) Summary After Additions
Figure 7: 5-node summary DAGs after DAG modifications.
Table 2: Datasets
Dataset # Nodes (Variables) # Edges # Tuples
Redshift 12 23 9900
Flights 11 15 1M
Adult 13 48 32.5K
German 21 43 1000
Accidents 41 368 2.8M
Urls 60 310 1.7M
{Plan Time ,Lock Time} is lost upon summarization. Any causal
estimation performed over the summary DAG considers all possible
causal DAGs compatible with this summary DAG, including once
where the edge is included. Thus, the impact of this error is reduced.
Extraneous Edges : Starting again from the Redshift DAG, we
add 5 random edges from the set of redundant edges produced
by GPT-4, marked in blue in Fig. 6. These additional edges reduce
the number of CIsentailed by the DAG, which can hurt causal
inference accuracy. However, manually pruning extraneous edges
would require having the user check each of the (now 28) edges in
the DAG for correctness. If we instead summarize the DAG using
CaGreS withğ‘˜=5, the user is faced with the simpler, 9-edge
summary DAG shown in Fig. 7b. It is sufficient for the user to de-
tect the 2 suspicious orange edges among these 9 to discover 3 of
the 5 extraneous edges. The remaining 2 extraneous edges (from
Num Columns toPlan Time andLock Wait Time ) are subsumed
grouping Num Columns together with other, highly semantically
similar, query-related features. As such, graph summarization effec-
tively helps address extraneous edges by facilitating their detection.
8 EXPERIMENTAL EVALUATION
We empirically demonstrate the following claims: ( C1) Our sum-
mary DAGs support reliable causal inference. ( C2) Our objective
evaluation method effectively determines superior summary DAGs.
(C3)CaGreS outperforms other methods and achieves efficient
performance. ( C4) Our proposed optimizations help improve the
runtime of CaGreS without compromising quality.
8.1 Experimental Setting
All algorithms are implemented in Python 3.7. Causal effect com-
putation was performed using the DoWhy library [ 93]. The ex-
periments were executed on a PC with a 4.8GHz CPU, and 16GB
memory. Our code and datasets are available at [3].Datasets . We examine six datasets, as shown in Table 2. Five of the
datasets are publicly available, while Redshift was collected by run-
ning a publicly available benchmark on publicly available cloud re-
sources. We use the DAG in Fig. 1 for Redshift and build the causal
DAGs using [ 113] for the remaining datasets. Redshift : A dataset
collected by running queries from the TPC-DS benchmark [ 80] on
Amazon Redshift Serverless [ 8]. We execute 100 queries from the
query benchmark and retrieve the associated entries in the moni-
toring view [ 6].Flights [2]: a dataset describing domestic flight
statistics in the US. We enriched it with attributes describing the
weather, population, and the airline carriers. Adult [1]: a dataset
comprises demographic information of individuals including their
education, age, and income. German [10]: a dataset that contains
details of bank account holders, including demographic and finan-
cial information. Accidents [66]: This dataset includes key factors
influencing car accident severity, such as weather and traffic signs.
Urls [4]: a dataset containing descriptions of malicious and non-
malicious URLs. It encompasses properties such as URL length, the
number of digits, and the occurrence of sensitive words.
We also created synthetic data using the DoWhy package [ 93],
enabling manipulation of node count, edge count, and data size.
Baselines . We examine the following baselines: Brute-Force :
This algorithm implements an exhaustive search over all possible
summary DAGs that satisfy the constraint yielding the optimal
solution. k-Snap [103]: A general-purpose graph summarization
algorithm that employs bottom-up node contractions (akin to Ca-
GreS ). The primary distinction lies in the objective function: k-Snap
focuses on ensuring homogeneity among nodes within a cluster. We
have enhanced k-Snap to address acyclicity. Transit-Cluster
In [104], the authors proposed Transit Clusters as a specific type
of summary causal DAG that maintains identifiability properties
under certain conditions. They introduced an algorithm to identify
all transit clusters for a graph. For a fair comparison, we consider
the transit cluster that meets the constraints and has the maximal
RB.CIC [68] The authors of [ 68] proposed a Clustering Information
Criterion (CIC) that represents various complex interactions among
variables in a causal DAG. Based on this criterion, they developed
a greedy-based approach to learn clustered causal DAGs directly
from the data. Random : As a sanity check, this algorithm generates
a random summary DAG that adheres to the size constraint.
Metrics of evaluation . In some cases, summary DAGs are incom-
parable, meaning their RBs do not strictly imply one another. To as-
sess quality, we count additional edges in the canonical causal DAG
absent from the original DAGâ€”fewer edges indicate a sparser sum-
mary DAG encoding more CIs.
As a default configuration, we set the size constraint ğ‘˜toğ‘›
2,
whereğ‘›is the number of nodes in the input causal DAG. The
runtime cutoff was set at 1hour.
8.2 Usability Evaluation (C1)
8.2.1 The utility of the summary causal DAGs for causal inference.
We assess the utility of the summary causal DAGs for causal infer-
ence. To this end, we compare the causal effects estimated within
the original DAG with those computed within the summary causal
DAGs. Each causal effect estimation yields an interval (of 95% con-
fidence). We compare the intervals derived from the input DAG
Anna Zeng, Michael Cafarella, Batya Kenig, Markos Markakis, Brit Youngmann, and Babak Salimi
20406080percentage overlapBrute-Force
CaGreS
k-Snap
Random
Transit-Cluster
CIC
(a)Flights
20406080percentage overlap (b)German
Figure 8: Average percentage overlap with ground truth.
1030507090110130
Number of Nodes020406080100percentage overlap
CaGreS
k-Snap
Random
(a)
102103104105
Number of Tuples020406080100percentage overlap
 (b)
Figure 9: Average percentage overlap vs. data properties.
(the ground truth) with those obtained by the baselines. Given that
the adjustment sets in the summary DAGs may differ from those
in the original DAG, we anticipate getting different intervals.
Average Percentage Overlap : We report the average percentage
of overlap of the causal interval across all node pairs connected
by a causal path in the input DAG. A higher percentage overlap
indicates greater robustness in causal inference. The results for
Flights andGerman are shown in Fig. 8 (similar trends were
observed for the other datasets). CaGreS â€™s average percentage
overlap is close to that of Brute-Force , suggesting a high degree
of similarity between the two summary DAGs. CaGreS surpasses
all other competitors. This underscores the superior suitability of
CaGreS for causal inference compared to the baselines.
In what comes next, we use synthetic data, allowing us to manage
the number of nodes in the input DAG and database tuples. We
omit from presentation the Brute-Force ,Transit-Cluster , and
CICbaselines as they exceeded our time limit cutoff.
# of attributes : We examine how the number of nodes in the
input causal DAG affects the performance. With a larger number
of nodes, the task of finding the optimal summary DAG becomes
harder. Here, the number of data tuples is fixed at 10K. The results
are depicted in Fig. 9(a). For all baselines, with more data attributes,
their alignment with the input causal DAG diminishes. Nevertheless,
CaGreS consistently outperforms the competing methods.
# of tuples : We analyze the impact of data size on performance,
fixing the input causal DAG at 30 nodes. Since causal effects are
sensitive to sample size, we expect larger datasets to yield effects
on summary DAGs closer to those on the input DAG. As shown in
Fig. 9(b), small data sizes produce noisy results, while larger sizes
stabilize them. Again, CaGreS outperforms its competitors .
8.3 Quality Evaluation (C2)
When multiple summary DAGs achieve maximal RBs, we use three
metrics to identify a superior summary DAG: (1) the percentage
ofCIsin one summary DAGâ€™s RB implied by another; (2) number
of additional edges in the canonical causal DAG ; and (3) the size of
adjustment sets in causal estimation, with smaller sets enhancing
accuracy. As we show, these metrics are highly correlated.
10 30 50 70 90110 130 150
Number of Nodes020406080% of Implied CIsCaGreS -> k-Snap
k-Snap -> CaGreS(a) Higher bars are better
10 30 50 70 90110 130 150
Number of Nodes102103104# Additional EdgesCaGreS
k-Snap
Random (b) Lower bars are better
Figure 10: Quality metrics vs. the number of nodes.
Table 3: Pair-wise percentage of the RBâ€™s CIsimplied.
Brute-Force CaGreS k-Snap Random TC CIC
Brute-Force - 83.3% 50% 50% 16.6% 16.6%
CaGreS 50% - 60% 16.6% 0% 16%
k-Snap 0% 16.6% - 50% 16.6% 0%
Random 16.6% 0% 50% - 0% 16.6%
TC 0% 0% 16.6% 16.6% - 50%
CIC 0% 0% 0% 16.6% 0% -
We generated random causal DAGs with various number of
nodes (five DAGs for each node count), while keeping all other
parameters fixed. We omit from presentation the Brute-Force ,
Transit-Cluster , and CIC baselines as they exceeded our time
cutoff. The results are depicted in Fig. 10. Fig. 10(a) depicts the
percentage of CIsin the RB of k-Snap that are implied by that
ofCaGreS and vice versa. Similar trends were observed for Ran-
dom. A higher percentage of k-Snap â€™sCIsare implied by CaGreS
compared to the percentage of CaGreS â€™sCIsthat are implied by
k-Snap . Hence, while no RB entirely implies the other, we can still
conclude that the summary DAG of CaGreS is superior to that
ofk-Snap . Fig. 10(b) depicts the number of additional edges in
thecanonical causal DAG .CaGreS consistently yields summary
DAGs with fewer edges. We also considered the average size of the
adjustment sets in the computation of causal estimations (omitted
from the presentation). We report that CaGreS outperforms the
competitors, consistently yielding smaller adjustment sets. Since
these metrics are closely interrelated, we deduce that it is appropriate
to use the count of additional edges for comparing quality.
8.4 Effectiveness Evaluation (C3)
We assess CaGreS based on quality and runtime performance.
Case Study: Flights We present the pairwise percentage of the
CIsin the RB implied by all baseline pairs. The results are shown in
Table 3. The summary DAGs obtained by CaGreS andk-Snap are
given in Fig. 11 (The optimal summary DAG by Brute-Force is
omitted from presentation). Brute-Force yields the most effective
summary DAG, as it implies the highest percentage of CIsof any
other baseline. While 60% of the CIsofk-Snap are implied by the
RB of CaGreS , only 16% of the CIsofCaGreS are implied by the
RB of k-Snap . This superiority of CaGreS over k-Snap is further
supported by a lower number of additional edges (7 for CaGreS , 13
forBrute-Force , and 14 for k-Snap ). Intuitively, this stems from
k-Snap â€™s decision to form two 3-size clusters, connected by an edge.
In the resulting canonical causal DAG , every pair of nodes within
and between the clusters is connected by an edge.
Next, for each dataset, we report the runtime and the number of
additional edges. The results are depicted in Fig. 12. Only CaGreS ,
k-Snap , and Random can handle causal DAGs with more than 20
nodes within a responsive runtime. While Random andk-Snap
Causal DAG Summarization (Full Version)
State City
Humidity,
Prec.Temp.
DelayAirport,
Pop.,
Traffic
Airline,
Fleet
(a)CaGreSState City
Humidity Temp.
Prec.Airport,
Pop.,
Traffic
Airline,
Fleet,
Delay
(b)k-Snap
Figure 11: Summary causal DAGs for the Flights dataset.
exhibit runtimes comparable to that of CaGreS ,CaGreS consis-
tently produces summary DAGs with fewer additional edges. As
expected, Brute-Force outperforms CaGreS in terms of quality
but is impractical for interactive interaction. CICexhibits relatively
low performance, primarily due to a causal discovery component.
Transit-Cluster cannot handle large causal DAGs, as the algo-
rithm materializes all transit clusters to select the maximal one.
We next analyze the influence of different parameters on per-
formance. In these experiments, our focus shifts to synthetic data,
which enables us to manipulate data-related factors.
Input DAG size We vary the number of nodes in the input DAG
by generating a series of random DAGs varying the number of
nodes (5 DAGs per node count) and keeping all other parameters
constant. The results are shown in Fig. 13. As expected, k-Snap
andCaGreS exhibit a polynomial increase in runtime (Fig. 13(a)).
The improvement relative to k-Snap is attributed to our caching
mechanisms. CaGreS consistently generates summary DAGs with
fewer additional edges (Fig. 13(b)), indicating better quality.
Summary size We vary the size constraint ğ‘˜. Here, the node count
is set to 50, and the graph density is held constant at 0.3. The results
are depicted in Fig.14. The runtime of both CaGreS andk-Snap
demonstrate a linear increase with ğ‘˜(Fig. 14(a)). This is because
largerğ‘˜values necessitate more merges. As expected, CaGreS
manages to generate summary DAGs corresponding to canonical
causal DAGs with fewer edges (Fig. 14(b)).
Graph density We investigate the influence of graph density on
performance. We observe a nearly linear increase in runtime for
both CaGreS andk-Snap as graph density rises (Fig. 15(a)). This is
because both algorithms examine neighboring nodes of each node
pair, and higher density increases the number of neighbors. As
density increases, both algorithms add more edges. However, at
high densities (above 0.7), fewer edges remain to be added, so the
number of additional edges decreases (Fig. 15(b)).
Data size We report that the data size, i.e., number of tuples, has no
effect on the performance of CaGreS andk-Snap . This is because
both algorithms only examine the input causal DAGs.
8.5 Optimizations (C4)
We assess the effect of our optimizations on the performance of
CaGreS . To this end, we examine three variants of CaGreS : (I) No
Cache, a version of CaGreS without caching, (ii) No Preprocessing,
a version without the low-cost merge optimization, and (iii) No
Optimizations: a variant without either optimization.
We used synthetic data to generate causal DAGs, varying node
count and density as in Section 8.4. Figure 16 shows the results. Thenumber of additional edges remained constant across baselines, so
we omit that plot. This confirms that our optimizations enhance run-
time without sacrificing quality . Notably, caching improves runtime
nearly threefold , while preprocessing has a modest effect. For large,
dense DAGs, preprocessing may slightly slow the algorithm but
benefits smaller, sparser ones.
9 RELATED WORK
Summary Causal DAGs . The abstraction of causal models has
been studied in literature [ 91]. Previous work [ 12,72] investigated
the problem of determining under what assumptions DAGs over
sets of variables can represent the same CIs. The authors of [ 17,
18,86] explored the problem of determining the causes of a target
behavior (macro-variable) from micro-variables (e.g., image pixels).
Other works consider chain or ancestral causal graphs [ 46,114].
[81] presented a method to compress causal graphs by removing
nodes to eliminate redundancy. In contrast, our work addresses
causal DAG summarization, where some causal information is lost
but the summary DAG still supports reliable causal inference. The
authors of [ 9] expanded the do-calculus framework [ 74] to clustered
causal graphs, a related but distinct concept. Our contribution lies
in presenting a more streamlined proof of this principle, relying on
the connection between node contraction and edge addition.
General-Purpose Summary Graphs Graph summarization aims
to condense an input graph into a more concise representation.
Graph summarization has been extensively studied within the
data management community [ 14,25,50,54,67], as summary
graphs not only reduce the graphâ€™s size but also enable efficient
query answering [ 25,52,82,96], and enables enhanced data vi-
sualization and pattern discovery [ 22,24,41,44,48,94], and sup-
ports extraction of influence dynamics [ 58]. Various techniques
have been explored, including grouping nodes based on similarity
[47,50,62,67,82,96,97,99,103,110,116], reducing the number
bits required to represent graphs [ 15,54,67,83,92], and removing
unimportant nodes/edges [ 52,100]. We argue that existing tech-
niques are ill-suited for the causal DAG summarization problem.
Graph summarization objectives differ across applications, often
prioritizing minimizing the reconstruction error [ 47,110], facili-
tating accurate query answering [ 52,96], or selecting contractions
that preserve shortest paths to facilitate routing queries [ 34]. Con-
sequently, existing methods inadequately cater to the objective of
preserving causal information, often yielding graphs unsuitable
for causal inference, as shown in Section 1. Our algorithm follows
a previous line of work [ 34,103], where a bottom-up greedy ap-
proach is used to identify promising node pairs for contraction. Our
main contribution lies in how it estimates merge costs related to the
causal interpretation of the graph and the objective of preserving
causal information.
Causal Discovery . Causal discovery is a well-studied problem [ 35,
113,115], whose goal is to infer causal relationships among vari-
ables. While background knowledge is crucial [ 75], causal DAGs
can be inferred from data under certain assumptions [ 20,35]. Exist-
ing methods include constraint-based [ 101] and score-based algo-
rithms [ 20,95,108,119]. Pashami et al. [ 73] proposed a cluster-based
conflict resolution mechanism to determine the causal relationship
among variables. Recent works [ 16,106] have explored the use of
Anna Zeng, Michael Cafarella, Batya Kenig, Markos Markakis, Brit Youngmann, and Babak Salimi
Brute-Force
CaGreSk-Snap
RandomTransit-Cluster
CIC
10 15 20 25 30
# Additional Edges0123456Runtime (s)
(a)Redshift
7 10 13 16 19
# Additional Edges0123456Runtime (s) (b)Flights
30 35 40 45
# Additional Edges101
101103Runtime (s) (c)Adult
60 80 100 120
# Additional Edges0.000.010.020.030.040.05Runtime (s)
(d)German
0 100 200 300 400
# Additional Edges0.00.20.40.60.8Runtime (s) (e)Accidents
0 50 100 150 200 250
# Additional Edges0.00.20.40.60.81.01.21.4Runtime (s) (f)Urls
Figure 12: Number of additional edges vs. runtime. The optimal solution should be located in the lower right region.
020406080100120140
Number of Nodes0100020003000Runtime (s)
CaGreS
k-Snap
(a) Runtime
20406080100120140
Number of Nodes02000400060008000# Additional EdgesCaGreS
k-Snap (b) Quality
Figure 13: Number of nodes vs. running times and quality.
15 20 25 30 35 40 45
k024681012Runtime (s)
CaGreS
k-Snap
(a) Runtime
15202530354045
k503005508001050# Additional EdgesCaGreS
k-Snap (b) Quality
Figure 14: Summary size ğ‘˜vs. running times and quality.
0.1 0.3 0.5 0.7 0.9
Graph Density051015202530Runtime (s)
CaGreS
k-Snap
(a) Runtime
0.10.20.30.40.50.60.70.80.9
Graph Density02505007501000# Additional EdgesCaGreS
k-Snap (b) Quality
Figure 15: Graph density vs. running times and quality.
20 40 60 80 100 120
Number of Nodes (n)02004006008001000Runtime (s)
CaGreS
No Cache
No Preprocessing
No Optimizations
(a) Varying # nodes (density = 0.4)
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
Density0.40.60.81.01.21.4Runtime (s)
 (b) Varying density (n = 30)
Figure 16: Optimizations.
LLMs for causal discovery. Our work serves as a complementary
endeavor to existing research in causal discovery.10 CONCLUSIONS & LIMITATIONS
A mixed graph is a typical output of causal discovery algorithms [ 19,
77,101]. For simplicity in exposition, we concentrated on regular
causal DAGs throughout this paper. Nevertheless, our results and
algorithms apply to mixed graphs as well.
This paper opens up promising future research directions. This
includes the development of compact representations of node sets
tailored specifically for causal inference, addressing additional size
constraints, and refining algorithms with theoretical guarantees.
Lastly, we note that the size constraint can impact the generated
summary DAG, and users may need to adjust it to obtain a desirable
summary. Future research will explore methods to recommend an
optimal value for this parameter. For example, a heuristic stopping
condition could be added to the algorithm, signaling it to halt if the
next merge would result in a significant loss of information.
ACKNOWLEDGMENTS
We sincerely appreciate the support from DARPA ASKEM Award
HR00112220042, the ARPA-H Biomedical Data Fabric project, and
Liberty Mutual. We also want to thank the NSF Award IIS-2340124,
NIH Grant U54HG012510, and the Israel Science Foundation (ISF)
Grant No. 1442/24 for their partial funding of this work.
Causal DAG Summarization (Full Version)
REFERENCES
[1] 2016. Adult Census Income Dataset. https://www.kaggle.com/datasets/uciml/
adult-census-income. Accessed: 2024-04-04.
[2]2020. Kaggle Datasets: Flights Delay. https://www.kaggle.com/usdot/flight-
delays.
[3] 2024. Code Repository. https://github.com/TechnionTDK/causalens. Accessed:
2024-07-30.
[4]2024. Kaggle Datasets: malicious url detection. https://www.kaggle.com/
datasets/pilarpieiro/tabular-dataset-ready-for-malicious-url-detection. Ac-
cessed: 2024-04-04.
[5] 2024. OpenAI ChatGPT (3.5) [Large language model]. https://openai.com/blog/
chatgpt. Accessed: 2024-04-04.
[6]2024. SYS_QUERY_HISTORY - Amazon Redshift. https://docs.aws.amazon.
com/redshift/latest/dg/SYS_QUERY_HISTORY.html. Accessed: 2024-04-04.
[7] Abdullah Alomar, Pouya Hamadanian, Arash Nasr-Esfahany, Anish Agarwal,
Mohammad Alizadeh, and Devavrat Shah. 2023. CausalSim: A Causal Frame-
work for Unbiased Trace-Driven Simulation. In 20th USENIX Symposium on
Networked Systems Design and Implementation (NSDI 23) . 1115â€“1147.
[8] Amazon Web Services. 2024. Amazon Redshift Serverless. https://aws.amazon.
com/redshift/redshift-serverless/.
[9] Tara V Anand, Adele H Ribeiro, Jin Tian, and Elias Bareinboim. 2023. Causal
Effect Identification in Cluster DAGs. In Proceedings of the AAAI Conference on
Artificial Intelligence .
[10] Arthur Asuncion and David Newman. 2007. UCI machine learning repository.
[11] Sander Beckers. 2022. Causal explanations and XAI. In Conference on causal
learning and reasoning . PMLR, 90â€“109.
[12] Sander Beckers and Joseph Y Halpern. 2019. Abstracting causal models. In
Proceedings of the aaai conference on artificial intelligence , Vol. 33. 2678â€“2685.
[13] Leopoldo Bertossi and Babak Salimi. 2017. From causes for database queries to
repairs and model-based diagnosis and back. Theory of Computing Systems 61
(2017), 191â€“232.
[14] Sourav S Bhowmick and Byron Choi. 2022. Data-driven visual query inter-
faces for graphs: Past, present, and (near) future. In Proceedings of the 2022
International Conference on Management of Data . 2441â€“2447.
[15] Paolo Boldi and Sebastiano Vigna. 2004. The webgraph framework I: compres-
sion techniques. In Proceedings of the 13th international conference on World
Wide Web . 595â€“602.
[16] Alessandro Castelnovo, Riccardo Crupi, Fabio Mercorio, Mario Mezzanzanica,
Daniele PotertÃ¬, and Daniele Regoli. 2024. Marrying LLMs with Domain Expert
Validation for Causal Graph Generation. (2024).
[17] Krzysztof Chalupka, Frederick Eberhardt, and Pietro Perona. 2016. Multi-level
cause-effect systems. In Artificial intelligence and statistics . PMLR, 361â€“369.
[18] Krzysztof Chalupka, Pietro Perona, and Frederick Eberhardt. 2015. Visual causal
feature learning. In Proceedings of the Thirty-First Conference on Uncertainty in
Artificial Intelligence . AUAI Press, 181â€“190.
[19] Wenyu Chen, Mathias Drton, and Ali Shojaie. 2023. Causal Structural Learning
via Local Graphs. SIAM Journal on Mathematics of Data Science 5, 2 (2023),
280â€“305.
[20] D.M Chickering. 2002. Optimal structure identification with greedy search.
JMLR 3, Nov (2002), 507â€“554.
[21] Anthony C Constantinou, Yang Liu, Kiattikun Chobtham, Zhigao Guo, and
Neville K Kitson. 2021. Large-scale empirical validation of Bayesian Network
structure learning algorithms with noisy data. International Journal of Approxi-
mate Reasoning 131 (2021), 151â€“188.
[22] Diane J Cook and Lawrence B Holder. 1993. Substructure discovery using
minimum description length and background knowledge. Journal of Artificial
Intelligence Research 1 (1993), 231â€“255.
[23] Nilesh Dalvi and Dan Suciu. 2007. Efficient query evaluation on probabilistic
databases. The VLDB Journal 16 (2007), 523â€“544.
[24] Cody Dunne and Ben Shneiderman. 2013. Motif simplification: improving
network visualization readability with fan, connector, and clique glyphs. In
Proceedings of the SIGCHI Conference on Human Factors in Computing Systems .
3247â€“3256.
[25] Wenfei Fan, Jianzhong Li, Xin Wang, and Yinghui Wu. 2012. Query preserving
graph compression. In Proceedings of the 2012 ACM SIGMOD international
conference on management of data . 157â€“168.
[26] Sainyam Galhotra, Amir Gilad, Sudeepa Roy, and Babak Salimi. 2022. Hyper:
Hypothetical reasoning with what-if and how-to queries using a probabilistic
causal approach. In Proceedings of the 2022 International Conference on Manage-
ment of Data . 1598â€“1611.
[27] Sainyam Galhotra, Yue Gong, and Raul Castro Fernandez. 2023. Metam: Goal-
oriented data discovery. In 2023 IEEE 39th International Conference on Data
Engineering (ICDE) . IEEE, 2780â€“2793.
[28] Sainyam Galhotra, Romila Pradhan, and Babak Salimi. 2021. Explaining black-
box algorithms using probabilistic contrastive counterfactuals. In Proceedings
of the 2021 International Conference on Management of Data . 577â€“590.[29] Yu Gan, Mingyu Liang, Sundar Dev, David Lo, and Christina Delimitrou. 2021.
Sage: Practical and Scalable ML-Driven Performance Debugging in Microser-
vices. In Proceedings of the 26th ACM International Conference on Architectural
Support for Programming Languages and Operating Systems . 135â€“151.
[30] Markus Gangl. 2010. Causal inference in sociological research. Annual review
of sociology 36 (2010), 21â€“47.
[31] M. R. Garey, David S. Johnson, and Larry J. Stockmeyer. 1976. Some Simplified
NP-Complete Graph Problems. Theor. Comput. Sci. 1, 3 (1976), 237â€“267.
[32] Dan Geiger and Judea Pearl. 1988. On the logic of causal models. In UAI
â€™88: Proceedings of the Fourth Annual Conference on Uncertainty in Artificial
Intelligence, Minneapolis, MN, USA, July 10-12, 1988 . 3â€“14.
[33] Dan Geiger, Thomas Verma, and Judea Pearl. 1990. Identifying independence
in bayesian networks. Networks 20, 5 (1990), 507â€“534.
[34] Robert Geisberger, Peter Sanders, Dominik Schultes, and Christian Vetter. 2012.
Exact Routing in Large Road Networks Using Contraction Hierarchies. Trans-
portation Science 46, 3 (2012), 388â€“404.
[35] Clark Glymour, Kun Zhang, and Peter Spirtes. 2019. Review of causal discovery
methods based on graphical models. Frontiers in genetics 10 (2019), 524.
[36] Helga Gudmundsdottir, Babak Salimi, Magdalena Balazinska, Dan RK Ports, and
Dan Suciu. 2017. A demonstration of interactive analysis of performance mea-
surements with viska. In Proceedings of the 2017 ACM International Conference
on Management of Data . 1707â€“1710.
[37] SÃ©bastien Harispe, Sylvie Ranwez, Stefan Janaqi, and Jacky Montmain. 2015.
Semantic Similarity from Natural Language and Ontology Analysis. ArXiv
abs/1704.05295 (2015).
[38] Juris Hartmanis. 1982. Computers and intractability: a guide to the theory of
np-completeness (michael r. garey and david s. johnson). Siam Review 24, 1
(1982), 90.
[39] Weidong Huang, Peter Eades, and Seok-Hee Hong. 2009. Measuring effec-
tiveness of graph visualizations: A cognitive load perspective. Information
Visualization 8, 3 (2009), 139â€“152.
[40] Johannes Huegle, Christopher Hagedorn, Lukas Boehme, Mats Poerschke, Jonas
Umland, and Rainer Schlosser. 2021. MANM-CS: Data generation for bench-
marking causal structure learning from mixed discrete-continuous and nonlin-
ear data. WHY-21 at NeurIPS 2021 (2021).
[41] Lisa Jin and Danai Koutra. 2017. Ecoviz: Comparative vizualization of time-
evolving network summaries. In ACM SIGKDD 2017 Workshop on Interactive
Data Exploration and Analytics .
[42] Arijit Khan, Sourav S. Bhowmick, and Francesco Bonchi. 2017. Summarizing
Static and Dynamic Big Graphs. Proc. VLDB Endow. 10, 12 (2017), 1981â€“1984.
[43] Samantha Kleinberg and George Hripcsak. 2011. A review of causal inference
for biomedical informatics. Journal of biomedical informatics 44, 6 (2011), 1102â€“
1112.
[44] Danai Koutra, U Kang, Jilles Vreeken, and Christos Faloutsos. 2014. Vog: Sum-
marizing and understanding large graphs. In Proceedings of the 2014 SIAM
international conference on data mining . SIAM, 91â€“99.
[45] Oliver Kramer and Oliver Kramer. 2013. K-nearest neighbors. Dimensionality
reduction with unsupervised nearest neighbors (2013), 13â€“23.
[46] Steffen L Lauritzen and Thomas S Richardson. 2002. Chain graph models and
their causal interpretations. Journal of the Royal Statistical Society Series B:
Statistical Methodology 64, 3 (2002), 321â€“348.
[47] Kyuhan Lee, Hyeonsoo Jo, Jihoon Ko, Sungsu Lim, and Kijung Shin. 2020.
Ssumm: Sparse summarization of massive graphs. In Proceedings of the 26th
ACM SIGKDD International Conference on Knowledge Discovery & Data Mining .
144â€“154.
[48] Chenhui Li, George Baciu, and Yunzhe Wang. 2015. Modulgraph: modularity-
based visualization of massive graphs. In SIGGRAPH Asia 2015 Visualization in
High Performance Computing . 1â€“4.
[49] Aristidis Likas, Nikos Vlassis, and Jakob J Verbeek. 2003. The global k-means
clustering algorithm. Pattern recognition 36, 2 (2003), 451â€“461.
[50] Xingjie Liu, Yuanyuan Tian, Qi He, Wang-Chien Lee, and John McPherson. 2014.
Distributed graph summarization. In Proceedings of the 23rd ACM International
Conference on Conference on Information and Knowledge Management . 799â€“808.
[51] Yike Liu, Tara Safavi, Abhilash Dighe, and Danai Koutra. 2018. Graph summa-
rization methods and applications: A survey. ACM computing surveys (CSUR)
51, 3 (2018), 1â€“34.
[52] Antonio Maccioni and Daniel J Abadi. 2016. Scalable pattern matching over
compressed graphs via dedensification. In Proceedings of the 22nd ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining . 1755â€“1764.
[53] Sara Magliacane, Thijs Van Ommen, Tom Claassen, Stephan Bongers, Philip
Versteeg, and Joris M Mooij. 2018. Domain adaptation by using causal inference
to predict invariant conditional distributions. Advances in neural information
processing systems 31 (2018).
[54] Sebastian Maneth and Fabian Peternek. 2016. Compressing graphs by grammars.
In2016 IEEE 32nd International Conference on Data Engineering (ICDE) . IEEE,
109â€“120.
[55] Markos Markakis, An Bo Chen, Brit Youngmann, Trinity Gao, Ziyu Zhang,
Rana Shahout, Peter Baile Chen, Chunwei Liu, Ibrahim Sabek, and Michael
Anna Zeng, Michael Cafarella, Batya Kenig, Markos Markakis, Brit Youngmann, and Babak Salimi
Cafarella. 2024. Sawmill: From Logs to Causal Diagnosis of Large Systems. In
SIGMOD . 444â€“447.
[56] Markos Markakis, Brit Youngmann, Trinity Gao, Ziyu Zhang, Rana Shahout,
Peter Baile Chen, Chunwei Liu, Ibrahim Sabek, and Michael Cafarella. 2024.
From Logs to Causal Inference: Diagnosing Large Systems. Proceedings of the
VLDB Endowment 18, 2 (2024), 158â€“172.
[57] Markos Markakis, Ziyu Zhang, Rana Shahout, Trinity Gao, Chunwei Liu,
Ibrahim Sabek, and Michael Cafarella. 2024. Press ECCS to Doubt (Your
Causal Graph). In Proceedings of the Conference on Governance, Understand-
ing and Integration of Data for Effective and Responsible AI (Santiago, AA, Chile)
(GUIDE-AI â€™24) . Association for Computing Machinery, New York, NY, USA,
6â€“15. https://doi.org/10.1145/3665601.3669842
[58] Yasir Mehmood, Nicola Barbieri, Francesco Bonchi, and Antti Ukkonen. 2013.
Csi: Community-level social influence analysis. In Machine Learning and Knowl-
edge Discovery in Databases: European Conference, ECML PKDD 2013, Prague,
Czech Republic, September 23-27, 2013, Proceedings, Part II 13 . Springer, 48â€“63.
[59] Alexandra Meliou, Wolfgang Gatterbauer, Joseph Y Halpern, Christoph Koch,
Katherine F Moore, and Dan Suciu. 2010. Causality in databases. IEEE Data
Engineering Bulletin 33, 3 (2010), 59â€“67.
[60] Alexandra Meliou, Wolfgang Gatterbauer, Katherine F Moore, and Dan Suciu.
2009. Why so? or why no? functional causality for explaining query answers.
arXiv preprint arXiv:0912.5340 (2009).
[61] Alexandra Meliou, Sudeepa Roy, and Dan Suciu. 2014. Causality and explana-
tions in databases. Proceedings of the VLDB Endowment 7, 13 (2014), 1715â€“1716.
[62] Arpit Merchant, Michael Mathioudakis, and Yanhao Wang. 2023. Graph Summa-
rization via Node Grouping: A Spectral Algorithm. In Proceedings of the Sixteenth
ACM International Conference on Web Search and Data Mining . 742â€“750.
[63] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient esti-
mation of word representations in vector space. arXiv preprint arXiv:1301.3781
(2013).
[64] Tim Miller. 2019. Explanation in artificial intelligence: Insights from the social
sciences. Artificial intelligence 267 (2019), 1â€“38.
[65] Karthika Mohan, Judea Pearl, and Jin Tian. 2013. Graphical models for inference
with missing data. Advances in neural information processing systems 26 (2013).
[66] Sobhan Moosavi, Mohammad Hossein Samavatian, Srinivasan Parthasarathy,
Radu Teodorescu, and Rajiv Ramnath. 2019. Accident risk prediction based on
heterogeneous sparse data: New dataset and insights. In Proceedings of the 27th
ACM SIGSPATIAL international conference on advances in geographic information
systems . 33â€“42.
[67] Saket Navlakha, Rajeev Rastogi, and Nisheeth Shrivastava. 2008. Graph sum-
marization with bounded error. In Proceedings of the 2008 ACM SIGMOD inter-
national conference on Management of data . 419â€“432.
[68] Xueyan Niu, Xiaoyun Li, and Ping Li. 2022. Learning Cluster Causal Diagrams:
An Information-Theoretic Approach. (2022).
[69] Chris J Oates, Jessica Kasza, Julie A Simpson, and Andrew B Forbes. 2017. Repair
of partly misspecified causal diagrams. Epidemiology 28, 4 (2017), 548â€“552.
[70] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]
[71] RT Oâ€™donnell, Ann E Nicholson, B Han, Kevin B Korb, MJ Alam, and LR Hope.
2006. Incorporating expert elicited structural information in the CaMML causal
discovery program. In Proceedings of the 19th Australian Joint Conference on
Artificial Intelligence: Advances in Artificial Intelligence . 1â€“16.
[72] Pekka Parviainen and Samuel Kaski. 2016. Bayesian networks for variable
groups. In Conference on Probabilistic Graphical Models . PMLR, 380â€“391.
[73] Sepideh Pashami, Anders Holst, Juhee Bae, and SÅ‚awomir Nowaczyk. 2018.
Causal discovery using clusters from observational data. In FAIMâ€™18 Workshop
on CausalML, Stockholm, Sweden, July 15, 2018 .
[74] Judea Pearl. 2000. Causality : models, reasoning, and inference . Cambridge
University Press.
[75] J. Pearl and D. Mackenzie. 2018. The book of why: the new science of cause and
effect . Basic books.
[76] Sriram Pemmaraju, Steven Skiena, et al .2003. Computational discrete mathemat-
ics: Combinatorics and graph theory with mathematica Â®. Cambridge university
press.
[77] Jose M PeÃ±a. 2016. Learning acyclic directed mixed graphs from observations
and interventions. In Conference on Probabilistic Graphical Models . PMLR, 392â€“
402.
[78] Emilija Perkovic. 2020. Identifying causal effects in maximally oriented partially
directed acyclic graphs. In Conference on Uncertainty in Artificial Intelligence .
PMLR, 530â€“539.
[79] Alireza Pirhadi, Mohammad Hossein Moslemi, Alexander Cloninger, Mostafa
Milani, and Babak Salimi. 2024. Otclean: Data cleaning for conditional in-
dependence violations using optimal transport. Proceedings of the ACM on
Management of Data 2, 3 (2024), 1â€“26.
[80] Meikel Poess, Bryan Smith, Lubor Kollar, and Paul Larson. 2002. TPC-DS,
Taking Decision Support Benchmarking to the Next Level. In Proceedings of the
2002 ACM SIGMOD international conference on Management of data . 582â€“587.
[81] Cristina Puente, JosÃ© Angel Olivas, E Garrido, and R Seisdedos. 2013. Compress-
ing the representation of a causal graph. In 2013 Joint IFSA World Congress andNAFIPS Annual Meeting (IFSA/NAFIPS) . IEEE, 122â€“127.
[82] Sriram Raghavan and Hector Garcia-Molina. 2003. Representing web graphs.
InProceedings 19th International Conference on Data Engineering (Cat. No.
03CH37405) . IEEE, 405â€“416.
[83] Ryan A Rossi and Rong Zhou. 2018. Graphzip: a clique-based sparse graph
compression method. Journal of Big Data 5, 1 (2018), 1â€“14.
[84] Sudeepa Roy. 2022. Toward interpretable and actionable data analysis with
explanations and causality. Proc. VLDB Endow. 15, 12 (2022), 3812â€“3820.
[85] Sudeepa Roy and Dan Suciu. 2014. A formal approach to finding explanations
for database queries. In Proceedings of the 2014 ACM SIGMOD international
conference on Management of data . 1579â€“1590.
[86] Paul K Rubenstein, Sebastian Weichwald, Stephan Bongers, Joris M Mooij, Do-
minik Janzing, Moritz Grosse-Wentrup, and Bernhard SchÃ¶lkopf. 2017. Causal
consistency of structural equation models. arXiv preprint arXiv:1707.00819
(2017).
[87] Donald B Rubin. 2005. Causal inference using potential outcomes: Design,
modeling, decisions. J. Amer. Statist. Assoc. 100, 469 (2005), 322â€“331.
[88] Babak Salimi, Johannes Gehrke, and Dan Suciu. 2018. Bias in olap queries:
Detection, explanation, and removal. In SIGMOD . 1021â€“1035.
[89] Babak Salimi, Harsh Parikh, Moe Kayali, Lise Getoor, Sudeepa Roy, and Dan
Suciu. 2020. Causal relational learning. In Proceedings of the 2020 ACM SIGMOD
international conference on management of data . 241â€“256.
[90] Babak Salimi, Luke Rodriguez, Bill Howe, and Dan Suciu. 2019. Interventional
fairness: Causal database repair for algorithmic fairness. In Proceedings of the
2019 International Conference on Management of Data . 793â€“810.
[91] Bernhard SchÃ¶lkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke,
Nal Kalchbrenner, Anirudh Goyal, and Yoshua Bengio. 2021. Toward causal
representation learning. Proc. IEEE 109, 5 (2021), 612â€“634.
[92] Neil Shah, Danai Koutra, Lisa Jin, Tianmin Zou, Brian Gallagher, and Christos
Faloutsos. 2017. On Summarizing Large-Scale Dynamic Graphs. IEEE Data Eng.
Bull. 40, 3 (2017), 75â€“88.
[93] Amit Sharma and Emre Kiciman. 2020. DoWhy: An End-to-End Library for
Causal Inference. arXiv preprint arXiv:2011.04216 (2020).
[94] Zeqian Shen, Kwan-Liu Ma, and Tina Eliassi-Rad. 2006. Visual analysis of large
heterogeneous social networks by semantic and structural abstraction. IEEE
transactions on visualization and computer graphics 12, 6 (2006), 1427â€“1439.
[95] Shohei Shimizu, Patrik O Hoyer, Aapo HyvÃ¤rinen, Antti Kerminen, and Michael
Jordan. 2006. A linear non-Gaussian acyclic model for causal discovery. Journal
of Machine Learning Research 7, 10 (2006).
[96] Kijung Shin, Amol Ghoting, Myunghwan Kim, and Hema Raghavan. 2019. Sweg:
Lossless and lossy summarization of web-scale graphs. In The World Wide Web
Conference . 1679â€“1690.
[97] Maryam Shoaran, Alex Thomo, and Jens H Weber-Jahnke. 2013. Zero-
knowledge private graph summarization. In 2013 IEEE International Conference
on Big Data . IEEE, 597â€“605.
[98] Karamjit Singh, Garima Gupta, Vartika Tewari, and Gautam Shroff. 2018. Com-
parative benchmarking of causal discovery algorithms. In Proceedings of the
ACM India Joint International Conference on Data Science and Management of
Data . Association for Computing Machinery, 46â€“56.
[99] Qi Song, Yinghui Wu, Peng Lin, Luna Xin Dong, and Hui Sun. 2018. Mining
summaries for knowledge graph search. IEEE Transactions on Knowledge and
Data Engineering 30, 10 (2018), 1887â€“1900.
[100] Daniel A Spielman and Nikhil Srivastava. 2008. Graph sparsification by effective
resistances. In Proceedings of the fortieth annual ACM symposium on Theory of
computing . 563â€“568.
[101] P. Spirtes et al. 2000. Causation, prediction, and search . MIT press.
[102] Xinwei Sun, Botong Wu, Xiangyu Zheng, Chang Liu, Wei Chen, Tao Qin,
and Tie-Yan Liu. 2021. Recovering latent causal factor for generalization to
distributional shifts. Advances in Neural Information Processing Systems 34
(2021), 16846â€“16859.
[103] Yuanyuan Tian, Richard A Hankins, and Jignesh M Patel. 2008. Efficient ag-
gregation for graph summarization. In Proceedings of the 2008 ACM SIGMOD
international conference on Management of data . 567â€“580.
[104] Santtu Tikka, Jouni Helske, and Juha Karvanen. 2021. Clustering and Structural
Robustness in Causal Diagrams. arXiv preprint arXiv:2111.04513 (2021).
[105] Hal R Varian. 2016. Causal inference in economics and marketing. Proceedings
of the National Academy of Sciences 113, 27 (2016), 7310â€“7315.
[106] Aniket Vashishtha, Abbavaram Gowtham Reddy, Abhinav Kumar, Saketh Bachu,
Vineeth N Balasubramanian, and Amit Sharma. 2023. Causal inference using
llm-guided discovery. arXiv preprint arXiv:2310.15117 (2023).
[107] Thomas Verma and Judea Pearl. 1988. Causal networks: semantics and expres-
siveness. In UAI â€™88: Proceedings of the Fourth Annual Conference on Uncertainty
in Artificial Intelligence, Minneapolis, MN, USA, July 10-12, 1988 , Ross D. Shachter,
Tod S. Levitt, Laveen N. Kanal, and John F. Lemmer (Eds.). North-Holland, 69â€“
78.
[108] Marco A Wiering et al .2002. Evolving causal neural networks. In Benelearnâ€™02:
Proceedings of the Twelfth Belgian-Dutch Conference on Machine Learning . 103â€“
108.
Causal DAG Summarization (Full Version)
[109] Raymond W. Yeung. 2008. Information Theory and Network Coding (1 ed.).
Springer Publishing Company, Incorporated.
[110] Quinton Yong, Mahdi Hajiabadi, Venkatesh Srinivasan, and Alex Thomo. 2021.
Efficient graph summarization using weighted lsh at billion-scale. In Proceedings
of the 2021 International Conference on Management of Data . 2357â€“2365.
[111] Brit Youngmann, Michael Cafarella, Amir Gilad, and Sudeepa Roy. 2024. Sum-
marized Causal Explanations For Aggregate Views. Proceedings of the ACM on
Management of Data 2, 1 (2024), 1â€“27.
[112] Brit Youngmann, Michael Cafarella, Yuval Moskovitch, and Babak Salimi. 2023.
On Explaining Confounding Bias. In 2023 IEEE 39th International Conference on
Data Engineering (ICDE) . IEEE, 1846â€“1859.
[113] Brit Youngmann, Michael Cafarella, Babak Salimi, and Zeng Anna. 2023. Causal
Data Integration. Proceedings of the VLDB Endowment 16, 1- (2023), 2665â€“2659.
[114] Jiji Zhang. 2008. On the completeness of orientation rules for causal discovery
in the presence of latent confounders and selection bias. Artificial Intelligence
172, 16-17 (2008), 1873â€“1896.
[115] Boxiang Zhao, Shuliang Wang, Lianhua Chi, Qi Li, Xiaojia Liu, and Jing Geng.
2023. Causal Discovery via Causal Star Graphs. ACM Transactions on Knowledge
Discovery from Data 17, 7 (2023), 1â€“24.
[116] Yang Zhou, Hong Cheng, and Jeffrey Xu Yu. 2009. Graph clustering based on
structural/attribute similarities. Proceedings of the VLDB Endowment 2, 1 (2009),
718â€“729.
[117] Jiongli Zhu, Sainyam Galhotra, Nazanin Sabri, and Babak Salimi. 2023. Con-
sistent Range Approximation for Fair Predictive Modeling. Proceedings of the
VLDB Endowment 16, 11 (2023), 2925â€“2938.
[118] Jiongli Zhu and Babak Salimi. 2024. Overcoming Data Biases: Towards Enhanced
Accuracy and Reliability in Machine Learning. IEEE Data Eng. Bull. 47, 1 (2024),
18â€“35.
[119] Shengyu Zhu, Ignavier Ng, and Zhitang Chen. 2020. Causal Discovery with
Reinforcement Learning. In 8th International Conference on Learning Represen-
tations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net.
A SEMIGRAPHOID-AXIOMS
The semi-graphoid axioms are the following:
(1) Triviality: ğ¼(ğ´;âˆ…|ğ¶)=0.
(2) Symmetry: ğ¼(ğ´;ğµ|ğ¶)=0=â‡’ğ¼(ğ´;ğ¶|ğµ)=0.
(3) Decomposition: ğ¼(ğ´;ğµğ·|ğ¶)=0=â‡’ğ¼(ğ´;ğµ|ğ·)=0.
(4)Contraction: ğ¼(ğ´;ğµ|ğ¶)=0,ğ¼(ğ´;ğ·|ğµğ¶)=0=â‡’ğ¼(ğ´;ğµğ·|ğ¶)=0.
(5)Weak Union: ğ¼(ğ´;ğµğ·|ğ¶)=0=â‡’ğ¼(ğ´;ğµ|ğ¶ğ·)=0,ğ¼(ğ´;ğ·|ğµğ¶)=0.
The semi-graphoid axioms can be summarized using the following
identity, which follows from the chain-rule for mutual informa-
tion [109].
ğ¼(ğ´;ğµğ·|ğ¶)=0if and only if ğ¼(ğ´;ğµ|ğ¶)=0andğ¼(ğ´;ğ·|ğµğ¶)=0(5)
B PROOFS
Next, we provide the missing proofs.
We begin with some basic definitions used in the proofs.
LetGbe a causal DAG. and let ğ‘ˆ,ğ‘‰âˆˆV(G)be two nodes. We say
thatğ‘ˆis aparent ofğ‘‰, andğ‘‰achild ofğ‘ˆif(ğ‘ˆâ†’ğ‘‰)âˆˆE(G). A
directed path ğ‘¡=(ğ‘‰1,...,ğ‘‰ğ‘›)is a sequence of nodes such that there
is an edge(ğ‘‰ğ‘–â†’ğ‘‰ğ‘–+1)âˆˆE(G)for everyğ‘–âˆˆ{1,...,ğ‘›âˆ’1}. We say that
ğ‘‰is adescendant ofğ‘ˆ, andğ‘ˆanancestor ofğ‘‰if there is a directed
path fromğ‘ˆtoğ‘‰. We denote the child-nodes of ğ‘‰inGaschG(ğ‘‰)
; the descendants of ğ‘‰(we assume that ğ‘‰âˆˆDscG(ğ‘‰)) asDscG(ğ‘‰),
and the nodes ofGthat are not descendants of ğ‘‰asNDscG(ğ‘‰). For
a set of nodes SâŠ†V(G), we let DscG(S)def=Ã
ğ‘ˆâˆˆSDscG(ğ‘ˆ), and by
NDscG(S)def=Ã‘
ğ‘ˆâˆˆSNDscG(ğ‘ˆ).
Atrailğ‘¡=(ğ‘‰1,...,ğ‘‰ğ‘›)is a sequence of nodes such that there
is an edge between ğ‘‰ğ‘–andğ‘‰ğ‘–+1for everyğ‘–âˆˆ{1,...,ğ‘›âˆ’1}. That is,
(ğ‘‰ğ‘–â†’ğ‘‰ğ‘–+1)âˆˆE(G)or(ğ‘‰ğ‘–â†ğ‘‰ğ‘–+1)âˆˆE(G)for everyğ‘–âˆˆ{1,...,ğ‘›âˆ’1}. A
nodeğ‘‰ğ‘–is said to be head-to-head with respect to ğ‘¡if(ğ‘‰ğ‘–âˆ’1â†’ğ‘‰ğ‘–)âˆˆE(G)
and(ğ‘‰ğ‘–â†ğ‘‰ğ‘–+1)âˆˆE(G). A trailğ‘¡=(ğ‘‰1,...,ğ‘‰ğ‘›)isactive given ZâŠ†V
if (1) everyğ‘‰ğ‘–that is a head-to-head node with respect to ğ‘¡eitherbelongs to Zor has a descendant in Z, and (2) every ğ‘‰ğ‘–that is not
a head-to-head node w.r.t. ğ‘¡does not belong to Z. If a trailğ‘¡is not
active given Z, then it is blocked given Z.
B.1 Proofs for Section 3
Proof of Lemma 3.1. Letğ‘ƒbe a directed path from ğ´toğµ, such
that|ğ‘ƒ|â‰¥2. Letğ‘‹beğ´â€™s successor in ğ‘ƒ, andğ‘Œbeğµâ€™s predecessor
inğ‘ƒ. By our assumption that |ğ‘ƒ|â‰¥2,ğ‘‹âˆ‰{ğ´,ğµ}, butğ‘Œmay be the
same asğ‘‹. Now, consider the graph ğ». By definition, ğ»contains a
nodeğ´ğµ, with an incoming edge from ğ‘Œ, and an outgoing edge to ğ‘‹.
Ifğ‘‹=ğ‘Œ, we immediately get the cycle ğ´ğµâ†’ğ‘‹â†’ğ´ğµ. Otherwise,
we consider the subpath ğ‘ƒâ€²(ofğ‘ƒ) fromğ‘‹toğ‘Œ(ğ‘‹â‡
ğ‘ƒâ€²ğ‘Œ) inğº. This
results in the following cycle in ğ»:ğ‘Œâ†’ğ´ğµâ†’ğ‘‹â‡
ğ‘ƒâ€²ğ‘Œ.
Now, suppose that ğ»contains the cycle ğ¶.ğ¶must contain the
nodeğ´ğµ. Otherwise, the cycle is included in ğº, which leads to a
contradiction that ğºis a DAG. Let ğ‘Œandğ‘‹be the incoming and
outgoing vertices, respectively, to ğ´ğµinğ¶. Then, there is a directed
pathğ‘ƒfromğ‘‹toğ‘Œinğ»that avoidsğ´ğµ. That is, every vertex and
edge on the path ğ‘ƒbelongs toğºas well. Hence, ğ‘ƒis a directed path
fromğ‘‹toğ‘Œinğº. Sinceğ‘Œis incoming to ğ´ğµ, thenğ‘Œis incoming to
eitherğ´orğµinğº. Assume, wlog, that ğ‘Œâ†’ğ´âˆˆğ¸. Sinceğ‘‹is an
outgoing vertex from ğ´ğµ, then it is outgoing from from either ğ´
orğµ(or both). If ğ‘‹is outgoing from ğ´, then we get the following
cycle inğº:ğ‘Œâ†’ğ´â†’ğ‘‹â‡
ğ‘ƒğ‘Œ. Sinceğºis a DAG, this brings us to
a contradiction. Therefore, ğ‘‹must be outgoing from ğµand notğ´.
But this gives us the following directed path from ğµtoğ´:
ğ´â†ğ‘Œf
ğ‘ƒğ‘‹â†ğµ.
This completes the proof. â–¡
Next, we show that the causal DAG summarization problem is
ğ‘ğ‘ƒ-hard via a reduction from the ğ‘˜-Max-Cut problem [ 31]. Letğº
be an undirected graph with weighted edges (i.e., ğ‘¤:V(ğº)â†’Râ‰¥0).
Theğ‘˜-Max-Cut problem consists of partitioning V(ğº)intoğ‘˜disjoint
clusters so as to maximize the sum of weights of the edges joining
vertices in different clusters. It is well-known that ğ‘˜-Max-Cut is
NP-hard even if ğ‘˜=2[31], and the weight of every edge is 1. In
other words, deciding whether there exists a ğ‘˜-clustering of V(ğº)
to clusters{ğ‘‰1,...,ğ‘‰ğ‘˜}, where the sum of weights of edges between
vertices in distinct clusters is at least a given threshold ğ›¾is NP-hard.
LetSdef={ğ‘‰1,...,ğ‘‰ğ‘˜}be ağ‘˜-clustering of ğº. We define ğ‘€ğº(S)
to be the number of edges of between vertices in a common cluster:
ğ‘€ğº(S)def=ğ‘˜âˆ‘ï¸
ğ‘–=1âˆ‘ï¸
ğ‘¢,ğ‘£âˆˆğ‘‰ğ‘–1[(ğ‘¢,ğ‘£)âˆˆE(ğº)]. (6)
We defineğ‘€ğº(S)to be the number of non-edges between vertices
in a common cluster:
ğ‘€ğº(S)def=ğ‘˜âˆ‘ï¸
ğ‘–=1âˆ‘ï¸
ğ‘¢,ğ‘£âˆˆğ‘‰ğ‘–1[(ğ‘¢,ğ‘£)âˆ‰E(ğº)]. (7)
Similarly, we define ğµğº(S)to be the number of edges of between
vertices in distinct clusters:
ğµğº(S)def=âˆ‘ï¸
1â‰¤ğ‘–<ğ‘—â‰¤ğ‘˜âˆ‘ï¸
ğ‘¢âˆˆğ‘‰ğ‘–,
ğ‘£âˆˆğ‘‰ğ‘—1[(ğ‘¢,ğ‘£)âˆˆE(ğº)]. (8)
Anna Zeng, Michael Cafarella, Batya Kenig, Markos Markakis, Brit Youngmann, and Babak Salimi
Since every edge(ğ‘¢,ğ‘£)âˆˆE(ğº)is either between vertices in the
same cluster or vertices in distinct clusters, then:
ğ‘€ğº(S)+ğµğº(S)=|E(ğº)|.
In particular, ğ‘€ğº(S)â‰¤ğœif and only if ğµğº(S)â‰¥| E(ğº)|âˆ’ğœ, for
everyğœâˆˆ[0,|E(ğº)|].
Lemma B.1. Letğºbe an undirected graph, ğ‘˜â‰¥2, andğœâˆˆ
[0,|E(ğº)|]. Deciding whether there exists a ğ‘˜-clusteringSofğºsuch
thatğ‘€ğº(S)â‰¤ğœis NP-Complete.
Proof. The problem is clearly in NP because given a ğ‘˜-clustering
S, computing ğ‘€ğº(S)(see (6)) can be done in polynomial time.
We prove hardness by reduction from ğ‘˜-Max-Cut. Suppose there
exists ağ‘ƒ-Time algorithm that given an undirected graph ğº, and a
threshold value ğœâˆˆ[0,|E(ğº)|], returns ağ‘˜-clusteringSsuch that
ğ‘€ğº(S)â‰¤ğœif one exists, and null otherwise. We show that such an
algorithm can be applied to solve ğ‘˜-Max-Cut in polynomial time.
Letğº,ğ‘˜, andğ›¾be an instance of the ğ‘˜-max-Cut problem where ğº
is an undirected graph, ğ‘˜â‰¤|V(ğº)|, andğ›¾âˆˆ[0,|E(ğº)|]. We define
ğœdef=|E(ğº)|âˆ’ğ›¾, and execute the algorithm for finding a ğ‘˜-clustering
Ssuch thatğ‘€ğº(S)â‰¤ğœ. Sinceğ‘€ğº(S)+ğµğº(S)=|E(ğº)|, then:
ğµğº(S)=|E(ğº)|âˆ’ğ‘€ğº(S) â‡’ ğ‘€ğº(S)â‰¤ğœ
â‰¥|E(ğº)|âˆ’ğœ â‡’ğœdef=|E(ğº)|âˆ’ğ›¾
=|E(ğº)|âˆ’(| E(ğº)|âˆ’ğ›¾)
=ğ›¾
Hence, we can decide in ğ‘ƒ-Time whether ğºhas ağ‘˜-cut whose
cardinality is at least ğ›¾. â–¡
Lemma B.2. Letğºbe an undirected graph, ğ‘˜â‰¥2, andğœâˆˆ
[0,|E(ğº)|]. Deciding whether there exists a ğ‘˜-clusteringSofğºsuch
thatğ‘€ğº(S)â‰¤ğœis NP-Complete.
Proof. The problem is clearly in NP because given a ğ‘˜-clustering
S, computing ğ‘€ğº(S), and verifying ğ‘€ğº(S)â‰¤ğœcan be done in
polynomial time (see (7)).
So, suppose that there exists a ğ‘ƒ-Time algorithm that given an
undirected graph ğº, and a threshold value ğœâˆˆ[0,|E(ğº)|], returns
ağ‘˜-clusteringSsuch thatğ‘€ğº(S)â‰¤ğœif one exists, and null oth-
erwise. We show that such an algorithm can be applied to decide
whether there exists a ğ‘˜-clustering of ğºwhereğ‘€ğº(S)â‰¤ğ›¼. By
Lemma B.1, this problem is NP-Hard, and hence this will prove that
minimizing ğ‘€ğº(S)is NP-hard as well.
So letğº,ğ‘˜, andğ›¼be the input to the problem for deciding whether
there exists a ğ‘˜-clusteringSsuch thatğ‘€ğº(S)â‰¤ğ›¼. Letğºdenote
the complement graph of ğº. That is V(ğº)=V(ğº)and(ğ‘¢,ğ‘£)âˆˆE(ğº)
if and only if(ğ‘¢,ğ‘£)âˆ‰E(ğº). In particular, ğ‘€ğº(S)=ğ›¼if and only
ifğ‘€ğº(S)=ğ›¼. Therefore, if we can, in ğ‘ƒ-Time find a clustering
that minimizes ğ‘€ğº(S), then we have also found a clustering that
minimizesğ‘€ğº(S). This completes the proof. â–¡
We now show that the causal DAG summarization problem is
ğ‘ğ‘ƒ-hard. Specifically, we show that finding a summary DAG (H,ğ‘“)
whose canonical DAG Gminimizes|E(GH)|âˆ’| E(ğº)|; that is, the
canonical DAGGHresults in the smallest number of added edges,
is NP-Hard.Theorem B.3. The causal DAG summarization problem which
minimizes the number of added edges to the canonical DAG is NP-
Hard.
Proof. Given a DAG ğ·and ağ‘˜-clusteringSofğ·, it is straight-
forward to verify that ğ‘€ğ·(S)â‰¤ğœ, and hence this problem is in
NP.
Letğºbe an undirected graph, ğ‘˜>0, andğ›¾>0a threshold. Let
V(ğº)={ğ‘£1,...,ğ‘£ğ‘›}denote a complete order of V(ğº). Defineğ·to
be the directed graph where V(ğ·)=V(ğº)and(ğ‘£ğ‘–â†’ğ‘£ğ‘—)âˆˆE(ğ·)if
and only if(ğ‘£ğ‘¢,ğ‘£ğ‘—)âˆˆE(ğº)andğ‘–<ğ‘—. LetS={ğ‘‰1,...,ğ‘‰ğ‘˜}be ağ‘˜-
clustering of V(ğ·)=V(ğº)such thatğ‘€ğ·(S)â‰¤ğ›¾. By the definition
ofE(ğ·), we immediately get that ğ‘€ğº(S)â‰¤ğ›¾. By Lemma B.2, the
causal DAG summarization problem is ğ‘ğ‘ƒ-hard. â–¡
B.2 Proofs for Section 4
Next, we prove some simple lemmas that will be useful later on.
We denote byHğ‘ˆğ‘‰the summary DAG where nodes ğ‘ˆandğ‘‰are
contracted.
Lemma B.4. The following holds:
ğœ‹Hğ‘ˆğ‘‰(XUV)=ğœ‹Gğ‘ˆğ‘‰(ğ‘ˆ)ğœ‹Gğ‘ˆğ‘‰(ğ‘‰)=ğœ‹Gğ‘ˆğ‘‰(ğ‘ˆ)âˆª{ğ‘ˆ}(9)
chHğ‘ˆğ‘‰(XUV)=chGğ‘ˆğ‘‰(ğ‘‰) chGğ‘ˆğ‘‰(ğ‘ˆ)=chGğ‘ˆğ‘‰(ğ‘‰)âˆª{ğ‘‰}
(10)
NDscHğ‘ˆğ‘‰(XUV)=NDscGğ‘ˆğ‘‰(ğ‘ˆ)NDscGğ‘ˆğ‘‰(ğ‘‰)=NDscGğ‘ˆğ‘‰(ğ‘ˆ)âˆª{ğ‘ˆ}
(11)
Lemma B.5. Letğ‘‡âˆˆV such thatğ‘‡âˆ‰{ğ‘ˆ,ğ‘‰}âˆªchG(ğ‘ˆ)âˆªchG(ğ‘‰).
Then it holds that:
ğœ‹Gğ‘ˆğ‘‰(ğ‘‡)=ğœ‹Hğ‘ˆğ‘‰(ğ‘‡)and (12)
NDscGğ‘ˆğ‘‰(ğ‘‡)\{ğ‘ˆğ‘‰}=NDscH(ğ‘‡)\{XUV}and (13)
{ğ‘ˆ,ğ‘‰}âŠ†NDscGğ‘ˆğ‘‰(ğ‘‡)if and only if XUVâˆˆNDscH(ğ‘‡) (14)
Now, letğ‘‡âˆˆV such thatğ‘‡âˆ‰{ğ‘ˆ,ğ‘‰}âˆªğœ‹G(ğ‘ˆ)âˆªğœ‹G(ğ‘‰). Then:
chGğ‘ˆğ‘‰(ğ‘‡)=chHğ‘ˆğ‘‰(ğ‘‡)and (15)
DscGğ‘ˆğ‘‰(ğ‘‡)\{ğ‘ˆ,ğ‘‰}=DscHğ‘ˆğ‘‰(ğ‘‡)\{XUV} (16)
{ğ‘ˆ,ğ‘‰}âŠ†DscGğ‘ˆğ‘‰(ğ‘‡)if and only if XUVâˆˆDscHğ‘ˆğ‘‰(ğ‘‡) (17)
proof of Lemma B.4. By the definition of ğºâ€², it holds that ğœ‹ğºâ€²(ğ‘¢)=
ğœ‹ğº(ğ‘¢)âˆªğœ‹ğº(ğ‘£). Since(ğ‘¢,ğ‘£)âˆˆE(ğºâ€²), thenğœ‹ğºâ€²(ğ‘£)=ğœ‹ğºâ€²(ğ‘¢)âˆª{ğ‘¢}.
Similarly, chğºâ€²(ğ‘£)=chğº(ğ‘¢)âˆªchğº(ğ‘£), and since(ğ‘¢,ğ‘£)âˆˆE(ğºâ€²),
then chğºâ€²(ğ‘¢)=chğºâ€²(ğ‘£)âˆª{ğ‘£}. By definition of edge contraction,
it holds that ğœ‹ğ»(ğ‘‹ğ‘¢ğ‘£)=ğœ‹ğº(ğ‘¢)âˆªğœ‹ğº(ğ‘£)=ğœ‹ğºâ€²(ğ‘¢), proving (9).
Also, by definition of edge contraction, it holds that chğ»(ğ‘‹ğ‘¢ğ‘£)=
chğº(ğ‘¢)âˆªchğº(ğ‘£)=chğºâ€²(ğ‘£), proving (10).
We now prove (11). Letğ‘¡âˆˆNDscğ»(ğ‘‹ğ‘¢ğ‘£). Ifğ‘¡âˆ‰NDscğºâ€²(ğ‘¢), then
ğ‘¡âˆˆDscğºâ€²(ğ‘¢)\{ğ‘£}. This means that there is a directed path ğ‘ƒfrom
ğ‘¢toğ‘¡inğºâ€². Letğ‘ be the first vertex on this path (after ğ‘¢). Since
ğ‘ âˆˆchğºâ€²(ğ‘¢)\{ğ‘£}, then by the definition of ğºâ€²,ğ‘ âˆˆchğº(ğ‘¢)âˆªchğº(ğ‘£).
By the definition of edge contraction, ğ‘ âˆˆchğ»(ğ‘‹ğ‘¢ğ‘£). Sinceğ‘ âˆ‰
ğ‘¢ğ‘£âˆªğœ‹ğº(ğ‘¢)âˆªğœ‹ğº(ğ‘£), then every directed path starting at ğ‘ inğº
remains a directed path in ğ». But this means that there is a directed
path fromğ‘‹ğ‘¢ğ‘£toğ‘¡(viağ‘ ); contradicting the assumption that ğ‘¡âˆˆ
NDscğ»(ğ‘‹ğ‘¢ğ‘£). Now, letğ‘¡âˆˆNDscğºâ€²(ğ‘¢). Ifğ‘¡âˆ‰NDscğ»(ğ‘‹ğ‘¢ğ‘£), thenğ‘¡âˆˆ
Dscğ»(ğ‘‹ğ‘¢ğ‘£)\{ğ‘‹ğ‘¢ğ‘£}. This means that there is a directed path ğ‘ƒfrom
ğ‘‹ğ‘¢ğ‘£toğ‘¡inğ». Letğ‘ be the first vertex on this path (after ğ‘‹ğ‘¢ğ‘£). Since
Causal DAG Summarization (Full Version)
ğ‘ âˆˆchğ»(ğ‘‹ğ‘¢ğ‘£), then by the definition of ğ»,ğ‘ âˆˆchğº(ğ‘¢)âˆªchğº(ğ‘£).
But then, by the definition of ğºâ€², it holds that ğ‘ âˆˆchğºâ€²(ğ‘¢). Since
no edges are removed by the transition from ğºtoğºâ€², there is a
directed path from ğ‘¢to (viağ‘ ) inğºâ€²; contradicting the assumption
thatğ‘¡âˆˆNDscğºâ€²(ğ‘¢). â–¡
Proof of Lemma B.5. By the definition of contraction, the only
vertices inğºwhose parent-set can potentially change following the
contraction of ğ‘¢andğ‘£belong to the set ğ‘¢ğ‘£âˆªchğº(ğ‘¢)âˆªchğº(ğ‘£). By
the definition of E(ğºâ€²), the only vertices in ğºwhose parent-set can
potentially change belong to the set ğ‘¢ğ‘£âˆªchğº(ğ‘¢)âˆªchğº(ğ‘£). Therefore,
ifğ‘¡âˆ‰ğ‘¢ğ‘£âˆªchğº(ğ‘¢)âˆªchğº(ğ‘£), thenğœ‹ğºâ€²(ğ‘¡)=ğœ‹ğ»(ğ‘¡)=ğœ‹ğº(ğ‘¡). This
proves (12).
By the definition of contraction, the only vertices in ğºwhose
child-set can potentially change following the contraction of ğ‘¢and
ğ‘£belong to the set ğ‘¢ğ‘£âˆªğœ‹ğº(ğ‘¢)âˆªğœ‹ğº(ğ‘£). By the definition of E(ğºâ€²),
the only vertices in ğºwhose child-set can potentially change belong
to the setğ‘¢ğ‘£âˆªğœ‹ğº(ğ‘¢)âˆªğœ‹ğº(ğ‘£). Therefore, if ğ‘¡âˆ‰ğ‘¢ğ‘£âˆªğœ‹ğº(ğ‘¢)âˆªğœ‹ğº(ğ‘£),
then chğºâ€²(ğ‘¡)=chğ»(ğ‘¡)=chğº(ğ‘¡). This proves (15).
We now prove (13); Letğ‘ âˆˆNDscğ»(ğ‘¡)\{ğ‘‹ğ‘¢ğ‘£}. Ifğ‘ âˆ‰NDscğºâ€²(ğ‘¡),
thenğ‘ âˆˆDscğºâ€²(ğ‘¡). That is, there is a directed path ğ‘ƒfromğ‘¡toğ‘ inğºâ€².
Let us assume wlog that ğ‘ƒis the shortest directed path from ğ‘¡toğ‘ in
ğºâ€². By this assumption, exactly one of the following holds: (1) ğ‘¢,ğ‘£âˆ‰
V(ğ‘ƒ)(2)ğ‘¢âˆˆV(ğ‘ƒ),ğ‘£âˆ‰ğ‘›ğ‘œğ‘‘ğ‘’ğ‘ (ğ‘ƒ)(3)ğ‘£âˆˆV(ğ‘ƒ),ğ‘¢âˆ‰ğ‘›ğ‘œğ‘‘ğ‘’ğ‘ (ğ‘ƒ), or (4)
(ğ‘¢,ğ‘£)âˆˆE(ğ‘ƒ). In the first case, every edge of ğ‘ƒis also an edge of E(ğº),
that does not enter or exit {ğ‘¢,ğ‘£}. Therefore, ğ‘ƒis a directed path in
ğ», a contradiction. In case (2), since ğœ‹ğºâ€²(ğ‘¢)=ğœ‹ğ»(ğ‘‹ğ‘¢ğ‘£)(see (9)),
andchğºâ€²(ğ‘¢)=chğ»(ğ‘‹ğ‘¢ğ‘£)\{ğ‘£}(see(10)), then the path with nodes
ğ‘‹ğ‘¢ğ‘£âˆª(V(ğ‘ƒ)\{ğ‘¢}), is a directed path in ğ»fromğ‘ toğ‘¡; a contradiction.
In case (3), since chğºâ€²(ğ‘£)=chğ»(ğ‘‹ğ‘¢ğ‘£)(see (10)), andğœ‹ğºâ€²(ğ‘£)=
ğœ‹ğ»(ğ‘‹ğ‘¢ğ‘£)\{ğ‘¢}(see(9)), then the path with nodes ğ‘‹ğ‘¢ğ‘£âˆª(V(ğ‘ƒ)\{ğ‘£}),
is a directed path in ğ»fromğ‘ toğ‘¡; a contradiction. Finally, if (ğ‘¢,ğ‘£)âˆˆ
E(ğ‘ƒ), then sinceğœ‹ğºâ€²(ğ‘¢)=ğœ‹ğ»(ğ‘‹ğ‘¢ğ‘£)andchğºâ€²(ğ‘£)=chğ»(ğ‘‹ğ‘¢ğ‘£), then
the path with nodes ğ‘‹ğ‘¢ğ‘£âˆª(V(ğ‘ƒ)\ğ‘¢ğ‘£), is a directed path from ğ‘ toğ‘¡
inğ»; a contradiction. For the other direction, let ğ‘ âˆˆNDscğºâ€²(ğ‘¡)\ğ‘¢ğ‘£.
Ifğ‘ âˆ‰NDscğ»(ğ‘¡), then there is a directed path ğ‘ƒfromğ‘¡toğ‘ in
ğ». Ifğ‘‹ğ‘¢ğ‘£âˆ‰V(ğ‘ƒ), then E(ğ‘ƒ) âŠ† E(ğº) âŠ† E(ğºâ€²), and hence ğ‘ƒis a
directed path from ğ‘¡toğ‘ inğºâ€². Otherwise, if ğ‘‹ğ‘¢ğ‘£âˆˆV(ğ‘ƒ), then since
ğœ‹ğ»(ğ‘‹ğ‘¢ğ‘£)=ğœ‹ğºâ€²(ğ‘¢),chğ»(ğ‘‹ğ‘¢ğ‘£)=chğºâ€²(ğ‘£), and(ğ‘¢,ğ‘£)âˆˆE(ğºâ€²), then
replacingğ‘‹ğ‘¢ğ‘£, with the edge(ğ‘¢,ğ‘£)results in a directed ğ‘¡,ğ‘ -path in
ğºâ€²; a contradiction. â–¡
Proof of Theorem 4.1. We first prove that Î£RB(ğºâ€²)=â‡’Î£RB(ğ»).
We divide to cases. Let (ğ‘‹ğ‘–;ğµğ‘–|ğœ‹ğ»(ğ‘‹ğ‘–))âˆˆÎ£RB(ğ»), whereğ‘‹ğ‘¢ğ‘£âˆ‰
ğµğ‘–âˆªğœ‹ğ»(ğ‘‹ğ‘–)âˆª{ğ‘‹ğ‘–}. In particular, ğ‘‹ğ‘–âˆˆV(ğº), andğ‘‹ğ‘–âˆ‰{ğ‘¢,ğ‘£}âˆª
chğ»(ğ‘‹ğ‘¢ğ‘£)={ğ‘¢,ğ‘£}âˆªchğº(ğ‘¢)âˆªchğº(ğ‘£). By (13), we have that
ğœ‹ğºâ€²(ğ‘‹ğ‘–)=ğœ‹ğ»(ğ‘‹ğ‘–), and that NDscğºâ€²(ğ‘‹ğ‘–)\ğ‘¢ğ‘£=NDscğ»(ğ‘‹ğ‘–)\ğ‘‹ğ‘¢ğ‘£.
Therefore, we have that (ğ‘‹ğ‘–;NDscğ»(ğ‘‹ğ‘–)\{ğ‘‹ğ‘¢ğ‘£}|ğœ‹ğºâ€²(ğ‘‹ğ‘–))ğºâ€². Since
ğµğ‘–âŠ†NDscğ»(ğ‘‹ğ‘–)\{ğ‘‹ğ‘¢ğ‘£}, then, by decomposition, we have that
Î£RB(ğºâ€²)=â‡’ (ğ‘‹ğ‘–;ğµğ‘–|ğœ‹ğ»(ğ‘‹ğ‘–)).
Now, let(ğ‘‹ğ‘–;ğ‘‹ğ‘¢ğ‘£ğµğ‘–|ğœ‹ğ»(ğ‘‹ğ‘–)) âˆˆ Î£RB(ğ»). In this case as well
ğ‘‹ğ‘–âˆˆV(ğº), andğ‘‹ğ‘–âˆ‰{ğ‘¢,ğ‘£}âˆªchğ»(ğ‘‹ğ‘¢ğ‘£)={ğ‘¢,ğ‘£}âˆªchğº(ğ‘¢)âˆªchğº(ğ‘£).
By(13), we have that ğœ‹ğºâ€²(ğ‘‹ğ‘–)=ğœ‹ğ»(ğ‘‹ğ‘–), and that NDscğºâ€²(ğ‘‹ğ‘–)\ğ‘¢ğ‘£=
NDscğ»(ğ‘‹ğ‘–)\ğ‘‹ğ‘¢ğ‘£. By , we have that ğ‘‹ğ‘¢ğ‘£âˆˆNDscğ»(ğ‘‹ğ‘–)iffğ‘¢ğ‘£âŠ†
NDscğºâ€²(ğ‘‹ğ‘–). Therefore,ğµğ‘–ğ‘‹ğ‘¢ğ‘£âŠ†NDscğ»(ğ‘‹ğ‘–)iffğµğ‘–ğ‘¢ğ‘£âŠ†NDscğºâ€²(ğ‘‹ğ‘–).This means that Î£RB(ğºâ€²)=â‡’ (ğ‘‹ğ‘–;NDscğºâ€²(ğ‘‹ğ‘–)|ğœ‹ğºâ€²(ğ‘‹ğ‘–)). By de-
composition, we have that Î£RB(ğºâ€²)=â‡’ (ğ‘‹ğ‘–;ğµğ‘–ğ‘¢ğ‘£|ğœ‹ğ»(ğ‘‹ğ‘–))as
required.
Now, suppose that ğ‘‹ğ‘¢ğ‘£âˆˆğœ‹ğ»(ğ‘‹ğ‘–), or thatğ‘‹ğ‘–âˆˆchğ»(ğ‘‹ğ‘¢ğ‘£). Since
ğ‘‹ğ‘–âˆˆV(ğº)\{ğ‘¢,ğ‘£}, then by (10), we have that ğ‘‹ğ‘–âˆˆchğºâ€²(ğ‘£)\{ğ‘£}.
Therefore,ğœ‹ğºâ€²(ğ‘‹ğ‘–)=ğœ‹ğ»(ğ‘‹ğ‘–)\{ğ‘‹ğ‘¢ğ‘£}âˆª{ğ‘¢,ğ‘£}. By(13), we have that:
NDscğºâ€²(ğ‘‹ğ‘–)\ğœ‹ğºâ€²(ğ‘‹ğ‘–)=NDscğºâ€²(ğ‘‹ğ‘–)\(ğœ‹ğ»(ğ‘‹ğ‘–)\{ğ‘‹ğ‘¢ğ‘£}âˆªğ‘¢ğ‘£)
=(NDscğºâ€²(ğ‘‹ğ‘–)\ğ‘¢ğ‘£)\(ğœ‹ğ»(ğ‘‹ğ‘–)\{ğ‘‹ğ‘¢ğ‘£})
=|{z}
(13)(NDscğ»(ğ‘‹ğ‘–)\{ğ‘‹ğ‘¢ğ‘£})\(ğœ‹ğ»(ğ‘‹ğ‘–)\{ğ‘‹ğ‘¢ğ‘£})
=NDscğ»(ğ‘‹ğ‘–)\ğœ‹ğ»(ğ‘‹ğ‘–)
Therefore, Î£RB(ğºâ€²)=â‡’ (ğ‘‹ğ‘–;NDscğ»(ğ‘‹ğ‘–)\ğœ‹ğ»(ğ‘‹ğ‘–)|ğœ‹ğ»(ğ‘‹ğ‘–)\{ğ‘‹ğ‘¢ğ‘£}âˆª
{ğ‘¢,ğ‘£}). Finally, we consider the case where ğ‘‹ğ‘–=ğ‘‹ğ‘¢ğ‘£. By construc-
tion ofğºâ€², and by (11), it holds that:
Î£RB(ğºâ€²)=â‡’ (ğ‘¢;NDscğºâ€²(ğ‘¢)\ğœ‹ğºâ€²(ğ‘¢)|ğœ‹ğºâ€²(ğ‘¢)) (18)
Î£RB(ğºâ€²)=â‡’ (ğ‘£;NDscğºâ€²(ğ‘¢)\ğœ‹ğºâ€²(ğ‘¢)|ğœ‹ğºâ€²(ğ‘¢)âˆª{ğ‘¢}) (19)
By applying the contraction axiom on (18) and (19), we get that
Î£RB(ğºâ€²)=â‡’ (ğ‘¢ğ‘£;NDscğºâ€²(ğ‘¢)\ğœ‹ğºâ€²(ğ‘¢)|ğœ‹ğºâ€²(ğ‘¢)).
Using the fact that hat ğœ‹ğ»(ğ‘‹ğ‘¢ğ‘£)=ğœ‹ğºâ€²(ğ‘¢)(see(9)), and that NDscğ»(ğ‘‹ğ‘¢ğ‘£)=
NDscğºâ€²(ğ‘¢)(see (11)), we get that
Î£RB(ğºâ€²)=â‡’ (ğ‘¢ğ‘£;NDscğ»(ğ‘‹ğ‘¢ğ‘£)\ğœ‹ğ»(ğ‘‹ğ‘¢ğ‘£)|ğœ‹ğ»(ğ‘‹ğ‘¢ğ‘£)).
Sinceğµğ‘–âŠ†NDscğ»(ğ‘‹ğ‘¢ğ‘£)\(ğœ‹ğ»(ğ‘‹ğ‘¢ğ‘£)âˆª{ğ‘‹ğ‘¢ğ‘£}), this proves the claim.
Now, for the other direction. Let (ğ‘‹ğ‘–;ğµğ‘–|ğœ‹ğºâ€²(ğ‘‹ğ‘–))âˆˆÎ£RB(ğºâ€²).
Ifğ‘¢,ğ‘£âˆ‰ğ‘‹ğ‘–âˆªğµğ‘–âˆªğœ‹ğºâ€²(ğ‘‹ğ‘–)), thenğ‘‹ğ‘–âˆ‰{ğ‘¢,ğ‘£}âˆªchğº(ğ‘¢)âˆªchğº(ğ‘£).
By(13), it holds that ğœ‹ğ»(ğ‘‹ğ‘–)=ğœ‹ğºâ€²(ğ‘‹ğ‘–), and that NDscğºâ€²(ğ‘‹ğ‘–)\ğ‘¢ğ‘£=
NDscğ»(ğ‘‹ğ‘–)\ğ‘‹ğ‘¢ğ‘£. Sinceğµğ‘–âŠ†NDscğºâ€²(ğ‘‹ğ‘–)\ğ‘¢ğ‘£=NDscğ»(ğ‘‹ğ‘–)\ğ‘‹ğ‘¢ğ‘£,
thenÎ£RB(ğ»)=â‡’ (ğ‘‹ğ‘–;ğµğ‘–|ğœ‹ğºâ€²(ğ‘‹ğ‘–)).
Ifğ‘¢ğ‘£âŠ†ğµğ‘–, thenğ‘¢,ğ‘£âˆ‰ğœ‹ğºâ€²(ğ‘‹ğ‘–), thenğ‘‹ğ‘–âˆ‰ğ‘¢ğ‘£âˆªchğº(ğ‘¢)âˆªchğº(ğ‘£).
By(13), we have that ğœ‹ğ»(ğ‘‹ğ‘–)=ğœ‹ğºâ€²(ğ‘‹ğ‘–), and that NDscğ»(ğ‘‹ğ‘–)\ğ‘‹ğ‘¢ğ‘£=
NDscğºâ€²(ğ‘‹ğ‘–)\ğ‘¢ğ‘£. Therefore,ğµğ‘–\ğ‘¢ğ‘£âŠ†NDscğ»(ğ‘‹ğ‘–), and by (14), ifğ‘¢ğ‘£âŠ†
ğµğ‘–âŠ†NDscğºâ€²(ğ‘‹ğ‘–), thenğ‘‹ğ‘¢ğ‘£âˆˆNDscğ»(ğ‘‹ğ‘–). Therefore, Î£RB(ğ»)=â‡’
(ğ‘‹ğ‘–;ğµğ‘–\ğ‘¢ğ‘£âˆªğ‘‹ğ‘¢ğ‘£|ğœ‹ğºâ€²(ğ‘‹ğ‘–)), and sinceğ‘‹ğ‘¢ğ‘£=ğ‘¢ğ‘£, then Î£RB(ğ»)=â‡’
(ğ‘‹ğ‘–;ğµğ‘–|ğœ‹ğºâ€²(ğ‘‹ğ‘–)).
Since(ğ‘¢,ğ‘£)âˆˆE(ğºâ€²), we are left with two other cases. First, that
(ğ‘¢;ğµğ‘¢|ğœ‹ğºâ€²(ğ‘¢)), and second(ğ‘£;ğµğ‘£|ğœ‹ğºâ€²(ğ‘£)). Byğ‘‘-separation in ğ»,
the following holds:
Î£RB(ğ»)=â‡’ (ğ‘‹ğ‘¢ğ‘£;NDscğ»(ğ‘‹ğ‘¢ğ‘£)\ğœ‹ğ»(ğ‘‹ğ‘¢ğ‘£)|ğœ‹ğ»(ğ‘‹ğ‘¢ğ‘£))
=â‡’|{z}
(9),(11)(ğ‘‹ğ‘¢ğ‘£;NDscğºâ€²(ğ‘¢)\ğœ‹ğºâ€²(ğ‘¢)|ğœ‹ğºâ€²(ğ‘¢))
=â‡’ (ğ‘¢ğ‘£;NDscğºâ€²(ğ‘¢)\ğœ‹ğºâ€²(ğ‘¢)|ğœ‹ğºâ€²(ğ‘¢)) (20)
By(9), it holds that ğœ‹ğºâ€²(ğ‘£)=ğœ‹ğºâ€²(ğ‘¢)âˆª{ğ‘¢}. By(11), it holds that
ğµğ‘£âŠ†NDscğºâ€²(ğ‘£)\ğœ‹ğºâ€²(ğ‘£)=(NDscğºâ€²(ğ‘¢)âˆª{ğ‘¢})\(ğœ‹ğºâ€²(ğ‘¢)âˆª{ğ‘¢})
=NDscğºâ€²(ğ‘¢)\ğœ‹ğºâ€²(ğ‘¢)
Therefore,ğµğ‘£âˆªğµğ‘¢âŠ†NDscğºâ€²(ğ‘¢)\ğœ‹ğºâ€²(ğ‘¢). In other words, by (20),
we have that;
Î£RB(ğ»)=â‡’ (ğ‘¢ğ‘£;ğµğ‘¢âˆªğµğ‘£|ğœ‹ğºâ€²(ğ‘¢))if and only if
Î£RB(ğ»)=â‡’ (ğ‘¢;ğµğ‘¢âˆªğµğ‘£|ğœ‹ğºâ€²(ğ‘¢)),(ğ‘£;ğµğ‘¢âˆªğµğ‘£|ğœ‹ğºâ€²(ğ‘¢)âˆª{ğ‘¢})
Anna Zeng, Michael Cafarella, Batya Kenig, Markos Markakis, Brit Youngmann, and Babak Salimi
Sinceğœ‹ğºâ€²(ğ‘£)=ğœ‹ğºâ€²(ğ‘¢)âˆª{ğ‘¢}, then overall, we have that Î£RB(ğ»)=â‡’
(ğ‘¢;ğµğ‘¢|ğœ‹ğºâ€²(ğ‘¢)), and Î£RB(ğ»)=â‡’ (ğ‘£;ğµğ‘£|ğœ‹ğºâ€²(ğ‘£)). This com-
pletes the proof. â–¡
To prove Theorem 4.2, we first show the following lemma that es-
tablishes the connection between ğ‘‘-separation on the canonical causal DAG
and the original causal DAG.
Lemma B.6. LetGandGâ€²be causal DAGs defined over the same
set of nodes, i.e., V(G)=V(Gâ€²), whereGâ€²is a supergraph of G
(E(Gâ€²)âŠ‡E(G)). Then, for any three disjoint subsets X,Y,ZâŠ†V(G),
it holds that:(XâŠ¥âŠ¥ğ‘‘Y|Z)Gâ€²=â‡’ (XâŠ¥âŠ¥ğ‘‘Y|Z)G
Proof of Lemma B.6. Suppose that ğ‘‹andğ‘Œareğ‘‘-separated by
ğ‘inğºâ€²(i.e.,(ğ‘‹;ğ‘Œ|ğ‘)ğºâ€²). Ifğ‘‹andğ‘Œareğ‘‘-connected by ğ‘inğº,
then letğ‘ƒdenote the unblocked path between ğ‘‹andğ‘Œ, relative
toğ‘. Since E(ğºâ€²)âŠ‡ E(ğº), then clearly ğ‘ƒis a path in ğºâ€²as well.
Consider any triple (ğ‘¥,ğ‘¤,ğ‘¦)on this path. If this triple has one of
the forms
{ğ‘¥â†’ğ‘¤â†’ğ‘¦,ğ‘¥â†ğ‘¤â†ğ‘¦,ğ‘¥â†”ğ‘¤â†’ğ‘¦,ğ‘¥â†ğ‘¤â†”ğ‘¦},
then sinceğ‘ƒis unblocked in ğº, relative to ğ‘, thenğ‘¤âˆ‰ğ‘. Since
ğ‘¤âˆ‰ğ‘, then the subpath (ğ‘¥,ğ‘¤,ğ‘¦)is also unblocked in ğºâ€². If the
triple has one of the forms:
{ğ‘¥â†”ğ‘¤â†ğ‘¦,ğ‘¥â†’ğ‘¤â†”ğ‘¦,ğ‘¥â†’ğ‘¤â†ğ‘¦},
then sinceğ‘ƒis unblocked in ğº, relative to ğ‘, then Dscğº(ğ‘¤)âˆ©ğ‘â‰ 
âˆ…. Since E(ğºâ€²) âŠ‡ E(ğº), then Dscğº(ğ‘¤) âŠ† Dscğºâ€²(ğ‘¤). Therefore,
Dscğºâ€²(ğ‘¤)âˆ©ğ‘â‰ âˆ…. Consequently, we have, again, that the sub-
path(ğ‘¥,ğ‘¤,ğ‘¦)is unblocked in ğºâ€². Overall, we get that every triple
(ğ‘¥,ğ‘¤,ğ‘¦)on the path ğ‘ƒis unblocked in ğºâ€², relative toğ‘, and hence
ğ‘‹andğ‘Œareğ‘‘-connected in ğºâ€², a contradiction.
By definition, ğºâ€²is compatible with ğºâ€². Therefore, if ğ‘‹andğ‘Œare
ğ‘‘-connected by ğ‘inğºâ€², then by the completeness of ğ‘‘-separation,
there exists a probability distribution that factorizes according to ğºâ€²
in which the CI(ğ‘‹;ğ‘Œ|ğ‘)does not hold. This proves completeness.
â–¡
Proof of Theorem 4.2. LetGHdenote the canonical causal DAG
corresponding toH. By Theorem 4.1, Î£RB(H)â‰¡ Î£RB(GH). There-
fore,(XâŠ¥âŠ¥ğ‘‘Y|Z)Hâ‡â‡’(ğ‘“âˆ’1(X)âŠ¥âŠ¥ğ‘‘ğ‘“âˆ’1(Y)|ğ‘“âˆ’1(Z))GHSince
E(G)âŠ† E(GH), the claim immediately follows from Lemma B.6.
â–¡
B.3 Proofs for Section 6
We next show a smile lemma that will be useful for proving the
soundness and completeness of do-calculus in summary graphs.
Lemma B.7. Letğºbe ADMG, and let ğºâ€²be an ADMG where
V(ğºâ€²)=V(ğº), and E(ğºâ€²)âŠ‡E(ğº). Letğ´,ğµ,ğ¶âŠ†V(ğº)be disjoint sets
of variables, and let ğ‘‹,ğ‘âŠ†V(ğº). Then:
(ğ´;ğµ|ğ¶)ğºâ€²
ğ‘‹ğ‘=â‡’ (ğ´;ğµ|ğ¶)ğºğ‘‹ğ‘(21)
Corollary B.7.1. Letğºbe ADMG, and let(ğ»,ğ‘“)be a summary-
DAG forğº. Letğ´,ğµ,ğ¶âŠ†V(ğ»)be disjoint sets of nodes, and let
ğ‘‹,ğ‘âŠ†V(ğ»). Then:
(ğ´;ğµ|ğ¶)ğ»ğ‘‹ğ‘=â‡’ (A;B|C)ğºğ‘¿ğ’(22)
where forğ‘ˆâŠ†V(ğ»), we denote Udef=ğ‘“(ğ‘ˆ).Theorem B.8 (Soundness of Do-Calculus in supergraphs).
LetGbe a causal DAG encoding an interventional distribution ğ‘ƒ(Â·|
ğ‘‘ğ‘œ(Â·)). LetGâ€²be a causal DAG where V(G)=V(Gâ€²)andE(G)âŠ†
E(Gâ€²). For any disjoint subsets X,Y,Z,WâŠ†V(G), the following three
rules hold:
R1:(YâŠ¥âŠ¥Z|X,W)Gâ€²
X=â‡’ğ‘ƒ(Y|ğ‘‘ğ‘œ(X),Z,W)=ğ‘ƒ(Y|ğ‘‘ğ‘œ(X),W)
R2:(YâŠ¥âŠ¥Z|X,W)Gâ€²
XZ=â‡’ğ‘ƒ(Y|ğ‘‘ğ‘œ(X),ğ‘‘ğ‘œ(Z),W)=ğ‘ƒ(Y|ğ‘‘ğ‘œ(X),Z,W)
R3:(YâŠ¥âŠ¥Z|X,W)Gâ€²
XZ(W)=â‡’ğ‘ƒ(Y|ğ‘‘ğ‘œ(X),ğ‘‘ğ‘œ(Z),W)=ğ‘ƒ(Y|ğ‘‘ğ‘œ(X),W)
where Z(W)is the set of nodes in Zthat are not ancestors of any
node in W. That is, Z(W)=Z\AncsGâ€²(W)where AncsGâ€²(W)def=Ã
ğ‘ŠâˆˆWAncsGâ€²(ğ‘Š).
Theorem B.9 (Soundness of Do-Calculus in supergraphs).
Letğºbe a causal BN (CBN) encoding an interventional distributions
ğ‘ƒ(Â· |ğ‘‘ğ‘œ(Â·)). Letğºâ€²be an ADMG where E(ğº) âŠ† E(ğºâ€²). For any
disjoint subsets ğ‘‹,ğ‘Œ,ğ‘,ğ‘ŠâŠ†V(ğº), the following three rules hold:
R1:(ğ‘Œ;ğ‘|ğ‘‹,ğ‘Š)ğºâ€²
ğ‘‹=â‡’ğ‘ƒ(ğ‘Œ|ğ‘‘ğ‘œ(ğ‘‹),ğ‘,ğ‘Š)=ğ‘ƒ(ğ‘Œ|ğ‘‘ğ‘œ(ğ‘‹),ğ‘Š)
R2:(ğ‘Œ;ğ‘|ğ‘‹,ğ‘Š)ğºâ€²
ğ‘‹ğ‘=â‡’ğ‘ƒ(ğ‘Œ|ğ‘‘ğ‘œ(ğ‘‹),ğ‘‘ğ‘œ(ğ‘),ğ‘Š)=ğ‘ƒ(ğ‘Œ|ğ‘‘ğ‘œ(ğ‘‹),ğ‘,ğ‘Š)
R3:(ğ‘Œ;ğ‘|ğ‘‹,ğ‘Š)ğºâ€²
ğ‘‹ğ‘(ğ‘Š)=â‡’ğ‘ƒ(ğ‘Œ|ğ‘‘ğ‘œ(ğ‘‹),ğ‘‘ğ‘œ(ğ‘),ğ‘Š)=ğ‘ƒ(ğ‘Œ|ğ‘‘ğ‘œ(ğ‘‹),ğ‘Š)
whereğ‘(ğ‘Š)is the set of vertices in ğ‘that are not ancestors of any
vertex inğ‘Š. That is,ğ‘(ğ‘Š)=ğ‘\Ancsğºâ€²(ğ‘Š)where Ancsğºâ€²(ğ‘Š)def=Ã
ğ‘¤âˆˆğ‘ŠAncsğºâ€²(ğ‘¤).
Proof of Lemman B.7. We show that E(ğºğ‘‹ğ‘)âŠ†E(ğºâ€²
ğ‘‹ğ‘), and
the claim then follows from Theorem ??. Let(ğ‘¢,ğ‘£)âˆˆE(ğºğ‘‹ğ‘)âŠ†
E(ğº)âŠ†E(ğºâ€²). By definition, ğ‘¢âˆ‰ğ‘andğ‘£âˆ‰ğ‘‹. But this means that
(ğ‘¢,ğ‘£)âˆˆE(ğºâ€²
ğ‘‹ğ‘), which completes the proof. â–¡
Proof of Corollary B.7.1. Letğºğ»ğ‘‹ğ‘denote the grounded DAG
corresponding to ğ»ğ‘‹ğ‘. By Theorem 4.1, Î£RB(ğ»ğ‘‹ğ‘)â‰¡Î£RB(ğºğ»ğ‘¿ğ’),
and hence(ğ´;ğµ|ğ¶)ğ»ğ‘‹ğ‘if and only if(A;B|C)ğºğ»XZ. Since E(ğºğ»)âŠ‡
E(ğº), then by Lemma B.7, it holds that if (A;B|C)ğºğ»XZ, then(A;B|C)ğºXZ.
Overall, we have that:
(ğ´;ğµ|ğ¶)ğ»ğ‘‹ğ‘â‡”(A;B|C)ğºğ»ğ‘¿ğ’=â‡’ (A;B|C)ğºğ‘¿ğ’(23)
which proves the claim. â–¡
Proof of Theorem B.9. If(ğ‘Œ;ğ‘|ğ‘‹,ğ‘Š)ğºâ€²
ğ‘‹, then by Lemma B.7,
it holds that(ğ‘Œ;ğ‘|ğ‘‹,ğ‘Š)ğºğ‘‹. By the soundness of do-calculus for
causal BNs, we get that ğ‘ƒ(ğ’€|ğ‘‘ğ‘œ(ğ‘¿),ğ’,ğ‘¾)=ğ‘ƒ(ğ’€|ğ‘‘ğ‘œ(ğ‘¿),ğ‘¾). If
(ğ‘Œ;ğ‘|ğ‘‹,ğ‘Š)ğºâ€²
ğ‘‹ğ‘, then by Lemma B.7, it holds that (ğ‘Œ;ğ‘|ğ‘‹,ğ‘Š)ğºğ‘‹ğ‘.
By the soundness of do-calculus for causal BNs, we get that ğ‘ƒ(ğ‘Œ|
ğ‘‘ğ‘œ(ğ‘‹),ğ‘‘ğ‘œ(ğ‘),ğ‘Š)=ğ‘ƒ(ğ‘Œ|ğ‘‘ğ‘œ(ğ‘‹),ğ‘,ğ‘Š). Finally, if(ğ‘Œ;ğ‘|ğ‘‹,ğ‘Š)ğºâ€²
ğ‘‹ğ‘(ğ‘Š),
then by Lemma B.7, it holds that (ğ‘Œ;ğ‘|ğ‘‹,ğ‘Š)ğºğ‘‹ğ‘(ğ‘Š). By the sound-
ness of do-calculus for causal BNs, we get that ğ‘ƒ(ğ‘Œ|ğ‘‘ğ‘œ(ğ‘‹),ğ‘‘ğ‘œ(ğ‘),ğ‘Š)=
ğ‘ƒ(ğ‘Œ|ğ‘‘ğ‘œ(ğ‘‹),ğ‘Š). â–¡
Proof of Theorem 6.1. If(ğ‘Œ;ğ‘|ğ‘‹,ğ‘Š)ğ»ğ‘‹, then by Corollary B.7.1,
it holds that(ğ’€;ğ’|ğ‘¿,ğ‘¾)ğºğ‘‹. By the soundness of do-calculus for
causal BNs, we get that ğ‘ƒ(ğ’€|ğ‘‘ğ‘œ(ğ‘¿),ğ’,ğ‘¾)=ğ‘ƒ(ğ’€|ğ‘‘ğ‘œ(ğ‘¿),ğ‘¾). If
(ğ‘Œ;ğ‘|ğ‘‹,ğ‘Š)ğ»ğ‘‹ğ‘, then by Corollary B.7.1, it holds that (ğ’€;ğ’|ğ‘¿,ğ‘¾)ğºğ‘‹ğ‘.
Causal DAG Summarization (Full Version)
By the soundness of do-calculus for causal BNs, we get that ğ‘ƒ(ğ’€|
ğ‘‘ğ‘œ(ğ‘¿),ğ‘‘ğ‘œ(ğ’),ğ‘¤)=ğ‘ƒ(ğ’€|ğ‘‘ğ‘œ(ğ‘¿),ğ’,ğ‘¾). Finally, if(ğ‘Œ;ğ‘|ğ‘‹,ğ‘Š)ğ»ğ‘‹ğ‘(ğ‘Š),
then by Corollary B.7.1, it holds that (ğ’€;ğ’|ğ‘¿,ğ‘¾)ğºğ‘‹ğ‘(ğ‘Š). By the
soundness of do-calculus for causal BNs, we get that ğ‘ƒ(ğ’€|ğ‘‘ğ‘œ(ğ‘¿),ğ‘‘ğ‘œ(ğ’),ğ‘¾)=
ğ‘ƒ(ğ’€|ğ‘‘ğ‘œ(ğ‘¿),ğ‘¾). â–¡
Proof of Theorem 6.2. Considerğºğ», the grounded-DAG of
(ğ»,ğ‘“), that is, by definition, compatible with ğ». Ifğ‘Œisğ‘‘-connected
toğ‘inğ»ğ‘‹with respect to ğ‘‹âˆªğ‘Š, then by Definition 5, it holds
thatğ‘¦isğ‘‘-connected to ğ‘§inğºğ»ğ‘“(ğ‘‹)with respect to ğ‘“(ğ‘‹âˆªğ‘Š), for
everyğ‘¦âˆˆğ‘“(ğ‘Œ)andğ‘§âˆˆğ‘“(ğ‘). Therefore, ğ‘“(ğ‘Œ)isğ‘‘-connected to
ğ‘“(ğ‘)inğºğ»ğ‘“(ğ‘‹)with respect to ğ‘“(ğ‘‹âˆªğ‘Š). â–¡
C HANDLING MIXED GRAPHS
While one dominant form of graph input for causal inference is a
causal DAG, other graph representations are also used when a full
causal DAG is not retrievable, say, by a causal discovery algorithm
(e.g., [ 77]). Many of these graph representations are referred to asmixed graphs due to their inclusion of undirected, bidirected, and
other types of edges [20, 78].
One commonly used of mix graph is an acyclic-directed mixed
graph (ADMG), which consists of a DAG with bidirected edges. As
mentioned in the introduction, all of our results apply to scenarios
where the input graph is an ADMG. Subsequently, we present an
extension to the CaGreS algorithm to accommodate an ADMG.
In this scenario, we modify the cost function (Algorithm 2) as
follows: When we remove a bidirected edge between nodes ğ‘ˆand
ğ‘‰(i.e.,ğ‘ˆâ†”ğ‘‰) by merging ğ‘ˆandğ‘‰into a single node, the cost
incurred is doubled compared to removing a "standard" directed
edge. For instance, in line 4 of Algorithm 2, if UandVwere linked
by a bidirected edge, the line would be updated to:
ğ‘ğ‘œğ‘ ğ‘¡â†ğ‘ğ‘œğ‘ ğ‘¡+2Â·ğ‘ ğ‘–ğ‘§ğ‘’(U)Â·ğ‘ ğ‘–ğ‘§ğ‘’(V)
This adjustment is necessary because losing a bidirected edge
should carry a higher cost than losing a regular directed edge,
given that more information is lost.