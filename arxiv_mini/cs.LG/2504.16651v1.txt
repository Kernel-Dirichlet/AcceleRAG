MAYA: Addressing Inconsistencies in Generative Password
Guessing through a Unified Benchmark
William Corrias
Sapienza University of Rome
Rome, Italy
corrias@di.uniroma1.itFabio De Gaspari
Sapienza University of Rome
Rome, Italy
degaspari@di.uniroma1.it
Dorjan Hitaj
Sapienza University of Rome
Rome, Italy
hitaj.d@di.uniroma1.itLuigi V. Mancini
Sapienza University of Rome
Rome, Italy
mancini@di.uniroma1.it
Abstract
The rapid evolution of generative models has led to their integration
across various fields, including password guessing, aiming to gen-
erate passwords that resemble human-created ones in complexity,
structure, and patterns. Despite generative model’s promise, incon-
sistencies in prior research and a lack of rigorous evaluation have
hindered a comprehensive understanding of their true potential. In
this paper, we introduce MAYA, a unified, customizable, plug-and-
play password benchmarking framework. MAYA provides a stan-
dardized approach for evaluating generative password-guessing
models through a rigorous set of advanced testing scenarios and
a collection of eight real-life password datasets. Using MAYA, we
comprehensively evaluate six state-of-the-art approaches, which
have been re-implemented and adapted to ensure standardization,
for a total of over 15,000hours of computation. Our findings in-
dicate that these models effectively capture different aspects of
human password distribution and exhibit strong generalization
capabilities. However, their effectiveness varies significantly with
long and complex passwords. Through our evaluation, sequential
models consistently outperform other generative architectures and
traditional password-guessing tools, demonstrating unique capabil-
ities in generating accurate and complex guesses. Moreover, models
learn and generate different password distributions, enabling a
multi-model attack that outperforms the best individual model. By
releasing MAYA, we aim to foster further research, providing the
community with a new tool to consistently and reliably bench-
mark password-generation techniques. Our framework is publicly
available at https://github.com/williamcorrias/MAYA-Password-
Benchmarking.
Keywords
Password Security, Password Guessing, Password Cracking, Gener-
ative Models, Machine Learning, Deep Learning.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
Conference’17, Washington, DC, USA
©2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-x-xxxx-xxxx-x/YYYY/MM
https://doi.org/10.1145/nnnnnnn.nnnnnnn1 Introduction
Despite the rise of increasingly secure authentication methods,
such as biometric systems, multi-factor authentication, one-time
passwords, and passkeys, traditional passwords remain the most
widely used mechanism due to their efficiency, usability, and famil-
iarity [ 14,66]. However, users often utilize simplistic passwords
and reuse them across services, making them vulnerable to a va-
riety of attacks [ 13,24,25,35,82]. As a result, password security
has long been a focus of research. Brute-force and dictionary at-
tacks remain the most prevalent password-cracking techniques
today, primarily exploiting the weaknesses associated with weak
passwords and password reuse. The former attempts all possible
character combinations, and the latter leverages lists of commonly
used passwords. However, both methods are inherently limited
in scope. Tools like John The Ripper (JTR) [ 88] and Hashcat [ 43]
enhance these techniques by applying transformation rules to dic-
tionaries of passwords, generating variations based on common
patterns. Yet, effective transformation rules are complex to design
and require extensive contextual knowledge and manual labor.
In recent years, research has increasingly focused on harnessing
advancements in generative models to enhance password guessing
techniques [ 10,48,71,77,81,85,95,116,123]. Unlike traditional
approaches, these models aim to learn and replicate the complexity,
structure, and patterns of human-created passwords. Nevertheless,
the current literature suffers from inconsistencies and a lack of
rigorous, standardized evaluation. Proposed approaches are often
evaluated using methodologies that are inconsistent across stud-
ies and tested in non-uniform, often simplistic settings, hindering
meaningful comparisons and limiting a comprehensive understand-
ing of their capabilities.
To address this gap, we introduce MAYA1, a unified, fully cus-
tomizable plug-and-play password benchmarking framework that
enables fair comparisons and a rigorous evaluation. MAYA includes
six state-of-the-art generative models, re-implemented with stan-
dardized data preprocessing, dependencies, and configurations;
eight real-world password datasets for training and testing; and
a large set of comprehensive evaluation scenarios. Using MAYA,
1The name "MAYA " is inspired by the philosophical concept introduced by A. Schopen-
hauer, who referred to “the veil of Maya” as the illusion that obscures the true nature
of reality and once unveiled reveals the world as it truly is. Similarly, our framework
aims to unveil the real potential of password-guessing generative models, which has
so far remained covered.arXiv:2504.16651v1  [cs.CR]  23 Apr 2025
Conference’17, July 2017, Washington, DC, USA William Corrias, Fabio De Gaspari, Dorjan Hitaj, and Luigi V. Mancini
we show that (a) Increasing password length yields diminishing
returns in reducing guessability; similarly, increasing the number
of generated passwords offers reducing improvements in success-
ful guesses, as models tend to exhaust easily guessable passwords
early in the generation process. (b) The most performing genera-
tive models consistently outperform traditional approaches, which
remain competitive on smaller datasets, while generative models
show a clear advantage on larger ones. (c) Generative models do
not always require large datasets to effectively model data distribu-
tion. While some architectures benefit from larger datasets, others
exhibit minimal improvement or even a decrease in performance
as the training data increases. (d) Models can successfully guess
passwords even when trained on datasets from different communi-
ties and/or cultures compared to the test set, highlighting strong
generalization capabilities and suggesting the existence of common
structures underlying human-created passwords across disparate
groups. (e) While all models demonstrate strong capabilities in
guessing common and simple passwords and a decrease in perfor-
mance for rarer passwords, sequential models are the only ones that
remain effective in guessing longer passwords and more complex
patterns. (f) Different architectures learn generation functions with
different probability distributions over the codomain, enabling their
combination into a multi-model attack that outperforms individual
models. (g) Generative models effectively capture various aspects
of human-created passwords, generating high-quality and diverse
passwords while minimizing mode failures. However, some models
struggle to accurately replicate the length distribution.
This paper makes the following contributions:
•We introduce a standardized and comprehensive testing
methodology for assessing password-guessing approaches
and apply it to generative models to resolve inconsistencies
in the current literature.
•We develop MAYA, a customizable, plug-and-play bench-
marking framework based on our proposed methodology.
MAYA is designed to comprehensively evaluate generative
password-guessing models across an extensive set of ad-
vanced testing scenarios. We release our code with the aim of
fostering further research, providing the community with a
new tool to consistently and reliably benchmark approaches.
The code to reproduce our results is available at https://
github.com/williamcorrias/MAYA-Password-Benchmarking.
•We analyze eight real-life leaked password datasets, exam-
ining the characteristics of each dataset’s password distri-
bution and the impact of various factors such as language,
culture, leak data, and type of service on password guessing.
•We implement in MAYA and comprehensively assess six
state-of-the-art generative password-guessing approaches
for a total of over 15,000hours of computation, answering
seven key research questions.
•We provide insights to guide future research in enhancing
model password-guessing capabilities and integrating them
into other password-related domains.2 Background
Research has increasingly focused on applying machine learning
(ML) across various domains, including password security, where it
plays an important role in strengthening password-based authenti-
cation mechanisms and developing advanced attacks. This section
contextualized its primary applications in the field.
2.1 Enhancing Password-Based Authentication
Various approaches proposed to strengthen password-based authen-
tication mechanisms have clearly benefited from machine learning:
2.1.1 Honeywords .Honeywords serve as decoy passwords stored
alongside real ones to detect breaches. If a honeyword is used, a
Honeycheck system triggers an alert [ 55]. The method’s success re-
lies on how indistinguishable the decoys are from real passwords. To
generate more realistic honeywords, Dionysiou et al. [ 28] proposed
HoneyGen, a ML approach based on representation learning. How-
ever, such techniques are vulnerable to deep learning-based attacks,
such as PassFilter [ 23]. Another line of research has focused on
generating high-quality honeywords using users’ personally iden-
tifiable information (PII) to defend against targeted attacks [ 110].
Yu et al. [ 124] leveraged pre-trained language models to generate
honeywords from users’ PII, while Wang et al. [ 114] employed
probabilistic models. Recent efforts focus on generative models to
create realistic honeywords [29, 125].
2.1.2 Password Strength Meters .Modern websites and appli-
cations implement password strength meters (PSMs) to prevent
users from selecting easily crackable passwords. PSMs have been
significantly enhanced since their introduction [ 9,11]. Initially,
password strength was estimated using rule-based heuristics, such
as password length and character diversity [ 17,37], or pattern-based
approaches, such as zxcvbn [ 119] and KeePass [ 57]. However, these
heuristic methods often fail to evaluate password strength precisely,
pushing the focus towards statistical and machine learning-based
techniques [ 34]. Multiple Markov-based PSMs have been devel-
oped [ 12,18,100,101], along with Probabilistic Context-Free Gram-
mar (PCFG) approaches [ 111]. Melicher et al. [ 71] utilized Recurrent
Neural Networks (RNNs) to model the probability distribution of
passwords while Pasquini et al. [ 80] introduced an interpretable
deep-learning-based PSM capable of offering character-level feed-
back, allowing users to improve their passwords.
2.1.3 Keystroke Dynamics .Beyond traditional biometric au-
thentication methods like fingerprint and facial recognition, re-
searchers have explored behavioral biometrics as a complementary
approach to password-based authentication. By analyzing unique
user behaviors, such as keystroke dynamics [ 73], these systems can
detect and block unauthorized access, even if an attacker possesses
valid credentials but exhibits a different typing pattern. Machine
learning techniques [ 6,51,56,74,86] have shown promising results
in enhancing the accuracy and reliability of these systems.
2.2 Enhancing Password Attacks
Password attacks, both targeted and trawling, have fundamentally
changed their nature with the integration of machine learning,
evolving into more sophisticated and advanced methods.
MAYA: Addressing Inconsistencies in Generative Password Guessing through a Unified Benchmark Conference’17, July 2017, Washington, DC, USA
2.2.1 Targeted Password Attacks .Targeted password attacks
involve an adversary attempting to gain unauthorized access to a
specific account by exploiting a users’ PII. Wang et al. [ 113] were
among the first to highlight this threat, introducing TarGuess, a
suite of four targeted attack variants based on probabilistic tech-
niques such as PCFGs, Markov models, and Bayesian inference.
Their findings demonstrated that targeted attacks are significantly
more effective than previously assumed. Similarly, Li et al. [ 63]
proposed a personalized PCFG model. Beyond probabilistic ap-
proaches, Wang et al. [ 116] introduced RFGuess, a targeted attack
leveraging random forests. Pal et al. [ 78] proposed Pass2Pass and
Pass2Path, two deep-learning techniques designed to model users’
password reuse behavior. Wang et al. [ 115] developed Pass2Edit,
outperforming both Pass2Pass and Pass2Path. Recently, relying on
transformer-based model architectures, Passtrans [ 45] and Pass-
BERT [122] were able to outperform traditional approaches.
2.2.2 Trawling Password Attacks .In a trawling password at-
tack, an adversary attempts to crack as many user passwords as
possible across a dataset, without targeting any specific individ-
ual. Early trawling attacks relied on heuristics such as John The
Ripper (JTR) [ 88] and HashCat [ 43]. These tools use rule-based
guessing strategies, where a set of transformations are applied to
each password in a dictionary, generating multiple variations of the
original password. Pasquini et al. [ 80] proposed AdaMs (Adaptive
Dynamic Mangling rules attack), which optimizes the process by
assigning a dedicated set of rules to each password using a neural
network and adapting the guessing strategy based on the distri-
bution of previously guessed passwords. Beyond heuristic-based
methods, researchers have explored probabilistic models, including
PCFGs [ 19,42,58,118,121] and Markov Models [ 40,67,75,120].
Recently, numerous studies have integrated deep neural networks
and generative models. Melicher et al. [ 71] introduced FLA, an ap-
proach based on recurrent neural networks. Beyond RNNs, Genera-
tive Adversarial Networks (GANs) have been widely applied, with
PassGAN and PLR-GAN standing out as the most notable exam-
ples [ 30,48,49,81,106,127], along with Variational Autoencoders
(VAE) [ 81,117,123]. The only known model based on the flow ar-
chitecture is PassFlow [ 77]. Finally, for transformer-based models,
there are PassGPT [ 85], VGPT2 [ 10] (a hybrid model combining VAE
and Transformer), PagPassGPT [ 96], and PassBERT [ 122]. Details
on these architectures are found in Appendix A. Password-guessing
generative models have garnered significant attention from the re-
search community, with many papers published in highly reputable
venues, gathering a lot of attention also from the media [ 3,20–
22, 47, 50, 52, 53, 65, 68, 69, 72, 87, 89, 102, 107, 108].
3 MAYA
This section presents our framework, MAYA, designed to offer uni-
fied, plug-and-play, in-depth benchmarking for password-guessing
approaches. MAYA provides a standardized testing environment
for all types of password-guessing methods, ranging from tradi-
tional techniques to state-of-the-art generative models. The archi-
tecture of our framework is highly modular and easily extendable,
enabling plug-and-play integration of new modules to support fu-
ture research. MAYA includes a comprehensive set of experimental
settings, delivering thorough and detailed benchmarking acrossmultiple key metrics. Our framework currently implements six
state-of-the-art password-guessing generative models (Section 3.3),
eight real-world password datasets (Section 4), and a comprehen-
sive set of testing scenarios aimed at answering seven key research
questions (Section 3.4).
3.1 Motivation
The current literature faces two significant challenges that limit a
comprehensive understanding of password-guessing models’ ca-
pabilities. Firstly, there is a lack of consistency in the evaluation
methodology across different approaches, which hinders fair com-
parisons. Secondly, there is a lack of rigorousness in evaluating
them; models are often only tested in overly simplistic scenarios,
failing to capture their real capabilities.
3.1.1 Lack of Consistency .Each existing approach adopts its
own evaluation methodology, making them hard to compare, as
models are neither trained nor evaluated on the same data and
under the same settings. These methodological differences span
various factors, including data preprocessing algorithms, file en-
coding, vocabulary, maximum password length, the number of gen-
erated passwords, and the size of the training and testing dataset.
Fairly comparing approaches under such varying settings is chal-
lenging. For instance, comparing approaches utilizing different data
preprocessing algorithms is unfair, as the resulting training and
testing datasets differ. Similarly, evaluating models with different
maximum password lengths is unfair since shorter passwords are
generally easier to guess than longer ones. Similar considerations
apply to other settings. These inconsistencies highlight the need
for a standardized methodology to serve as a reference point.
3.1.2 Lack of Rigorousness .Current practices for assessing gen-
erative password-guessing models are often overly simplistic, typ-
ically limited to evaluating the number of successful guesses on
individual datasets such as RockYou or LinkedIn. This methodol-
ogy is highly restrictive, as it overlooks critical aspects such as
model generalizability, the human-likeness of generated passwords,
the cross-domain and cross-cultural adaptability, and the ability
to clearly identify model limitations. As a result, current literature
provides an incomplete view of the models’ true capabilities, high-
lighting the need for evaluation across a broader range of advanced
scenarios.
3.2 Methodology
MAYA addresses the shortcomings outlined in the previous section
through a unified benchmarking framework for comprehensively
evaluating generative password-guessing models. MAYA provides
an intuitive environment for training and testing with minimal
setup, following standardized guidelines to ensure fair compar-
isons and rigorous evaluation, ultimately enabling the research
community move in a unified direction. It is designed to be highly
customizable, allowing for seamless integration of custom models,
facilitating the comparison of different approaches, and enabling
easy configuration of customized testing scenarios.
3.2.1 Standardized Data Preprocessing and Settings .We pro-
pose the following procedure to standardize data preprocessing:
(1) open and read datasets using UTF-8 encoding while ignoring
Conference’17, July 2017, Washington, DC, USA William Corrias, Fabio De Gaspari, Dorjan Hitaj, and Luigi V. Mancini
errors, (2) remove passwords that exceed the specified maximum
length, contain non-ASCII characters, or include characters that are
not in the provided vocabulary. (3) split the dataset following the
standard 80%training and 20%testing ratios, (4) remove duplicates
from the testing dataset, and (5) remove from the training dataset
any overlap with the testing dataset. This approach ensures wide
language compatibility through UTF-8 encoding, avoids double-
counting by eliminating duplicates, and provides a fixed, consistent
testing set across experiments. We note that existing works typi-
cally define the training set first, and then derive the testing set by
removing any overlapping samples. This approach is undesirable,
as varying the training set leads to changes in the testing set as
well, thereby limiting the comparability across experiments. We
further define our vocabulary to include all uppercase and low-
ercase letters, digits, and the following widely-accepted symbols:
~!@#$%^&*(),.<>/? '"{}[]\-_=+;: `.
3.2.2 Advanced Evaluation Scenarios .We have designed a set
of advanced and realistic evaluation scenarios to accurately and
comprehensively assess the models’ capabilities and limitations,
with the goal of providing a complete and nuanced understanding
of their performance. Each scenario has been envisioned to address
one of the seven research questions outlined in Section 3.4.
3.3 Models
We selected six state-of-the-art generative models for password
guessing, aiming to provide a comprehensive evaluation of differ-
ent architectural approaches: (1) FLA [ 71], based on an LSTMs, (2)
PassGAN [ 48], and (3) PLR-GAN [ 81], both GAN-based approaches,
(4) PassFlow [ 77], a flow-based model, (5) VGPT2 [ 10], which com-
bines a VAE with GPT2-derived encoder and decoder blocks, and
(6) PassGPT [ 85], an autoregressive transformer model based on
the GPT2 architecture. To ensure standardization, each model was
adapted and re-implemented in strict accordance with the proce-
dures described in the respective papers, minimizing discrepancies
arising from implementation variations. We provide further imple-
mentation details in Appendix B. We validated the accuracy of our
implementations by comparing our results with those reported in
the original papers.
FLA .FLA [ 71] (Fast, Lean, and Accurate), was the first approach
to apply neural networks to the password-guessing task. It is the
only approach we selected based on a recurrent neural network,
specifically an LSTM, allowing us to examine how models designed
for sequential data processing perform compared to other, more
recent generative architectures.
PassGAN .PassGAN [ 48]is based on a Generative Adversarial Net-
work (GAN) architecture, which, unlike other designs, follows an
adversarial training approach. As they are implicit models, GANs
learn to generate data by capturing the underlying distribution of
the training set without explicitly defining a probability distribution,
offering a unique perspective in password generation.
PLR-GAN .PLR-GAN [ 81] is an enhanced version of PassGAN
and represents the current state-of-the-art for GAN-based methods.
PLR-GAN further adopts a Dynamic Password Guessing (DPG)
strategy, which allows the model to adapt its guesses based onthe distribution of successfully guessed passwords, increasing the
likelihood of generating relevant guesses.
PassFlow .Passflow [ 77] represents the first and only attempt to
integrate flow-based generative models into the field of password
guessing. Flow networks [ 79] offer an explicit latent space and an
invertible mapping between a data point and its latent representa-
tion, enabling complex operations such as interpolation and exact
latent variable inference. PassFlow adopts and further enhances
DPG by integrating Gaussian Smoothing in the generation process,
reducing the likelihood of generating duplicated passwords while
maintaining the benefits of DPG.
VGPT2 .VGPT2 [ 10] combines a Variational Autoencoder (VAE)
with an encoder-decoder architecture based on GPT2. The VAE
provides an explicit representation of the latent space, while GPT2
excels at processing sequential information and capturing long-
term dependencies, making it a unique approach to analyze.
PassGPT .PassGPT [ 85] proposes a GPT-2-based language model
for password generation. Similarly to FLA, PassGPT employs a
sequential generation process. However, GPT-2 is built upon an
attention mechanism, which allows it to capture long-range depen-
dencies more effectively.
3.4 Research Questions
This section presents the research questions that guided the design
of our evaluation.
RQ1 - How Do Different Settings Influence Models Perfor-
mance? Generative password-guessing models have two primary
settings: the maximum password length and the number of gener-
ated passwords. This RQ explores the impact of these two factors
on guessing performance. We test three different maximum lengths
(8, 10, and 12) and eleven generation quantities, from 106to5×108.
For each dataset, all models were trained three times—once per
length—and evaluated across all generation quantities.
RQ2 - Are Generative Models Truly Better than Traditional
Tools? Despite recent advancements in generative architectures,
there is limited evidence indicating whether generative models
outperform traditional approaches, and few direct experimental
comparisons have been conducted. We address this gap by conduct-
ing a comprehensive evaluation of generative models on 8 different
datasets and comparing them to traditional tools such as John the
Ripper (JtR) and Hashcat, offering a thorough, direct comparison
between generative and rule-based methods.
RQ3 - How and to What Extent Does the Size of the Training
Dataset Affect Model Performance? In real-world scenarios,
attackers often have limited or partial access to leaks. Therefore,
evaluating model performance across varying training data subset
sizes is essential for gaining a clear understanding of the effec-
tiveness of generative models. This RQ examines the ability of
generative models to reconstruct the full target data distribution
starting from varying portions of the source dataset and execute a
successful attack. We explore this capability by training models on
up to seven different data subsets on four distinct datasets.
MAYA: Addressing Inconsistencies in Generative Password Guessing through a Unified Benchmark Conference’17, July 2017, Washington, DC, USA
RQ4 - Can Models Generalize To Different Communities
and/or Cultures? A common real-world scenario involves attack-
ers obtaining leaked passwords from a distribution different from
their intended target, often due to cultural or community differ-
ences. This RQ investigates whether generative models can ef-
fectively generalize to unseen password distributions. We assess
this capability through extensive cross-dataset analysis, testing
the models on datasets distinct from those used during training.
Specifically, we examine their ability to guess passwords across (1)
different communities (i.e., same language but different online ser-
vices) and (2) different cultures (i.e., different languages and cultural
backgrounds), which, as demonstrated in Section 4.2, significantly
impacts password distributions.
RQ5 - Are Models Limited to Guessing Only Simple and
Common Passwords? Simple passwords are easily guessed using
traditional tools, whereas rare and complex passwords present a
significantly greater challenge. The ability of generative models to
arbitrarily sample from different areas of the password distribution
offers the potential to guess even rare and complex passwords. We
explore this RQ by dividing and categorizing the test dataset based
on password frequency and assessing each model’s performance on
the different subsets. Additionally, we analyze how well the models
perform in guessing passwords of varying lengths and patterns.
RQ6 - To What Extent Do the Distributions Learned by Dif-
ferent Models Align? Can We Combine Models to Maximize
Effectiveness? Generative models are trained to generate data that
matches the target distribution, learning this distribution either
implicitly or explicitly. However, it remains unclear to what extent
distributions learned by different models align, as various factors
influence the learning process. If the distributions are not fully
aligned, a multi-model attack could enhance guessing capabilities.
RQ6.1 - To What Extent Do the Distributions Learned by
Different Models Align? We explore the first part of this RQ using
two metrics computed across all model pairs (𝑀1,𝑀2): Jaccard
Index and Mergeability Index. Jaccard Index measures the ratio
between the intersection and union of the passwords generated
by each model, providing a metric for the overall diversity of all
passwords generated by 𝑀1and𝑀2. Additionally, we introduce
the Mergeability Index, which quantifies the ratio between the
marginal gain achieved by combining the outputs of 𝑀1and𝑀2
over the best performance between 𝑀1and𝑀2(see Eq. 4). This
metric assesses the weighted improvement gained by combining
𝑀1and𝑀2, compared to using only the best-performing model.
RQ6.2 - Can We Combine Models to Maximize Effectiveness?
We follow an iterative elimination approach to gain valuable in-
sights into whether and to what extent combining multiple models
enhances guessing effectiveness. We begin by combining passwords
generated by all models and progressively remove those generated
by the least effective model in terms of additional matched pass-
words. This process allows us to iteratively refine and identify
effective combinations of models for any desired number of models
to combine.
RQ7 - Do Models Truly Capture the Characteristics of Human-
Like Passwords? Despite ample research, it remains unclear toTable 1: Details of the selected datasets.
Dataset N. Pass N. Unique Loc Lang Year Service
Rockyou [91] 32.600.024 14.311.994 USA EN 2009 Gaming
Linkedin [2] 60.650.662 60.591.405 Global EN 2012 Social
Mail.ru [1] 3.723.472 2.260.454 RU RU 2014 Mail
000webhost [16] 15.269.739 10.587.879 USA EN 2015 Forum
Taobao [99] 7.492.035 6.165.957 CHN ZH 2012 Ecomm
Gmail [76] 4.912.520 3.122.573 RU RU 2014 Mail
Ashley Madison [126] 375.846 375.738 CA EN 2015 Social
Libero [32] 667.680 418.400 IT IT 2016 Mail
what extent the distribution learned by generative models aligns
with that of human-created passwords. Evaluation metrics such
as guess percentage are inherently limited, as they only measure
whether generated passwords match the test data, without offering
insights into the distribution of non-matching guesses. A major
challenge in answering this RQ is how to quantify the distance
between generative models’ password distribution and the over-
all distribution of human-like passwords. In related fields, metrics
such as Fréchet Inception Distance (FID) [ 46] and Inception Score
(IS) [ 90] are commonly used to assess the generated data [ 15]. How-
ever, these metrics rely on pre-trained classifiers and are unsuitable
for the password domain. We address this gap by identifying and
adopting four alternative metrics: (1) CNN Divergence [ 39], (2)
IMD [ 103], (3)𝛼-Precision𝛽-Recall Authenticity [ 5], and (4) MTop-
Div [ 8]. These metrics were carefully selected for their ability to
capture different aspects of the considered distribution. Appendix C
provides a detailed explanation of each metric. We computed these
metrics for each model across all datasets and averaged the results,
offering a comprehensive assessment of the human-likeness of the
generated passwords. Additionally, we complement this analysis
by examining the length distribution of the generated passwords
and comparing it to that of real passwords.
4 Datasets
Our framework incorporates eight real-world leaked password
datasets, carefully chosen to ensure diversity in size, geographic
origin, language, time of leakage, and service type. In the follow-
ing sections, we provide a detailed description of each dataset and
perform a statistical analysis to examine the distribution and char-
acteristics of the passwords across these datasets, providing insights
that form the basis for our subsequent experiments.
4.1 Datasets Selection
Table 1, presents the datasets selected for this study. To ensure that
the selected datasets provide generalizable insights, the following
criteria were considered in the selection process:
Dataset Size .We included datasets of varying sizes to evalu-
ate model performance under different dimensions. The collection
ranges from small datasets with a few hundred thousand passwords,
such as Libero [ 32], to large-scale datasets with tens of millions of
passwords, such as RockYou [ 91], LinkedIn [ 2] and 000webhost [ 16].
Geographic Diversity .The selected datasets vary in their geo-
graphical provenance. Previous studies have shown that users from
different countries exhibit distinct password creation behaviors,
leading to diverse password distributions [33, 41, 64, 109, 112].
Conference’17, July 2017, Washington, DC, USA William Corrias, Fabio De Gaspari, Dorjan Hitaj, and Luigi V. Mancini
Table 2: Distribution of Password Lengths.
Dataset 1-5 6 7 8 9 10 11 12 13+
rockyou 4.33% 26.06% 19.30% 19.99% 12.12% 9.06% 3.56% 2.10% 3.49%
linkedin 0.00% 8.95% 11.01% 24.60% 14.47% 12.54% 6.56% 4.19% 17.68%
mailru 1.78% 22.90% 14.68% 23.92% 11.64% 8.8%1 5.22% 4.31% 6.74%
000webh 0.06% 5.71% 7.94% 21.88% 15.41% 14.50% 10.48% 7.66% 16.36%
taobao 0.48% 12.93% 13.14% 16.90% 16.51% 16.10% 10.77% 6.80% 6.39%
gmail 4.13% 18.73% 13.49% 28.92% 13.85% 13.85% 3.10% 1.89% 2.05%
ashleym 9.97% 19.57% 18.15% 24.87% 13.21% 9.66% 2.19% 1.26% 1.12%
libero 0.09% 15.49% 11.34% 31.30% 16.34% 11.32% 5.54% 3.83% 4.76%
Average 2.60% 16.29% 13.63% 24.05% 14.19% 11.98% 5.93% 4.01% 7.32%
CDF 2.60% 18.90% 32.53% 56.58% 70.77% 82.75% 88.68% 92.68% 100.00%
Language and Cultural Background .We selected datasets
representing diverse languages and cultural contexts. Previous re-
search [ 7,64,109,112], suggest that users with different language
or cultural backgrounds tend to create different passwords.
Temporal Span .We consider the dataset leak date as an impor-
tant factor, as password policies and user awareness have evolved
over time, with stronger requirements introduced in recent years [ 31,
62,70]. Consequently, we expect older leaks to contain weaker pass-
words, while more recent datasets are expected to contain more
secure passwords. Our dataset collection spans from 2009 to 2016.
Service Types and Community Background .We selected
datasets from a wide range of services and communities, such as so-
cial networks, forums, e-commerce sites, dating sites, email services,
and gaming platforms. The type of service and the nature of the user
community can significantly influence password selection strate-
gies, with more sensitive service and close-knit communities often
prompting users to adopt different password behaviors [4, 54, 94].
4.2 Dataset Analysis
This section presents the main insights obtained from our statistical
analysis of the selected datasets.
4.2.1 Password Length Analysis .Table 2 shows that the most
common password lengths are between 6 and 10 characters. Taobao,
000webhost, LinkedIn, and Libero enforce a stricter policy (len >= 6),
with very few passwords shorter than 6 characters. The Cumulative
Distribution Function (CDF) reveals that over 50% of passwords
are at most 8 characters long, with 8 being, on average, the most
frequently chosen length. Notably, the CDF rapidly increases up to
12 characters, suggesting that passwords longer than 12 characters
are rare. Based on these observations, we selected passwords with
maximum lengths of 8, 10, and 12 in our experiments.
4.2.2 Password Patterns Analysis .Based on common password
rules and user behaviors, we defined 19 patterns, listed in Table 3,
to characterize password patterns across the datasets. For a de-
tailed distribution analysis, see Table 8 in the Appendix. In most
datasets, letter-only passwords are highly prevalent and are al-
most always exclusively lowercase, whereas uppercase-only or
mixed-case passwords appear infrequently, apart from 000webhost,
likely influenced by a stricter password policy enforced prior to
the breach. In European and American datasets, digit-only pass-
words are less frequent than letter-only ones, whereas Taobao (from
China) shows a higher prevalence of digit-only passwords, consis-
tent with prior studies [ 7,64,109]. This is likely due to users being
less familiar with the Latin alphabet. Passwords comprised entirelyTable 3: Selected patterns and their description.
ID Description ID Description
r1 Letters only. r10 Starts letter ends digit.
r2 Lowercase letters only. r11 Starts letter ends special.
r3 Uppercase letters only. r12 Starts digit then only letters.
r4 Digits only. r13 Starts digit ends special.
r5 Special only. r14 Starts and ends with digit.
r6 Letters and digits. r15 Starts special, then only letters.
r7 Letters and special. r16 Starts and ends with special.
r8 Digits and special. r17 Starts special, ends digit.
r9 Letters, digits, and special. r18 Ends with ’!’.
r19 Ends with ’1’.
of special characters are rarely used, as they are hard to remem-
ber, and users prioritize usability over security. A common pattern
across all datasets is the combination of letters and digits. In 000web-
host≈93%of passwords follow this pattern, suggesting a policy
requiring at least two character classes. Users often find password
policies frustrating [ 60,92,93] and tend to fall into predictable
patterns to comply with these requirements, such as appending a
trailing digit [ 109,112]. Our analysis supports this observation, as
passwords ending with a digit are highly prevalent, with nearly
half of all passwords following this pattern and ‘1’ being widely
used as a final character.
4.2.3 Top-10 Passwords Analysis .We analyzed the top 10 pass-
words in each dataset, finding that common choices like ‘123456’
appear across almost all datasets. Notably, password preferences
vary by region: European users favor lowercase-only passwords,
Chinese users often choose digit-based passwords resembling words
phonetically, and Russian users prefer keyboard patterns. Table 9
in the Appendix presents the top 10 most common passwords.
4.2.4 Frequency Distribution .Figure 1 illustrates the frequency
distribution of passwords for RockYou, 000webhost, Mail.ru, and
Libero. To enhance readability and reduce the long-tail effect caused
by less frequent passwords, we focus on passwords appearing at
least three times in the dataset. A clear trend clearly emerges: a small
subset of passwords are highly common, while the majority are rare.
This behavior is characteristic of a Zipf-like distribution, where the
frequency of a sample is inversely proportional to its rank. This
observation aligns with the findings of Wang et al. [ 109], who first
demonstrated that Zipf’s law accurately models the distribution of
human-chosen passwords.
5 Experiments
This section presents our evaluation, leveraging MAYA to answer
the research questions presented in Section 3.4.
5.1 RQ1 - How Do Different Settings Influence
Models Performance?
We examine the impact of two key parameters—maximum password
length and the number of generated passwords—on model perfor-
mance. Specifically, we compute the weighted average percentage
of guessed passwords across all datasets, considering maximum
lengths of 8, 10, and 12 characters. For each length, we evaluate
11 generation quantities, ranging from 106to5×108passwords.
Results are shown in Figure 2. As expected, performance declines
with increasing password length. However, this decline exhibits
MAYA: Addressing Inconsistencies in Generative Password Guessing through a Unified Benchmark Conference’17, July 2017, Washington, DC, USA
0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08
Fraction of passwords101102103104105Frequencies
(a) RockYou
0.00 0.01 0.02 0.03 0.04 0.05 0.06
Fraction of passwords101102103104Frequencies (b) 000webhost
0.00 0.01 0.02 0.03 0.04 0.05
Fraction of passwords101102103104105Frequencies (c) mailru
0.00 0.01 0.02 0.03 0.04 0.05 0.06
Fraction of passwords101102103Frequencies (d) Libero
Figure 1: Password frequency distribution. The x-axis value is computed over the total number of passwords without duplicates.
10^6 10^7 10^8 5*10^8
# generated passwords102030405060% of guessed passwords
fla
passflow
passgan
passgpt
plr-gan
vgpt2
(a) Password Length 8
10^6 10^7 10^8 5*10^8
# generated passwords102030405060% of guessed passwords
fla
passflow
passgan
passgpt
plr-gan
vgpt2 (b) Password Length 10
10^6 10^7 10^8 5*10^8
# generated passwords102030405060% of guessed passwords
fla
passflow
passgan
passgpt
plr-gan
vgpt2 (c) Password Length 12
Figure 2: Percentage of guessed passwords across three password lengths (8, 10, and 12) and eleven generation quantities.
10^6 10^7 10^8 5*10^8
# generated passwords0102030405060% of guessed passwords
fla
passflow
passgan
passgpt
plr-gan
vgpt2
JtR
hashcat
(a) Rockyou
10^6 10^7 10^8 5*10^8
# generated passwords05101520253035% of guessed passwords
fla
passflow
passgan
passgpt
plr-gan
vgpt2
JtR
hashcat (b) Linkedin
10^6 10^7 10^8 5*10^8
# generated passwords051015202530% of guessed passwords
fla
passflow
passgan
passgpt
plr-gan
vgpt2
JtR
hashcat (c) 000webhost
10^6 10^7 10^8 5*10^8
# generated passwords0102030405060% of guessed passwords
fla
passflow
passgan
passgpt
plr-gan
vgpt2
JtR
hashcat (d) Ashley Madison
10^6 10^7 10^8 5*10^8
# generated passwords01020304050% of guessed passwords
fla
passflow
passgan
passgpt
plr-gan
vgpt2
JtR
hashcat
(e) Gmail
10^6 10^7 10^8 5*10^8
# generated passwords01020304050% of guessed passwords
fla
passflow
passgan
passgpt
plr-gan
vgpt2
JtR
hashcat (f) Mailru
10^6 10^7 10^8 5*10^8
# generated passwords010203040% of guessed passwords
fla
passflow
passgan
passgpt
plr-gan
vgpt2
JtR
hashcat (g) Taobao
10^6 10^7 10^8 5*10^8
# generated passwords0102030405060% of guessed passwords
fla
passflow
passgan
passgpt
plr-gan
vgpt2
JtR
hashcat (h) Libero
Figure 3: Comparison between traditional methods and generative models across 8 datasets.
diminishing returns: while the drop from 8 to 10 characters is sub-
stantial, averaging 10.46%, the decrease from 10 to 12 characters
is markedly smaller, at just 2.15%, indicating a plateau as length
increases. Among all models, FLA consistently outperforms the oth-
ers, successfully guessing a substantial portion of passwords with
relatively few guesses. Notably, FLA requires fewer than 107gen-
erated passwords to outperform all other models except PassGPT.
PassGPT, while initially comparable to PLR-GAN, PassFlow, andPassGAN, exhibits a much steeper growth curve, ultimately closing
the gap with FLA after 5×107guesses. When considering few
guesses, PLR-GAN, PassFlow, and PassGAN exhibit nearly identical
performance, while VGPT2 initially underperforms. As the num-
ber of guesses increases, differences between the models become
more pronounced: PLR-GAN begins to outperform PassFlow, and
VGPT2—despite a weak start—shows a notable boost, improving at
a faster rate than PassFlow and PLR. In contrast, PassGAN gains the
Conference’17, July 2017, Washington, DC, USA William Corrias, Fabio De Gaspari, Dorjan Hitaj, and Luigi V. Mancini
1e6 2e6 3e6 5e6 1e7 2e7 4e7
train set sizes246810121416% of guessed passwords
linkedin
mailru
rockyou
taobao
(a) PassGAN
1e6 2e6 3e6 5e6 1e7 2e7 4e7
train set sizes6810121416182022% of guessed passwords
linkedin
mailru
rockyou
taobao (b) PLR-GAN
1e6 2e6 3e6 5e6 1e7 2e7 4e7
train set sizes7.510.012.515.017.520.022.5% of guessed passwords
linkedin
mailru
rockyou
taobao (c) PassFlow
1e6 2e6 3e6 5e6 1e7 2e7 4e7
train set sizes1520253035404550% of guessed passwords
linkedin
mailru
rockyou
taobao
(d) PassGPT
1e6 2e6 3e6 5e6 1e7 2e7 4e7
train set sizes2.55.07.510.012.515.017.5% of guessed passwords
linkedin
mailru
rockyou
taobao (e) VGPT2
1e6 2e6 3e6 5e6 1e7 2e7 4e7
train set sizes2030405060% of guessed passwords
linkedin
mailru
rockyou
taobao (f) FLA
Figure 4: Percentages of guessed passwords across various training dataset sizes, evaluated on four different datasets.
least from additional guesses, with a relatively slow improvement
rate.
While it is evident that generating more passwords leads to an
increased number of matches, we delve deeper into this trend by
analyzing the marginal gain —defined as the percentage increase in
guessed passwords between two generation intervals. We analyze
both total gains (relative to the overall number of correct matches)
andrelative gains (relative to the previous number of matches). Due
to space constraints, detailed results are reported in Table 11 in
the Appendix. Overall, all models exhibit clear diminishing returns,
with sub-linear growth in successful guesses. Match rates grow
rapidly in the early stages, then taper off as the space of common
passwords is exhausted. Since this pattern is consistent across all
maximum lengths, and considering that over 92% of passwords
are 12 characters or fewer (see Table 2), we limit our subsequent
analysis to his length without loss of generality.
5.2 RQ2 - Are Generative Models Truly Better
Than Traditional Tools?
We compare generative models with rule-based guessing tech-
niques. Specifically, we consider Hashcat using ‘Unicorn Rules’
and John the Ripper (JtR) ‘Wordlist’ rules. Additional details on
these rules and rationale are provided in Appendix B. As illustrated
in 3, traditional tools exhibit strong performance in smaller datasets,
such as Ashley Madison and Libero, where Hashcat outperforms
PassGPT, and JtR ranks immediately below it. However, as dataset
size increases, the advantage shifts towards generative models. The
performance gap between PassGPT and Hashcat widens signifi-
cantly, with PassGPT emerging as the only model approaching
FLA’s performance. Meanwhile, JtR consistently lags behind as the
least effective approach. We hypothesize that this behavior arises
from the underlying generation strategies employed by traditional
tools. Unicorn Rules are ordered by efficacy, and we selected the first𝑛passwords generated by Hashcat accordingly. However, Hashcat
applies a rule-first strategy: it processes all rules for the current
password before moving to the next. As the dataset size increases,
a smaller portion of the test set passwords are transformed, concen-
trating Hashcat’s guesses in a small subset of the overall password
space. JtR follows a password-first strategy, but WordList’s rules
are unsorted, so we randomly sampled 𝑛passwords from the gen-
erated set. In larger datasets, the number of generated candidates
increases substantially, diluting the proportion of successful guesses
and making it less likely to sample correct guesses.
To streamline subsequent analyses, we fix the number of gen-
erated passwords at 5×108generated passwords, as this provides
the best results for most models.
5.3 RQ3 - How and to What Extent Does the
Size of the Training Dataset Affect Model
Performance?
We assess the ability of generative models to capture the full pass-
word distribution when trained on different data subset sizes. Mod-
els were trained on up to seven different subset sizes across four
datasets, with a minimum initial size of 1M passwords. Results in
Figure 4 illustrate the models’ varying performance. Transformer-
based models (PassGPT and VGPT2) consistently improve as the
training dataset size increases, with VGPT2, in particular, struggling
on smaller subsets. FLA also generally benefits from larger training
subsets, although the rate of improvement tapers off between 3𝑒6
and1𝑒7passwords. We also observe an anomalous behavior on
LinkedIn, where performance drops significantly after 1𝑒7—or 25%
of the dataset size. PLR-GAN excels on small training subsets, but
its performance declines around 30%of the dataset size before im-
proving again, ultimately achieving its best performance on the full
dataset. PassFlow remains the most consistent model across sub-
set sizes, with minimal performance variation. Lastly, In contrast,
MAYA: Addressing Inconsistencies in Generative Password Guessing through a Unified Benchmark Conference’17, July 2017, Washington, DC, USA
Table 4: Percentage of guessed passwords in the cross-community scenario.
PassGAN PLR-GAN PassFlow PassGPT VGPT2 FLA
Train / Test Link Rock 000W Link Rock 000W Link Rock 000W Link Rock 000W Link Rock 000W Link Rock 000W
Linkedin 3.99% 6.51% 1.80% 8.44% 8.77% 3.65% 7.10% 6.61% 1.93% 28.58% 36.21% 16.90% 6.56% 11.48% 2.95% 36.37% 45.09% 19.01%
RockYou 4.72% 12.36% 1.59% 8.77% 19.65% 4.65% 8.16% 18.46% 3.55% 22.08% 48.85% 13.15% 6.93% 17.90% 2.84% 31.53% 60.47% 17.31%
000Webhost 2.27% 4.10% 2.63% 3.70% 5.56% 5.22% 6.58% 11.44% 2.69% 10.41% 13.77% 20.24% 1.87% 3.15% 2.59% 22.81% 26.72% 32.28%
Table 5: Percentage of guessed passwords in the cross-culture scenario.
PassGAN PLR-GAN PassFlow PassGPT VGPT2 FLA
Train / Test Rock. Mail. Taob. Rock Mailr Taob Rock Mailr Taob Rock Mailr Taob Rock Mailr Taob Rock Mailr Taob
RockYou 12.36% 8.43% 5.57% 19.65% 13.22% 9.10% 18.46% 14.86% 9.83% 48.85% 22.30% 13.65% 17.90% 11.40% 6.52% 60.47% 30.10% 18.71%
Mailru 7.39% 16.16% 4.72% 10.78% 21.30% 7.11% 16.35% 23.24% 13.25% 15.82% 40.35% 10.74% 6.89% 16.90% 4.11% 26.98% 54.95% 16.36%
Taobao 6.41% 7.39% 12.11% 9.37% 10.11% 16.82% 19.55% 20.20% 18.49% 13.26% 13.61% 30.80% 7.74% 8.16% 12.56% 23.77% 20.94% 45.53%
top 5% top 10% bottom 90%
test-set password frequency (%)020406080% of guessed passwordsfla
passflow
passgan
passgpt
plr-gan
vgpt2
Figure 5: Percentage of guessed passwords and standard de-
viation across three different password frequency groups.
PassGAN exhibits highly variable results, with performance peaks
at different dataset sizes, making its behavior less predictable.
Overall, transformer models show the most significant improve-
ment with increasing training data, whereas other architectures
tend to exhibit flat or minimal gains. This finding challenges the
conventional assumption that more training data universally leads
to better performance.
5.4 RQ4 - Can Models Generalize To Different
Communities and/or Cultures?
We investigate the generalization capabilities of generative mod-
els by evaluating them in two cross-dataset scenarios: (1) cross-
community and (2) cross-culture. In the cross-community setting,
we use three datasets—RockYou, 000webhost, and LinkedIn—that
share the same language but represent distinct user communities.
For the cross-culture setting, we used RockYou, Mailru, and Taobao,
which differ in language, cultural background, and community.
5.4.1 Cross-community .As shown in Table 4, Cross-community
generalization is strongly model- and dataset-dependant. PassGAN,
PLR-GAN, PassFlow, and VGPT2 achieve their highest performance
on LinkedIn when trained on RockYou, suggesting that the distri-
bution learned from RockYou generalizes well despite community
differences. However, stronger models such as PassGPT and FLA
experience notable performance drops in the same scenario. This
contrast leads us to hypothesize that the apparent generalization
4 5 6 7 8 9 10 11 12
test-set password length020406080100% of guessed passwordsfla
passflow
passgan
passgpt
plr_gan
vgpt2Figure 6: Guessed passwords by length. Dots represent the
percentage of generated passwords for each length.
success of weaker models stems from their limited capacity to cap-
ture fine-grained details of the training distribution—resulting in
broader, but less accurate, generalization. We also note how all
approaches struggle to generalize when training on 000webhost,
highlighting that performance significantly degrades when the
source and target distributions diverge too strongly (see Table 8).
5.4.2 Cross-culture .In the cross-culture scenario, model perfor-
mance follows a more expected pattern, as illustrated in Table 5:
models trained on one cultural or linguistic context tend to perform
significantly worse when evaluated on datasets from a different
one. This highlights the challenges of generalizing across culturally
distinct password distributions and underscores the importance of
training data alignment with the target population. However, some
interesting behaviors still emerge. Despite the challenging setting,
generative models still manage to guess a non-negligible portion of
the target passwords. Notably, models that typically underperform
in other settings—such as PassFlow—can surpass stronger models
like PassGPT when there is a significant mismatch between source
and target distributions. This suggests that certain models may
possess greater flexibility or robustness in the face of distributional
shifts, even if they are less effective under ideal, in-distribution
conditions.
Conference’17, July 2017, Washington, DC, USA William Corrias, Fabio De Gaspari, Dorjan Hitaj, and Luigi V. Mancini
5.5 RQ5 - Are Models Limited to Guessing Only
Simple and Common Passwords?
We evaluate the models’ ability to guess both common/rare and
simple/complex passwords through three complementary analyses.
5.5.1 Analysis by Password Frequency .From each test dataset,
we created three subsets based on password frequency: top 5%, top
10%, and bottom 90%. Since nearly 90%of test passwords across all
datasets are unique (see Figure 1), these subsets allow us to evaluate
models under three scenarios: (1) very common, (2) common, and (3)
rare passwords. We excluded LinkedIn and Ashley Madison, as they
consist entirely of unique passwords. Results, shown in Figure 5,
are reported as weighted averages. As expected, all models perform
better on common passwords. While the drop from the top 5%to the
top10%is small, performance declines sharply when focusing on the
bottom 90%. Still, models are surprisingly capable of guessing rare
passwords, with PassGPT and FLA guessing close to 40%and50%,
respectively. We leverage PassFlow’s encoder to visualize password
embeddings and observe that the bottom 90%of passwords tend
to form clusters around the top 10%. Since PassFlow’s latent space
is smooth [ 77], this behavior suggests that many rare passwords
are slight variations of frequently used ones, which may explain
why models are still able to guess a non-negligible portion of them
despite their low frequency.
5.5.2 Analysis by Password Length .We analyzed matches by
password length, ranging from 4 to 12 characters. Results, com-
puted using a weighted average across all datasets, are shown in
Figure 6. Notably, all models perform well on short passwords, with
FLA nearly achieving a 100% match rate. However, performance
declines as length increases. Only FLA and PassGPT maintain a sub-
stantial guessing rate beyond 8 characters. Notably, consistent with
findings from RQ1 , PassGPT surpasses FLA from length 11 onward.
PassFlow initially performs on par with PassGPT for shorter lengths
but undergoes a sharp decline, becoming the weakest performer
from length 8 onward.
5.5.3 Analysis by Password Patterns .We evaluated the models’
ability to guess passwords based on the structural patterns defined
in Table 3. The results, presented in Table 6 as a weighted average
across all datasets, show that PassGPT and FLA consistently achieve
the strongest performance, standing out as the only models capable
of successfully guessing passwords across all defined patterns. In
contrast, the remaining models show limited pattern coverage, par-
ticularly struggling with complex password structures that include
special characters (r5, r7, r8) or uncommon character combinations
(r11, r12, r13). GAN-based models fare especially poorly on pass-
words composed entirely of special characters (r5), significantly
underperforming PassFlow and VGPT2. This suggests a clear limita-
tion in capturing rare or unconventional character sets. In contrast,
all models exhibit strong performance on digit-only passwords (r4),
indicating that simple patterns are consistently learned regardless
of the architecture.
FLA PassFlow PassGAN PassGPT PLR-GAN VGPT2FLA PassFlow PassGAN PassGPT PLR-GAN VGPT21.0000 0.0341 0.0289 0.1003 0.0385 0.0205
0.0341 1.0000 0.0119 0.0147 0.0135 0.0123
0.0289 0.0119 1.0000 0.0158 0.0135 0.0070
0.1003 0.0147 0.0158 1.0000 0.0198 0.0095
0.0385 0.0135 0.0135 0.0198 1.0000 0.0091
0.0205 0.0123 0.0070 0.0095 0.0091 1.0000
0.00.20.40.60.81.0
Jaccard Index(a) Jaccard Index
FLA PassFlow PassGAN PassGPT PLR-GAN VGPT2FLA PassFlow PassGAN PassGPT PLR-GAN VGPT20.0000 0.0076 0.0060 0.0382 0.0085 0.0053
0.0076 0.0000 0.3004 0.0465 0.1916 0.2166
0.0060 0.3004 0.0000 0.0348 0.1957 0.2853
0.0382 0.0465 0.0348 0.0000 0.0519 0.0330
0.0085 0.1916 0.1957 0.0519 0.0000 0.1832
0.0053 0.2166 0.2853 0.0330 0.1832 0.0000
0.00.20.40.60.81.0
Mergeability Index (b) Mergeability Index
Figure 7: Heatmaps of the Jaccard Index (a) and Mergeability
index (b).
5.6 RQ6 - To What Extent Do the Distributions
Learned by Different Models Align? Can We
Combine Models to Maximize Effectiveness?
We assess the extent to which generative models’ learned distri-
butions overlap and investigate the effectiveness of combining
multiple models to enhance guessing performance.
5.6.1 RQ6.1 - To What Extent Do the Distributions Learned
by Different Models Align? We investigate the first aspect of
this RQ utilizing two primary metrics: the Jaccard Index and the
Mergeability Index. Their mathematical definitions are provided in
Appendix C, in Equations 3 and 4, respectively. The Jaccard Index
provides a quantitative measure of the overlap between two sets of
generated passwords, with a value closer to 1 indicating that the
two models produce similar distributions, and a value closer to 0 in-
dicating that they generate distinct sets of passwords. As illustrated
in Figure 7a, the FLA-PassGPT pair is the only combination with a
Jaccard Index greater than 0.1, suggesting that these two models
share some similarities in their password generation patterns. All
other pairs exhibit much lower Jaccard values. Interestingly, despite
PassGAN and PLR-GAN both being based on GANs, their Jaccard
Index is quite low. Similarly, VGPT2-PassGPT also shows a low
Jaccard Index. These results highlight that, even within the same
architectural family, models may produce highly distinct password
distributions, suggesting that combining multiple models could be
beneficial for maximizing coverage of the password space.
While the Jaccard Index provides insights into the overlap of
generated passwords, it does not account for their effectiveness in
terms of actual matches with real-world data. To address this, we
introduce the Mergeability Index, a complementary metric that mea-
sures the benefits of combining the output of two models based on
their successful guesses. It captures the performance improvement
achieved by merging the guesses relative to the best-performing
model. A Mergeability Index close to 0 indicates that the models
guess mostly the same set of passwords, while a value close to 1
indicates largely distinct matching password sets. The results are
shown in Figure 7b. Interestingly, FLA exhibits the lowest Merge-
ability across all models, indicating that its guessed passwords are
generally a superset of those generated by the other models. This
suggests that FLA has already captured a significant portion of
the password space, and merging it with other models provides
only limited additional benefit. In contrast, PassFlow, PassGAN,
MAYA: Addressing Inconsistencies in Generative Password Guessing through a Unified Benchmark Conference’17, July 2017, Washington, DC, USA
Table 6: Percentage of guessed passwords following specific patterns.
r1 r2 r3 r4 r5 r6 r7 r8 r9 r10 r11 r12 r13 r14 r15 r16 r17 r18 r19
PassGAN 16.73% 18.04% 1.17% 48.53% 3.33% 8.24% 0.91% 1.17% 0.09% 9.94% 0.85% 3.05% 1.35% 43.52% 0.21% 0.02% 0.42% 2.74% 18.71%
PLR-GAN 19.28% 20.57% 4.33% 55.06% 2.50% 10.91% 0.36% 0.33% 0.31% 12.35% 0.44% 3.73% 0.42% 50.64% 0.21% 0.01% 0.82% 0.00% 21.83%
PassFlow 12.83% 13.66% 5.19% 46.88% 16.67% 6.67% 1.05% 0.38% 0.10% 5.92% 0.54% 7.70% 0.42% 44.28% 25.20% 0.00% 0.13% 0.00% 17.25%
PassGPT 45.01% 46.57% 16.74% 62.21% 51.67% 34.43% 9.07% 40.31% 11.13% 39.74% 13.18% 25.28% 40.63% 58.75% 36.71% 20.33% 12.20% 7.04% 45.15%
VGPT2 6.78% 7.13% 5.31% 25.54% 17.50% 4.02% 0.46% 1.72% 0.36% 3.83% 0.73% 1.17% 1.96% 23.19% 5.51% 0.05% 0.39% 0.01% 9.69%
FLA 62.64% 64.33% 18.10% 73.80% 30.83% 51.10% 14.63% 51.23% 18.71% 58.39% 22.23% 45.41% 50.39% 70.99% 16.30% 15.22% 16.37% 22.94% 62.12%
n1:FLAn2:PassGPTn3:PLRn4:PassFlown5:PassGANn6:VGPT2
combinations of models0123456gain (percentage points)
RockYou
000webhost
AshleyMad
Gmail
Libero
LinkedIn
Mailru
T aobao
Average
Figure 8: Percentage points increase of guessed passwords
for the best model combinations.
PLR-GAN, and VGPT2 show high Mergeability, with their varying
combinations improving successful guesses by 18% to 30%. This
suggests that these models guess highly distinct sets of passwords,
meaning they are each capturing different aspects of the password
space.
5.6.2 RQ6.2 - Can We Combine Models to Maximize Effec-
tiveness? We assess the potential of a multi-model attack by evalu-
ating the performance gain achieved through the combination of the
generated password sets from 𝑛models. Our results, presented in
Figure 8, show the relative gain with respect to the best-performing
single model, computed as percentage points on the test set. We
identify the following optimal model combinations for different
values of𝑛:𝑛1={𝐹𝐿𝐴},𝑛2=𝑛1+𝑃𝑎𝑠𝑠𝐺𝑃𝑇 ,𝑛3=𝑛2+𝑃𝐿𝑅-𝐺𝐴𝑁 ,
𝑛4=𝑛3+𝑃𝑎𝑠𝑠𝐹𝑙𝑜𝑤 ,𝑛5=𝑛4+𝑃𝑎𝑠𝑠𝐺𝐴𝑁 , and𝑛6=𝑛5+𝑉𝐺𝑃𝑇 2.
In datasets like LinkedIn and RockYou, combining models yields
significant gains over FLA alone—especially PassGPT. Remarkably,
combining PassGPT and FLA provides a larger average improve-
ment than adding the remaining four models combined. These
results mirror those presented in Figure 7b, indicating that FLA and
PassGPT capture somewhat complementary parts of the password
space. However, in datasets such as 000webhost and Mailru, FLA
alone performs nearly as well as the combination of all models,
with only a 1percentage point improvement in the multi-model
attack.
5.7 RQ7 - Do Models Truly Capture the
Characteristics of Human-Like Passwords?
We assess the similarity between the generated password distribu-
tion and that of human-created passwords using multiple metrics
and an analysis of their length distributions.Table 7: Distance between human- and generative model-
created passwords.
Models CNN Div 𝛼−Precision 𝛽−Recall Auth IMD MTopDiv
PassGAN
 16%
 19%
 4%
 14%
 65%
 1%
PLR-GAN
 6%
 -4%
 3%
 11%
 3%
 0%
PassFlow
 56%
 61%
 52%
 16%
 200%
 36%
PassGPT
 2%
 3%
 1%
 6%
 0%
 0%
VGPT2
 29%
 53%
 34%
 4%
 135%
 12%
FLA
 12%
 -15%
 -1%
 31%
 172%
 0%
5.7.1 Metrics-Based Evaluation .We identified four metrics to
evaluate the human-likeness of the generated passwords, each cap-
turing different aspects and characteristics:
CNN Divergence [ 39]:CNN Divergence utilizes a critic network
trained to differentiate between real and synthetic data, with the loss
serving as an estimator of their divergence. However, its reliability
is influenced by the dataset size, and it may show biases when used
to evaluate models trained with similar objectives, like GANs.
IMD [ 103]:IMD is an intrinsic, multi-scale metric that compares
the data manifolds of real and generated samples and is applicable
across diverse domains. However, its effectiveness is highly sensi-
tive to the choice of feature representation, which can introduce
bias into the distance estimation.
𝛼-Precision𝛽-Recall Authenticity [ 5]:This metric character-
izes distributions along three key dimensions: fidelity (precision)
and diversity (recall) of the generated data, and the generalization
(authentication) capabilities of the models. It is effective in iden-
tifying different types of mode failures, making it a versatile tool
applicable across various domains. However, as it relies on a neural
network to embed data into a hypersphere, it may introduce bias
through the learned latent representation, potentially affecting the
estimation of the radius used to distinguish inliers from outliers.
MTopDiv [ 8]:MTopDiv measures topological discrepancies be-
tween real and synthetic distributions at multiple scales, detecting
various degrees of mode failures. Notably, it does not rely on pre-
trained networks, ensuring greater flexibility in its application.
To contextualize the metric values, we define two baselines: (1) a
soft lower bound (optimal case), defined as the metric value between
test and training passwords; (2) a soft upper bound (worst case),
defined as the metric value between randomly generated and test
passwords. For each model, we normalize the metric value between
lower ( 0%) and upper ( 100% ) bounds, as displayed in Table 7. Val-
ues close to zero indicate closer alignment between the generated
and human-created password distributions. GAN-based models
demonstrate generally good performance across all metrics, with
PLR consistently achieving lower values than PassGAN. PassGPT
Conference’17, July 2017, Washington, DC, USA William Corrias, Fabio De Gaspari, Dorjan Hitaj, and Luigi V. Mancini
1-5 6 7 8 9 10 11 12
Password Lengths05101520253035Frequencies (%)
fla
passflow
passgan
passgpt
plr-gan
vgpt2
REAL_AVG
Figure 9: Password length distributions.
generates passwords that closely resemble human-created ones,
likely due to its underlying GPT2 architecture, which is well-suited
for modeling textual distributions. In contrast, PassFlow demon-
strates the weakest performance, particularly in the IMD metric,
where its score exceeds the soft upper bound by a factor of two.
We attribute this behavior to PassFlow’s Gaussian Smoothing (GS)
strategy, which introduces random perturbations in the generation
process. While GS enhances uniqueness [ 77], it artificially distorts
the distribution of generated passwords, ultimately reducing their
quality. VGPT2 and FLA exhibit similar trends, performing strongly
across most metrics except for IMD, where elevated values across
most models suggest that they all struggle to fully capture certain
characteristics of human passwords. Conversely, MTopDiv consis-
tently yields low values, indicating that the models effectively avoid
common mode failures, such as mode-dropping, mode-collapse, and
mode-invention.
5.7.2 Length Distribution in Generated Passwords .Figure 9
compares the length distribution of generated passwords with that
of real passwords. Among the models, PassFlow, FLA, and VGPT2
show the largest divergence from the real distribution: PassFlow
tends to generate shorter passwords, FLA overestimates the prob-
ability of 8-character passwords, and VGPT2 of longer ones. In
contrast, PassGAN, PLR-GAN, and PassGPT produce distributions
that more closely align with real data, reinforcing the observations
from our metric-based evaluation.
5.7.3 Why Does IMD Yield High Values? Figure 9 reveals a
clear correlation between length distribution and IMD values. Pass-
Flow, which exhibits the largest deviation from the real distribution,
also records the highest IMD score. A similar trend is observed for
FLA and VGPT2. Conversely, models such as PassGAN, PLR-GAN,
and PassGPT, which more closely replicate the real-world length
distribution, achieve lower IMD scores. Interestingly, Table 7 shows
that models such as PassFlow and FLA yield higher IMD scores
than those associated with the random-password soft upper bound.
We posit that this counterintuitive result stems from the uniform
length distribution of random passwords. Although this distribution
diverges significantly from that of real passwords, it nonetheless
includes longer passwords that PassFlow and FLA struggle to gen-
erate. These observations suggest that IMD is highly sensitive to
length distribution, favoring models that replicate it more accu-
rately. For an extended analysis of IMD, we refer to Appendix C.6 Insights and Lessons Learned
This section outlines key insights obtained in our study.
Sequential Architectures Perform Best .FLA and PassGPT,
based on LSTM and GPT2, consistently outperform other archi-
tectures across all scenarios. This underscores the advantages of
sequential models in text-related tasks such as password genera-
tion, as they more effectively capture sequential dependencies and
generate complex, more accurate guesses than other architectures.
Generative Models Surpsass Traditional Tools .Early gen-
erative approaches—such as PassGAN, PassFlow, and PLR-GAN—
struggled to match the performance of traditional tools. However,
the emergence of transformer-based architectures has shifted the
research landscape toward models capable of consistently outper-
forming them. PassGPT, the most recent approach evaluated in our
study, largely surpasses JtR and Hashcat. As research progresses,
more advanced LLMs are likely to further improve = password-
guessing effectiveness, whereas traditional tools have likely reached
their performance ceiling.
Stricter Policies Mean Safer Passwords .Our analysis reveals
that each dataset exhibits a distinct distribution, with some being
significantly easier to guess than others. Notably, 000webhost and
LinkedIn—both of which follow stricter password policies regarding
length and complexity—emerge as the most challenging datasets.
These findings reinforce the conventional understanding that even
modestly strict password policies enhance security—an effect that
holds true even in the context of generative models.
Guessing Complex/Long Passwords Remains Challenging .
Four out of six models struggle to guess complex passwords—those
containing special characters or multiple character classes—and
longer passwords exceeding eight characters. Only FLA and Pass-
GPT achieve a non-negligible success rate in these cases. These
findings highlight the persistent challenge posed by long and com-
plex passwords, revealing a significant limitation of these models.
Rare Does Not Mean Hard to Guess .Rare passwords, such
as those appearing only once in the dataset, are not necessarily
hard to guess, especially if they are semantically similar to common
passwords.
Models Go Beyond Memorization .While performance gener-
ally declines relative to in-distribution settings, models still gener-
alize well to unseen distributions, indicating they go beyond simple
memorization. Additionally, some models exhibited greater robust-
ness to distribution shifts, even if they performed worse under ideal,
in-distribution settings.
Models Generate and Match Distinct Passwords .Even when
trained on the same dataset, models generate and match distinct
sets of passwords. This diversity enables multi-model attacks that
outperform the best individual model.
Models Generate Human-Like Passwords .Generative mod-
els effectively learn to generate passwords that closely resemble
human-created ones, making them valuable for a variety of tasks
beyond just password-guessing.
MAYA: Addressing Inconsistencies in Generative Password Guessing through a Unified Benchmark Conference’17, July 2017, Washington, DC, USA
7 Conclusion
This work presented MAYA, a framework designed to comprehen-
sively evaluate password-guessing approaches. It includes stan-
dardized guidelines and advanced testing scenarios to ensure fair,
in-depth comparisons and reveal the strengths and limitations of
different models. By analyzing eight real-world password leaks and
thoroughly evaluating various password-guessing approaches, we
addressed seven key research questions, offering insights into the
factors influencing user password choices and the current state
of generative password-guessing research. We believe future re-
search could greatly benefit from MAYA, as it can accelerate the
development of new approaches beyond password-guessing, such
as enhancing password security mechanisms like honeywords and
password strength meters. MAYA is intended to foster academic
advancements in password security, not to facilitate or promote
malicious activity.
References
[1]“Mail.ru,” CyberInsurance , 2014, accessed: Apr. 14, 2025. [Online]. Available:
https://www.cyberinsurance.com/breaches/mailru/
[2]“2012 linkedin breach had 117 million emails and passwords stolen,
not 6.5m,” Trend Micro , 2016, accessed: Apr. 14, 2025. [Online]. Avail-
able: https://www.trendmicro.com/vinfo/us/security/news/cyber-attacks/2012-
linkedin-breach-117-million-emails-and-passwords-stolen-not-6-5m
[3]“Passgan, l’ai che può crackare qualsiasi password in pochi
minuti,” Fastweb , 2023, accessed: Apr. 11, 2025. [Online]. Avail-
able: https://www.fastweb.it/fastweb-plus/digital-dev-security/passgan-lai-
che-puo-crackare-qualsiasi-password-in-pochi-minuti/
[4]Y. Abdrabou, J. Schütte, A. Shams, K. Pfeuffer, D. Buschek, M. Khamis, and F. Alt,
“” your eyes tell you have used this password before”: Identifying password reuse
from gaze and keystroke dynamics,” in Proceedings of the 2022 CHI Conference
on Human Factors in Computing Systems , 2022, pp. 1–16.
[5]A. Alaa, B. Van Breugel, E. S. Saveliev, and M. Van Der Schaar, “How faithful is
your synthetic data? sample-level metrics for evaluating and auditing generative
models,” in International Conference on Machine Learning . PMLR, 2022, pp.
290–306.
[6]M. L. Ali, J. V. Monaco, C. C. Tappert, and M. Qiu, “Keystroke biometric systems
for user authentication, ” Journal of Signal Processing Systems , vol. 86, pp. 175–190,
2017.
[7]M. AlSabah, G. Oligeri, and R. Riley, “Your culture is in your password: An
analysis of a demographically-diverse password dataset,” Computers & security ,
vol. 77, pp. 427–441, 2018.
[8]S. Barannikov, I. Trofimov, G. Sotnikov, E. Trimbach, A. Korotin, A. Filippov,
and E. Burnaev, “Manifold topology divergence: a framework for comparing
data manifolds.” Advances in neural information processing systems , vol. 34, pp.
7294–7305, 2021.
[9]F. Bergadano, B. Crispo, and G. Ruffo, “High dictionary compression for proac-
tive password checking,” ACM Transactions on Information and System Security
(TISSEC) , vol. 1, no. 1, pp. 3–25, 1998.
[10] D. Biesner, K. Cvejoski, and R. Sifa, “Combining variational autoencoders and
transformer language models for improved password generation,” in Proceedings
of the 17th International Conference on Availability, Reliability and Security , 2022,
pp. 1–6.
[11] M. Bishop and D. V. Klein, “Improving system security via proactive password
checking,” Computers & Security , vol. 14, no. 3, pp. 233–249, 1995.
[12] U. Bodkhe, J. Chaklasiya, P. Shah, S. Tanwar, and M. Vora, “Markov model for
password attack prevention,” in Proceedings of first international conference on
computing, communications, and cyber-security (IC4S 2019) . Springer, 2020, pp.
831–843.
[13] J. Bonneau, “The science of guessing: Analyzing an anonymized corpus of 70
million passwords,” in 2012 IEEE Symposium on Security and Privacy , 2012, pp.
538–552.
[14] J. Bonneau, C. Herley, P. C. Van Oorschot, and F. Stajano, “The quest to replace
passwords: A framework for comparative evaluation of web authentication
schemes,” in 2012 IEEE symposium on security and privacy . IEEE, 2012, pp.
553–567.
[15] A. Borji, “Pros and cons of gan evaluation measures: New developments,” Com-
puter Vision and Image Understanding , vol. 215, p. 103329, 2022.
[16] T. Brewster, “13 million passwords appear to have leaked from this free web host
- updated,” Forbes , 2015, accessed: Apr. 14, 2025. [Online]. Available: https://www.
forbes.com/sites/thomasbrewster/2015/10/28/000webhost-database-leak/[17] W. Burr, D. Dodson, R. Perlner, S. Gupta, and E. Nabbus, “Nist sp800-63-2: Elec-
tronic authentication guideline,” National Institute of Standards and Technology,
Reston, VA, Tech. Rep , 2013.
[18] C. Castelluccia, M. Dürmuth, and D. Perito, “Adaptive password-strength meters
from markov models. ” in Proceedints of the 19th Annual Networks and Distributed
Systems Symposium, NDSS , 2012.
[19] H. Cheng, W. Li, P. Wang, and K. Liang, “Improved probabilistic context-free
grammars for passwords using word extraction,” in ICASSP 2021-2021 IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP) .
IEEE, 2021, pp. 2690–2694.
[20] M. Cobb, “How machine learning-powered password guessing impacts
security,” TechTarget , 2017, accessed: Apr. 11, 2025. [Online]. Avail-
able: https://www.techtarget.com/searchsecurity/tip/How-machine-learning-
powered-password-guessing-impacts-security
[21] J. Condliffe, “A pair of ais have become very good at guessing your
passwords,” Technology Review News , 2017, accessed: Apr. 11, 2025. [Online].
Available: https://www.technologyreview.com/2017/09/18/67897/a-pair-of-ais-
have-become-very-good-at-guessing-your-passwords/
[22] F. Corti, “Password bucate da passgan: cosa sta succedendo?” Labor Project ,
2023, accessed: Apr. 11, 2025. [Online]. Available: https://laborproject.it/2023/
06/21/password-bucate-passgan-cosa-sta-succedendo/
[23] J. Dani, B. McCulloh, and N. Saxena, “When ai defeats password deception!
a deep learning framework to distinguish passwords and honeywords,” arXiv
preprint arXiv:2407.16964 , 2024.
[24] A. Das, J. Bonneau, M. Caesar, N. Borisov, and X. Wang, “The tangled web of
password reuse.” in NDSS , vol. 14, no. 2014, 2014, pp. 23–26.
[25] M. Dell’Amico, P. Michiardi, and Y. Roudier, “Password strength: An empirical
analysis,” in Proceedings of the 2010 IEEE INFOCOM . IEEE, 2010, pp. 1–9.
[26] L. Dinh, D. Krueger, and Y. Bengio, “Nice: Non-linear independent components
estimation,” arXiv preprint arXiv:1410.8516 , 2014.
[27] L. Dinh, J. Sohl-Dickstein, and S. Bengio, “Density estimation using real nvp,”
International Conference on Learning Representations, ICLR , 2017.
[28] A. Dionysiou, V. Vassiliades, and E. Athanasopoulos, “Honeygen: Generating
honeywords using representation learning,” in Proceedings of the 2021 ACM Asia
Conference on Computer and Communications Security , 2021, pp. 265–279.
[29] M. A. Fauzi, B. Yang, and E. Martiri, “Passgan based honeywords system for
machine-generated passwords database,” in 2020 IEEE 6th Intl Conference on
Big Data Security on Cloud (BigDataSecurity), IEEE Intl Conference on High
Performance and Smart Computing,(HPSC) and IEEE Intl Conference on Intelligent
Data and Security (IDS) . IEEE, 2020, pp. 214–220.
[30] C. Fu, M. Duan, X. Dai, Q. Wei, Q. Wu, and R. Zhou, “Densegan: A password
guessing model based on densenet and passgan,” in Information Security Prac-
tice and Experience: 16th International Conference, ISPEC 2021, Nanjing, China,
December 17–19, 2021, Proceedings 16 . Springer, 2021, pp. 296–305.
[31] S. Furnell, “Assessing password guidance and enforcement on leading websites, ”
Computer Fraud & Security , vol. 2011, no. 12, pp. 10–18, 2011.
[32] R. Gagliardi, “Libero.it password leak - an analysis in-depth,” Scip AG , 2016,
accessed: Apr. 14, 2025. [Online]. Available: https://www.scip.ch/en/?labs.
20180913
[33] X. Gan, D. Li, and H. Chen, “Analysis of words in passwords from three different
countries,” in 2022 IEEE 10th Joint International Information Technology and
Artificial Intelligence Conference (ITAIC) , vol. 10, 2022, pp. 1775–1781.
[34] M. Golla and M. Dürmuth, “On the accuracy of password strength meters,” in
Proceedings of the 2018 ACM SIGSAC conference on computer and communications
security , 2018, pp. 1567–1582.
[35] M. Golla, M. Wei, J. Hainline, L. Filipe, M. Dürmuth, E. Redmiles, and B. Ur, “"
what was that site doing with my facebook password?" designing password-
reuse notifications, ” in Proceedings of the 2018 acm sigsac conference on computer
and communications security , 2018, pp. 1549–1566.
[36] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
A. Courville, and Y. Bengio, “Generative adversarial networks,” Communications
of the ACM , vol. 63, no. 11, pp. 139–144, 2020.
[37] P. A. Grassi, E. Newton, R. Perlner, A. Regenscheid, W. Burr, and J. P. Richer, “Nist
800-63b digital identity guidelines: Authentication and lifecycle management,”
McLean, VA, Tech. Rep , 2017.
[38] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville, “Improved
training of wasserstein gans,” Advances in neural information processing systems ,
vol. 30, 2017.
[39] I. Gulrajani, C. Raffel, and L. Metz, “Towards gan benchmarks which require
generalization,” 2020. [Online]. Available: https://arxiv.org/abs/2001.03653
[40] X. Guo, Y. Liu, K. Tan, W. Mao, M. Jin, and H. Lu, “Dynamic markov model:
Password guessing using probability adjustment method,” Applied Sciences ,
vol. 11, no. 10, p. 4607, 2021.
[41] W. Han, Z. Li, L. Yuan, and W. Xu, “Regional patterns and vulnerability analysis
of chinese web passwords,” IEEE Transactions on Information Forensics and
Security , vol. 11, no. 2, pp. 258–272, 2016.
[42] W. Han, M. Xu, J. Zhang, C. Wang, K. Zhang, and X. S. Wang, “Transpcfg: trans-
ferring the grammars from short passwords to guess long passwords effectively, ”
Conference’17, July 2017, Washington, DC, USA William Corrias, Fabio De Gaspari, Dorjan Hitaj, and Luigi V. Mancini
IEEE Transactions on Information Forensics and Security , vol. 16, pp. 451–465,
2020.
[43] hashcat. [Online]. Available: https://hashcat.net/hashcat/
[44] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition, ”
2016.
[45] X. He, H. Cheng, J. Xie, P. Wang, and K. Liang, “Passtrans: An improved password
reuse model based on transformer,” in ICASSP 2022-2022 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP) . IEEE, 2022, pp.
3044–3048.
[46] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, “Gans
trained by a two time-scale update rule converge to a local nash equilibrium,”
Advances in neural information processing systems , vol. 30, 2017.
[47] K. J. Higgins, “The coolest hacks of 2017,” Dark Reading , 2017, accessed: Apr. 11,
2025. [Online]. Available: https://www.darkreading.com/threat-intelligence/the-
coolest-hacks-of-2017
[48] B. Hitaj, P. Gasti, G. Ateniese, and F. Perez-Cruz, “Passgan: A deep learning
approach for password guessing,” in Applied Cryptography and Network Security:
17th International Conference, ACNS 2019, Bogota, Colombia, June 5–7, 2019,
Proceedings 17 . Springer, 2019, pp. 217–237.
[49] D. Huang, Y. Wang, and W. Chen, “Rlpassgan: Password guessing model based
on gan with policy gradient,” in International Conference on Security and Privacy
in New Computing Environments . Springer, 2021, pp. 159–174.
[50] M. Hutson, “Artificial intelligence just made guessing your password
a whole lot easier.” Science , 2017, accessed: Apr. 11, 2025. [Online].
Available: https://www.science.org/content/article/artificial-intelligence-just-
made-guessing-your-password-whole-lot-easier
[51] S. Z. S. Idrus, E. Cherrier, C. Rosenberger, and P. Bours, “Soft biometrics for
keystroke dynamics: Profiling individuals while typing passwords,” Computers
& Security , vol. 45, pp. 147–155, 2014.
[52] E. Intini, “L’intelligenza artificiale indovina le password,” Focus , 2017, accessed:
Apr. 11, 2025. [Online]. Available: https://www.focus.it/tecnologia/digital-
life/lintelligenza-artificiale-indovina-le-password
[53] A. Janofsky, “Ai could make cyberattacks more dangerous, harder to detect, ” WSJ,
2018, accessed: Apr. 11, 2025. [Online]. Available: https://www.wsj.com/articles/
ai-could-make-cyberattacks-more-dangerous-harder-to-detect-1542128667
[54] S. Jin and M. Dupuis, “Password usage behavior of online users,” in 2024 Cyber
Awareness and Research Symposium (CARS) . IEEE, 2024, pp. 1–6.
[55] A. Juels and R. L. Rivest, “Honeywords: Making password-cracking detectable,”
inProceedings of the 2013 ACM SIGSAC conference on Computer & communica-
tions security , 2013, pp. 145–160.
[56] S. Kar, A. Bamotra, B. Duvvuri, and R. Mohanan, “Keydetect–detection of anom-
alies and user based on keystroke dynamics,” arXiv preprint arXiv:2304.03958 ,
2023.
[57] KeePass. [Online]. Available: https://keepass.info/help/kb/pw_quality_est.html
[58] P. G. Kelley, S. Komanduri, M. L. Mazurek, R. Shay, T. Vidas, L. Bauer, N. Christin,
L. F. Cranor, and J. Lopez, “Guess again (and again and again): Measuring
password strength by simulating password-cracking algorithms,” in 2012 IEEE
symposium on security and privacy . IEEE, 2012, pp. 523–537.
[59] D. P. Kingma, M. Welling et al. , “Auto-encoding variational bayes,” 2013.
[60] S. Komanduri, R. Shay, P. G. Kelley, M. L. Mazurek, L. Bauer, N. Christin, L. F.
Cranor, and S. Egelman, “Of passwords and people: measuring the effect of
password-composition policies,” in Proceedings of the sigchi conference on human
factors in computing systems , 2011, pp. 2595–2604.
[61] M. A. Kramer, “Nonlinear principal component analysis using autoassociative
neural networks,” AIChE journal , vol. 37, no. 2, pp. 233–243, 1991.
[62] B. T. Kuhn and C. Garrison, “A survey of passwords from 2007 to 2009,” in 2009
Information Security Curriculum Development Conference , ser. InfoSecCD ’09.
New York, NY, USA: Association for Computing Machinery, 2009, p. 91–94.
[Online]. Available: https://doi.org/10.1145/1940976.1940994
[63] Y. Li, H. Wang, and K. Sun, “A study of personal information in human-chosen
passwords and its security implications,” in IEEE INFOCOM 2016-The 35th Annual
IEEE International Conference on Computer Communications . IEEE, 2016, pp.
1–9.
[64] Z. Li, W. Han, and W. Xu, “A {Large-Scale}empirical analysis of chinese web
passwords,” in 23rd USENIX Security Symposium (USENIX Security 14) , 2014, pp.
559–574.
[65] M. Londei, “L’ia decifra la maggior parte delle password in pochi
minuti,” Security Info , 2023, accessed: Apr. 11, 2025. [Online]. Available:
https://www.securityinfo.it/2023/04/24/ia-password-rischio/
[66] S. G. Lyastani, M. Schilling, M. Neumayr, M. Backes, and S. Bugiel, “Is fido2
the kingslayer of user authentication? a comparative usability study of fido2
passwordless authentication,” in 2020 IEEE Symposium on Security and Privacy
(SP). IEEE, 2020, pp. 268–285.
[67] J. Ma, W. Yang, M. Luo, and N. Li, “A study of probabilistic password models,”
in2014 IEEE Symposium on Security and Privacy . IEEE, 2014, pp. 689–704.
[68] I. madan, “Up to speed on ai & deep learning: October update,” Hackernoon ,
2017, accessed: Apr. 11, 2025. [Online]. Available: https://hackernoon.com/up-
to-speed-on-deep-learning-october-update-815f5eef0e2b[69] A. Manning, “Researches show how a.i. is the end of passwords as we
know them.” Inverse , 2017, accessed: Apr. 11, 2025. [Online]. Available:
https://www.inverse.com/article/36604-ai-cracking-passwords
[70] P. Mayer, J. Kirchner, and M. Volkamer, “A second look at password
composition policies in the wild: Comparing samples from 2010 and
2016,” in Thirteenth Symposium on Usable Privacy and Security (SOUPS
2017) . Santa Clara, CA: USENIX Association, Jul. 2017, pp. 13–28.
[Online]. Available: https://www.usenix.org/conference/soups2017/technical-
sessions/presentation/mayer
[71] W. Melicher, B. Ur, S. M. Segreti, S. Komanduri, L. Bauer, N. Christin, and L. F.
Cranor, “Fast, lean, and accurate: Modeling password guessability using neural
networks,” in 25th USENIX Security Symposium (USENIX Security 16) , 2016, pp.
175–191.
[72] M. Mimoso, “Deep-learning passgan tool improves password guessing,” Threat
Post, 2017, accessed: Apr. 11, 2025. [Online]. Available: https://threatpost.com/
deep-learning-passgan-tool-improves-password-guessing/128039/
[73] F. Monrose, M. K. Reiter, and S. Wetzel, “Password hardening based on key-
stroke dynamics,” in Proceedings of the 6th ACM Conference on Computer and
Communications Security , 1999, pp. 73–82.
[74] Y. Muliono, H. Ham, and D. Darmawan, “Keystroke dynamic classification using
machine learning for password authorization,” Procedia Computer Science , vol.
135, pp. 564–569, 2018.
[75] A. Narayanan and V. Shmatikov, “Fast dictionary attacks on passwords using
time-space tradeoff,” in Proceedings of the 12th ACM conference on Computer and
communications security , 2005, pp. 364–372.
[76] J. Pagliery, “5 million gmail passwords leaked,” CNN , 2014, accessed: Apr.
14, 2025. [Online]. Available: https://money.cnn.com/2014/09/10/technology/
security/gmail-hack/index.html
[77] G. Pagnotta, D. Hitaj, F. De Gaspari, and L. V. Mancini, “Passflow: guessing
passwords with generative flows,” in 2022 52nd Annual IEEE/IFIP International
Conference on Dependable Systems and Networks (DSN) . IEEE, 2022, pp. 251–262.
[78] B. Pal, T. Daniel, R. Chatterjee, and T. Ristenpart, “Beyond credential stuffing:
Password similarity models using neural networks, ” in 2019 IEEE Symposium on
Security and Privacy (SP) . IEEE, 2019, pp. 417–434.
[79] G. Papamakarios, E. Nalisnick, D. J. Rezende, S. Mohamed, and B. Lakshmi-
narayanan, “Normalizing flows for probabilistic modeling and inference,” Jour-
nal of Machine Learning Research , vol. 22, no. 57, pp. 1–64, 2021.
[80] D. Pasquini, G. Ateniese, and M. Bernaschi, “Interpretable probabilistic password
strength meters via deep learning,” in Computer Security–ESORICS 2020: 25th
European Symposium on Research in Computer Security, ESORICS 2020, Guildford,
UK, September 14–18, 2020, Proceedings, Part I 25 . Springer, 2020, pp. 502–522.
[81] D. Pasquini, A. Gangwal, G. Ateniese, M. Bernaschi, and M. Conti, “Improving
password guessing via representation learning,” in 2021 IEEE Symposium on
Security and Privacy (SP) . IEEE, 2021, pp. 1382–1399.
[82] S. Pearman, J. Thomas, P. E. Naeini, H. Habib, L. Bauer, N. Christin, L. F. Cranor,
S. Egelman, and A. Forget, “Let’s go in for a closer look: Observing passwords
in their natural habitat,” in Proceedings of the 2017 ACM SIGSAC Conference on
Computer and Communications Security , 2017, pp. 295–310.
[83] A. Radford, “Unsupervised representation learning with deep convolutional
generative adversarial networks,” arXiv preprint arXiv:1511.06434 , 2015.
[84] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al. , “Improving language
understanding by generative pre-training,” 2018.
[85] J. Rando, F. Perez-Cruz, and B. Hitaj, “Passgpt: Password modeling and (guided)
generation with large language models,” in European Symposium on Research in
Computer Security . Springer, 2023, pp. 164–183.
[86] K. Revett, F. Gorunescu, M. Gorunescu, M. Ene, S. Magalhaes, and H. Santos, “A
machine learning approach to keystroke dynamics based user authentication,”
International Journal of Electronic Security and Digital Forensics , vol. 1, no. 1, pp.
55–70, 2007.
[87] RHC, “L’era delle password finisce con l’ai. l’uscita di passgan è un buon
incentivo a passare alla mfa,” Red Hot Cyber , 2023, accessed: Apr. 11, 2025.
[Online]. Available: https://www.redhotcyber.com/post/lera-delle-password-
finisce-con-lai-luscita-di-passgan-e-un-buon-incentivo-a-passare-alla-mfa/
[88] J. T. Ripper. [Online]. Available: https://www.openwall.com/john/
[89] R. RV, “How cracking passwords can be easier in the age of ai/ml,” okta, 2025,
accessed: Apr. 11, 2025. [Online]. Available: https://www.okta.com/blog/2025/
02/how-cracking-passwords-can-be-easier-in-the-age-of-aiml/
[90] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen, “Im-
proved techniques for training gans,” Advances in neural information processing
systems , vol. 29, 2016.
[91] J. Schofield, “32.6m passwords may have been compromised in rockyou hack,”
The Guardian , 2009, accessed: Apr. 14, 2025. [Online]. Available: https://www.
theguardian.com/technology/blog/2009/dec/15/rockyou-hacked-passwords
[92] R. Shay, S. Komanduri, A. L. Durity, P. Huh, M. L. Mazurek, S. M. Segreti, B. Ur,
L. Bauer, N. Christin, and L. F. Cranor, “Designing password policies for strength
and usability,” ACM Transactions on Information and System Security (TISSEC) ,
vol. 18, no. 4, pp. 1–34, 2016.
MAYA: Addressing Inconsistencies in Generative Password Guessing through a Unified Benchmark Conference’17, July 2017, Washington, DC, USA
[93] R. Shay, S. Komanduri, P. G. Kelley, P. G. Leon, M. L. Mazurek, L. Bauer,
N. Christin, and L. F. Cranor, “Encountering stronger password requirements:
user attitudes and behaviors,” in Proceedings of the sixth symposium on usable
privacy and security , 2010, pp. 1–20.
[94] E. Stobert and R. Biddle, “The password life cycle,” ACM Transactions on Privacy
and Security (TOPS) , vol. 21, no. 3, pp. 1–32, 2018.
[95] X. Su, X. Zhu, Y. Li, Y. Li, C. Chen, and P. Esteves-Veríssimo, “Pagpassgpt: Pattern
guided password guessing via generative pretrained transformer,” in 2024 54th
Annual IEEE/IFIP International Conference on Dependable Systems and Networks
(DSN) . IEEE, 2024, pp. 429–442.
[96] ——, “Pagpassgpt: Pattern guided password guessing via generative pretrained
transformer,” in 2024 54th Annual IEEE/IFIP International Conference on Depend-
able Systems and Networks (DSN) . IEEE, 2024, pp. 429–442.
[97] E. G. Tabak and C. V. Turner, “A family of nonparametric density estimation
algorithms,” Communications on Pure and Applied Mathematics , vol. 66, no. 2,
pp. 145–164, 2013.
[98] E. G. Tabak and E. Vanden-Eijnden, “Density estimation by dual ascent of the
log-likelihood,” 2010.
[99] T. Team, “What happened in the taobao data breach?” Twingate , 2024, accessed:
Apr. 14, 2025. [Online]. Available: https://www.twingate.com/blog/tips/taobao-
data-breach
[100] B. L. T. Thai and H. Tanaka, “A statistical markov-based password strength
meter,” Internet of Things , vol. 25, p. 101057, 2024.
[101] ——, “A study on markov-based password strength meters,” IEEE Access , 2024.
[102] I. Thomson, “Ai slurps, learns millions of passwords to work out which
ones you may use next.” The Register , 2017, accessed: Apr. 11, 2025. [Online].
Available: https://www.theregister.com/2017/09/20/researchers_train_ai_bots_
to_crack_passwords/
[103] A. Tsitsulin, M. Munkhoeva, D. Mottin, P. Karras, A. Bronstein, I. Oseledets, and
E. Müller, “The shape of data: Intrinsic distance for data distributions,” 2020.
[Online]. Available: https://arxiv.org/abs/1905.11141
[104] Unic0rn28, “hashcat-rules,” accessed: Apr. 11, 2025. [Online]. Available:
https://github.com/Unic0rn28/hashcat-rules
[105] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser,
and I. Polosukhin, “Attention is all you need,” Advances in neural information
processing systems , vol. 30, 2017.
[106] B. N. Vi, N. N. Tran, and T. G. V. The, “A gan-based approach for password guess-
ing,” in 2021 RIVF International Conference on Computing and Communication
Technologies (RIVF) . IEEE, 2021, pp. 1–5.
[107] J. Vijayan, “Passgan: Password cracking using machine learning,” Dark Reading ,
2017, accessed: Apr. 11, 2025. [Online]. Available: https://www.darkreading.com/
cybersecurity-analytics/passgan-password-cracking-using-machine-learning
[108] S. Wadhwani, “Ai cracker can guess over half of common passwords in
60 seconds,” Spiceworks , 2023, accessed: Apr. 11, 2025. [Online]. Avail-
able: https://www.spiceworks.com/tech/artificial-intelligence/news/passgan-
ai-password-cracking-time/
[109] D. Wang, H. Cheng, P. Wang, X. Huang, and G. Jian, “Zipf’s law in passwords,”
IEEE Transactions on Information Forensics and Security , vol. 12, no. 11, pp. 2776–
2791, 2017.
[110] D. Wang, H. Cheng, P. Wang, J. Yan, and X. Huang, “A security analysis of
honeywords,” 10 2017.
[111] D. Wang, D. He, H. Cheng, and P. Wang, “fuzzypsm: A new password strength
meter using fuzzy probabilistic context-free grammars,” in 2016 46th Annual
IEEE/IFIP International Conference on Dependable Systems and Networks (DSN) .
IEEE, 2016, pp. 595–606.
[112] D. Wang and P. Wang, “On the implications of zipf’s law in passwords,” in
European Symposium on Research in Computer Security . Springer, 2016, pp.
111–131.
[113] D. Wang, Z. Zhang, P. Wang, J. Yan, and X. Huang, “Targeted online password
guessing: An underestimated threat,” in Proceedings of the 2016 ACM SIGSAC
conference on computer and communications security , 2016, pp. 1242–1254.
[114] D. Wang, Y. Zou, Q. Dong, Y. Song, and X. Huang, “How to attack and generate
honeywords, ” in 2022 IEEE Symposium on Security and Privacy (SP) . IEEE, 2022,
pp. 966–983.
[115] D. Wang, Y. Zou, Y.-A. Xiao, S. Ma, and X. Chen, “ {Pass2Edit}: A{Multi-Step}
generative model for guessing edited passwords,” in 32nd USENIX Security
Symposium (USENIX Security 23) , 2023, pp. 983–1000.
[116] D. Wang, Y. Zou, Z. Zhang, and K. Xiu, “Password guessing using random forest, ”
USA, 2023.
[117] J. Wang, Y. Li, X. Chen, and Y. Zhou, “Modeling password guessability via
variational auto-encoder,” in 2021 IEEE 24th International Conference on Computer
Supported Cooperative Work in Design (CSCWD) . IEEE, 2021, pp. 348–353.
[118] M. Weir, S. Aggarwal, B. De Medeiros, and B. Glodek, “Password cracking using
probabilistic context-free grammars,” in 2009 30th IEEE symposium on security
and privacy . IEEE, 2009, pp. 391–405.
[119] D. L. Wheeler, “zxcvbn: {Low-Budget}password strength estimation,” in 25th
USENIX Security Symposium (USENIX Security 16) , 2016, pp. 157–173.[120] J. Xie, H. Cheng, R. Zhu, P. Wang, and K. Liang, “Wordmarkov: A new pass-
word probability model of semantics,” in ICASSP 2022-2022 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP) . IEEE, 2022, pp.
3034–3038.
[121] M. Xu, C. Wang, J. Yu, J. Zhang, K. Zhang, and W. Han, “Chunk-level password
guessing: Towards modeling refined password composition representations,” in
Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communica-
tions Security , 2021, pp. 5–20.
[122] M. Xu, J. Yu, X. Zhang, C. Wang, S. Zhang, H. Wu, and W. Han, “Improving
real-world password guessing attacks via bi-directional transformers,” in 32nd
USENIX Security Symposium (USENIX Security 23) , 2023, pp. 1001–1018.
[123] K. Yang, X. Hu, Q. Zhang, J. Wei, and W. Liu, “Vaepass: A lightweight passwords
guessing model based on variational auto-encoder,” Computers & Security , vol.
114, p. 102587, 03 2022.
[124] F. Yu and M. V. Martin, “Targeted honeyword generation with language models, ”
arXiv preprint arXiv:2208.06946 , 2022.
[125] F. Yu and M. Vargas Martin, “Honeygan: Creating indistinguishable honeywords
with improved generative adversarial networks,” in International Workshop on
Security and Trust Management . Springer, 2022, pp. 189–198.
[126] K. Zetter, “Hackers finally post stolen ashley madison data,” Wired , 2015,
accessed: Apr. 14, 2025. [Online]. Available: https://www.wired.com/2015/08/
happened-hackers-posted-stolen-ashley-madison-data/
[127] T. Zhou, H.-T. Wu, H. Lu, P. Xu, and Y.-M. Cheung, “Password guessing based
on gan with gumbel-softmax,” Security and Communication Networks , vol. 2022,
no. 1, p. 5670629, 2022.
Ethical Considerations
In line with prior research [ 71,81,85,122], we consider the use of
leaked datasets to be ethical, as: (1) they are publicly available, (2)
their usage does not cause additional harm, (3) we do not use any
additional sensitive information, such as email addresses, phone
numbers, or usernames, that could link specific passwords to indi-
vidual users, and (4) such data are essential for advancing research.
We strongly discourage any usage of MAYA for illegal or unethical
purposes. The framework is developed exclusively for academic
research to drive advancements in password security.
AAppendix: Background on Generative Models
In this appendix, we provide additional background details on gen-
erative models and their architectures.
A.1 Deep Generative Models
A deep generative model is a statistical model that employs a deep
neural network to learn complex data distributions. Given a data
space𝑋, defined over R𝑛, these models are trained to approximate
the underlying probability distribution 𝑝(𝑥), where𝑥∈𝑋. Rather
than directly operating in the high-dimensional data space, gen-
erative models learn a more compact representation using latent
variables, which capture intrinsic and complex features of the train-
ing data𝑋𝑡𝑟𝑎𝑖𝑛. The latent space is a k-dimensional space, 𝑍⊆Rk,
where each point corresponds to a latent representation of the
data, which can be seen as a set of hidden factors or features that
define the data in an abstract form. The ultimate goal is to gen-
erate new instances that resemble data from 𝑋. The generation
process is guided by the prior latent distribution, ¤𝑝(𝑧), imposed
over the latent variables during training. By sampling a point 𝑧
from the latent space according to ¤𝑝(𝑧)and subsequently mapping
it to the data space via the generative network, the model generates
a new sample 𝑥that approximates the true data distribution 𝑝(𝑥).
The prior latent distribution ¤𝑝(𝑧)is often chosen as a simple and
well-defined distribution, such as a standard Gaussian N(0,𝐼), to
facilitate sampling. A generative network can thus be viewed as a
Conference’17, July 2017, Washington, DC, USA William Corrias, Fabio De Gaspari, Dorjan Hitaj, and Luigi V. Mancini
Table 8: Distribution of password patterns from r1 to r19.
Dataset r1 r2 r3 r4 r5 r6 r7 r8 r9 r10 r11 r12 r13 r14 r15 r16 r17 r18 r19
rockyou 44.28 41.89 1.51 15.93 0.02 36.26 1.64 0.15 1.67 31.64 1.11 0.67 0.10 16.54 0.04 0.17 0.07 0.47 9.38
linkedin 20.38 18.02 0.78 19.59 0.01 53.22 1.28 0.16 5.36 43.10 1.97 0.66 0.25 21.23 0.07 0.35 0.31 0.82 10.02
mailru 27.19 24.37 0.27 18.54 0.00 52.19 0.62 0.35 1.12 27.00 0.14 0.37 0.16 21.23 0.03 0.04 0.06 0.03 5.75
000web 0.42 0.18 0.01 0.04 0.02 92.89 1.20 0.40 4.97 69.11 1.77 0.66 0.50 3.31 0.00 0.02 0.02 0.43 13.10
taobao 15.70 15.42 0.13 27.90 0.01 55.89 0.09 0.10 0.31 50.18 0.06 0.04 0.08 28.46 0.00 0.01 0.01 0.01 9.92
gmail 39.78 39.78 0.00 15.70 0.02 42.38 0.86 0.14 1.13 33.36 0.53 0.58 0.09 16.93 0.12 0.07 0.17 0.00 9.05
ashleym 35.04 33.18 0.98 12.32 0.00 52.24 0.12 0.02 0.27 41.79 0.11 1.05 0.01 13.43 0.00 0.01 0.01 0.05 10.11
libero 42.04 39.01 1.86 13.10 0.00 42.41 0.63 0.08 1.69 34.06 0.75 0.24 0.09 15.94 0.02 0.09 0.05 0.24 6.95
Table 9: Top 10 passwords for each dataset. The password at the 8th position in 000webhost (indicated as “*”) is “YfD-
bUfNjH10305070”: the letter portion of the password can be mapped to a Russian word meaning “Navigator”. The reasons for
its unexpected popularity remain unclear [112].
Dataset 1 2 3 4 5 6 7 8 9 10
Rockyou 123456 12345 123456789 password iloveyou princess rockyou 1234567 12345678 abc123
Linkedin linkedin 123456 123456789 abc123 idontknow ilovelinkedin Godisgood jaimatadi linkedin1 iloveindia
mailru qwerty qwertyuiop 123456 qwe123 qweqwe klaster 1qaz2wsx 1q2w3e4r qazwsx 1q2w3e
000web. abc123 123456a 12qw23we 123abc a123456 123qwe secret666 * asd123 qwerty123
Taobao 123456 111111 123456789 123123 000000 5201314 wangyut2 123 123321 12345678
Gmail 123456 password 123456789 12345 qwerty 12345678 111111 abc123 123123 1234567
Ashley M. eatpussy opensaysme christina longing nastygirl steve 11inches 2ofus 69sex 99wmp
Libero 123456 popopo90 francesco 123456789 12345678 napoli alessandro amoremio andrea francesca
deterministic mapping function between the latent space 𝑍and the
data space𝑋, formally defined as 𝐺:𝑍−→𝑋.
A.2 Generative Adversarial Networks
GANs (Generative Adversarial Networks) [ 36] are a type of im-
plicit generative model, meaning they generate new data without
explicitly modeling or learning a probability distribution 𝑝(𝑥). As a
result, they do not directly assign probabilities to generated samples.
GANs follow an adversarial training approach, where the training
process consists of a game between two deep neural networks: a
discriminator 𝐷and a generator 𝐺. During training, the genera-
tor takes latent points 𝑧sampled from¤𝑝(𝑧), and maps them into
the data space, producing synthetic samples. The discriminator,
which receives both generated samples and real data, is tasked with
distinguishing between them. Its objective is to assign a higher
probability to real data and a lower probability to generated data.
In contrast, the generator’s objective is to produce data that fools
the discriminator into treating the generated data as real. This ad-
versarial process drives both networks to improve iteratively: the
generator learns to create more realistic data, and the discriminator
becomes better at distinguishing real data from fake data.
A.3 Autoencoders
An autoencoder (AE) is a type of deep neural network consisting of
two parts: an encoder 𝐸𝑛𝑐:𝑋−→𝑍and a decoder 𝐷𝑒𝑐 :𝑍−→𝑋.
AEs were first introduced by Kramer et al. in [ 61]. The encoder takes
a data point 𝑥as input and maps it to a latent space representation
𝑧, while the decoder takes 𝑧and tries to reconstruct 𝑥. The goal
of an AE is to learn a compressed representation of the data andminimize the reconstruction error between the input x and the
reconstructed ˆ𝑥.
A.4 Variational Autoencoders
VAEs [ 59] introduce a probabilistic approach to autoencoders by
learning a distribution over the latent variables. Unlike the stan-
dard AE, the encoder in a VAE outputs parameters that define a
probability distribution, from which a latent vector is sampled. The
decoder then tries to reconstruct the input data. This allows the
VAE to model the underlying data distribution 𝑝(𝑥)by learning a
mapping from the latent space. Typically, the loss function of a VAE
consists of two main components: (1) a reconstruction loss, which
measures how well the decoder reconstructs the original inputs,
and (2) a KL Divergence term, which ensures that the learned latent
distribution is close to the prior distribution.
A.5 Flow-based generative models
Flow-based Generative Models are a class of explicit generative
models based on the concept of normalizing flows [ 26,27,79,97,98].
A flow𝑓𝜃:𝑋−→𝑍is a bijective function, where 𝑋⊆R𝐷and𝑍⊆
R𝐷, and𝜃represents the learned parameters of the model. Due to its
bijectivity, a flow function allows both the mapping of a latent space
point to a real data point and the reverse. Moreover, by modeling the
true data distributions, flow-based models can compute the exact
likelihood of the data. During training, normalizing flows transform
a simple prior distribution into a more complex one through a
sequence of inverse transformations, denoted as 𝑓−1(𝑧). In a flow
network𝑓𝜃, composed of 𝑘bijective functions 𝑓𝑖, the relationship
between a data point 𝑥∈𝑋and its latent representation 𝑧∈𝑍is
MAYA: Addressing Inconsistencies in Generative Password Guessing through a Unified Benchmark Conference’17, July 2017, Washington, DC, USA
Table 10: Original models dependencies.
Model Framework
PassGAN TensorFlow 1.13.1
PLR-GAN TensorFlow 1.14.0
PassFlow PyTorch 1.7.1
PassGPT PyTorch (N/A)
VGPT2 PyTorch 1.10.2 + PyTorchLight 1.5.10
FLA TensorFlow 1.4
given by:
𝑧=𝑓𝜃(𝑥)=(𝑓𝑘◦𝑓𝑘−1◦···◦𝑓1)(𝑥) (1)
Conversely, to recover 𝑥from its latent representation 𝑧, the inverse
function can be applied:
𝑥=𝑓−1
𝜃(𝑧)=(𝑓−1
1◦𝑓−1
2◦···◦𝑓−1
𝑘)(𝑧) (2)
A.6 Long Short-Term Memory
LSTMs are a type of Recurrent Neural Network designed to over-
come the vanishing gradient problem commonly encountered in
traditional RNNs. They are particularly effective at capturing long-
term dependencies in sequential data while selectively forgetting
irrelevant information. An LSTM uses two separate paths to make
predictions: a long-term memory path, which stores information
over extended periods to capture long-range dependencies, and a
short-term memory path, which processes more recent data com-
pared to the long path. An LSTM relies on three gates: (1) a forget
gate, which determines what information to discard from the previ-
ous state, (2) an input gate, which controls what new information
to store in the current state, and (3) an output gate, which decides
what information from the current state should be output.
A.7 Transformers
Transformers are a family of deep learning models based on the at-
tention mechanism, introduced by Vaswani et al. in “Attention Is All
You Need” [105]. They represent the state-of-the-art architecture for
NLP tasks. Transformers are considered the evolution of the tradi-
tional encoder-decoder architecture found in RNNs. In transformers,
a sequence of tokens 𝑥={𝑥1,𝑥2,...,𝑥𝑛}is first transformed into
numerical vectors through an embedding layer. These embeddings
are then augmented with positional encodings, which inject in-
formation about the relative or absolute positions of tokens in the
sequence. The resulting vectors are passed through a series of multi-
head self-attention layers followed by feed-forward layers, which
are applied in parallel to each token. This attention mechanism al-
lows the model to capture contextual relationships between tokens
in the sequence, regardless of their distance, making transformers
significantly faster and more effective than traditional RNN-based
models. The output of the encoder, denoted as 𝑧={𝑧1,𝑧2,...,𝑧𝑛},
is then passed to the decoder. Similar to the encoder, the decoder
consists of multi-head attention layers and feed-forward layers. In
particular, the decoder uses masked attention to ensure that each
token is generated in an autoregressive manner, i.e., the prediction
of the next token is dependent on the previously generated tokens.
The decoder produces the output sequence 𝑦={𝑦1,𝑦2,...,𝑦𝑚},
with each token 𝑦𝑚computed sequentially based on the contextfrom the encoder’s output 𝑧and the previously generated tokens
{𝑦1,𝑦2,...,𝑦𝑚−1}. The final output of the decoder is passed through
a linear layer followed by a softmax activation function, which
generates the probabilities for the next token in the sequence.
A.8 Generative Pre-trained Transformers
GPTs [ 84] are a family of Large Language Models (LLMs) based
on the autoregressive decoder of transformers. Unlike traditional
transformer models, which use both an encoder and a decoder,
GPTs utilize only the decoder part of the architecture. The decoder
is able to contextualize the prompt on its own, without the aid
of the encoder, by using exclusively the self-attention mechanism.
GPTs are trained on a vast corpus of text data and are designed
to predict the next token in a sequence given all previous tokens.
They have been proven to be highly effective for a wide range of
tasks, including language modeling and text generation.
BAppendix: Models and Tools Implementation
In this appendix, we provide further details on the implementation
of the password-guessing approaches studied in this paper.
The first challenge we faced was that the selected generative
approaches were developed at different times and relied on vary-
ing dependencies. As shown in Table 10, none of the six models
initially shared the same dependency set: some were implemented
in TensorFlow, others in PyTorch, often relying on outdated ver-
sions. To address this, we conducted an initial porting phase aimed
at standardizing dependencies. Specifically, all TensorFlow-based
models were ported to PyTorch version 2.6.0, and compatibility
adjustments were applied to PyTorch-based models to align them
with the new version.
B.1 FLA
Regarding FLA, we implemented the LSTM “large model” described
in [71], which consists of 1000 cells and employs backward training.
Unlike the other models, FLA assigns a probability to each character
based on the sequence generated up to that point and discards
passwords with a probability lower than a user-defined threshold.
We set the probability threshold to 10−8when generating up to 106
passwords, 10−9for a maximum of 107passwords, and 10−10to
generate up to 5×108passwords. Then we sorted passwords by
probability and selected the top n passwords, thereby effectively
choosing the n most probable ones. We adopted this approach as it
is conceptually equivalent to finding the optimal threshold, which
in turn is analogous to generate exactly n passwords.
B.2 PassGAN
We implemented PassGAN [ 48] exactly as described by the authors.
PassGAN is built upon the Improved Training of Wasserstein GANs
(IWGAN) by Gulrajani et al. [ 38], a specific type of GAN designed
for text generation. Both the generator and the discriminator consist
of 5 residual blocks, which are the core component of Residual
Networks [ 44]. PassGAN’s models are trained for up to 200,000
iterations, as this is when the model reaches its maximum potential,
with checkpoints evaluated every 10,000 iterations.
Conference’17, July 2017, Washington, DC, USA William Corrias, Fabio De Gaspari, Dorjan Hitaj, and Luigi V. Mancini
B.3 PLR-GAN
PLR-GAN [ 81] is an improved version of PassGAN, designed to
address several limitations of the original model. One of the major
challenges in PassGAN was an inherent instability in the training
process, which restricted both the generator and the critic from
performing only a small number of training iterations. This issue
derives from the discrete representation of passwords in the train-
ing dataset. Specifically, the generator struggled to replicate this
discrete format due to the continuous nature of the final softmax
activation function, which made it difficult for the model to pro-
duce realistic passwords and, consequently, harder for the critic to
distinguish real from generated data. To tackle this, PLR applies a
small magnitude of noise over the one-hot encoding representation
of each character. This operation allowed for 30 times more training
iterations and enabled the use of deeper architectures, replacing
plain residual blocks with deeper residual bottleneck blocks [ 44].
Despite the improvements introduced by PLR-GAN, we limited the
training iterations to 400k in our experiments. Extending training
to 4 million iterations would have been too time-consuming, and we
observed minimal improvements beyond the 400k iteration mark.
Pasquini et al. also introduced a guessing strategy called Dynamic
Password Guessing (DPG). The idea behind DPG is that initially,
the model has no prior knowledge of the target dataset. However,
each guessed password provides valuable feedback that can be used
to refine and adjust the latent distribution 𝑝(𝑧). This allows the
model to dynamically adapt to the testing dataset by altering the
latent distribution over time, thereby increasing the likelihood of
generating passwords from areas covered by the target distribution.
We implemented this DPG strategy within PLR-GAN and included
it in our experiments to assess its effectiveness.
B.4 PassFlow
We made several improvements to the original PassFlow imple-
mentation [ 77]. Due to time constraints, we introduced an early-
stopping mechanism that stops the training if the model fails to
show at least a 5% improvement over the best result after 10 con-
secutive iterations, starting from epoch 100. Otherwise, training
continues until epoch 200. In addition to Dynamic Password Guess-
ing, PassFlow introduced a guessing technique called Gaussian
Smoothing (GS), which enhances DPG. In GS, random perturba-
tions sampled from a Gaussian distribution are applied to generated
passwords to avoid collisions, allowing the model to explore di-
verse regions of the latent space. Since GS has proven to be more
effective than other guessing techniques, we opted to use PassFlow
with GS. We also optimized the password generation algorithm:
in the original PassFlow, the GS technique continues generating
passwords until a unique one is found, adding 0.05 of noise every
20 attempts. However, we observed that this approach can become
inefficient on certain datasets, significantly slowing the training. To
address this, we modified the algorithm: after four attempts to add
random perturbations, if PassFlow still fails to generate a unique
password, it stops further attempts.B.5 VGPT2
VGPT2 was implemented exactly as described by Biesner et al. [ 10],
with no modifications to the architecture or training process, except
where necessary to standardize dependencies.
B.6 PassGPT
We used the original PassGPT code as provided, making modifi-
cations only when necessary to incorporate additional features or
adjust specific functions. Following the authors’ recommendations
[85], we trained PassGPT on a dataset composed exclusively of
unique passwords.
B.7 Hashcat
Regarding Hashcat, we use the Unicorn Rules [ 104], a suite of
rules sorted by efficiency, which allows us to select the top-n rules
proportionally to the dataset size.
B.8 John The Ripper
In our experiments with John the Ripper, we use the ’Wordlist’ set
of rules, which is included in the latest version of the tool.
C Appendix: Additional Details On RQs
In this appendix, we provide additional details regarding some of
the research questions.
C.1 RQ6
To address RQ6, we employed two metrics: the Jaccard Index and the
Mergeability Index. We now present their mathematical definitions:
Jaccard Index: Given two models 𝑚1and𝑚2and a set of datasets
D ={𝐷1,𝐷2,...,𝐷𝑧}, let𝑓(𝑚𝑖,𝐷𝑡𝑟𝑎𝑖𝑛
𝑗,𝑠)=𝑃𝑖,𝑗denote the set of pass-
words generated by model 𝑚𝑖after being trained on dataset 𝐷𝑡𝑟𝑎𝑖𝑛
𝑗,
using settings 𝑠. We define the Jaccard Index between 𝑚1and𝑚2
as:
𝐽(𝑚1,𝑚2,𝐷)=1
|𝐷||𝐷|∑︁
𝑗=1|(𝑃1,𝑗∩𝑃2,𝑗)|
|(𝑃1,𝑗∪𝑃2,𝑗)|where𝑃𝑖,𝑗=𝑓(𝑚𝑖,𝐷𝑡𝑟𝑎𝑖𝑛
𝑗,𝑠)
(3)
Mergeability Index: Let𝐺𝑖,𝑗=𝑃𝑖,𝑗∩𝐷𝑡𝑒𝑠𝑡
𝑗denote the set of
passwords generated by model 𝑚𝑖, after being trained on 𝐷𝑡𝑟𝑎𝑖𝑛
𝑗,
that match those in the corresponding testing set 𝐷𝑡𝑒𝑠𝑡
𝑗. The Merge-
ability Index is defined as follows:
𝑀𝐼(𝑚1,𝑚2,𝐷)=1
|𝐷||𝐷|∑︁
𝑗=1 
|(𝐺1,𝑗∪𝐺2,𝑗)|−𝐺MAX
𝐺MAX!
(4)
where:𝐺MAX=max(|𝐺1,𝑗|,|𝐺2,𝑗|).
C.2 RQ7
Let𝐷𝑡𝑟𝑎𝑖𝑛=Ð|𝐷|
𝑗=1𝐷𝑡𝑟𝑎𝑖𝑛
𝑗and𝐷𝑡𝑒𝑠𝑡=Ð|𝐷|
𝑗=1𝐷𝑡𝑒𝑠𝑡
𝑗denote the union
of all training and testing datasets, respectively. Let 𝑅be the set of
randomly generated passwords, each between 6 and 12 characters
in length. The lower-bound baseline 𝐿and upper-bound baseline
𝑈for a given metric 𝑑𝑖𝑠𝑡are computed as follows:
MAYA: Addressing Inconsistencies in Generative Password Guessing through a Unified Benchmark Conference’17, July 2017, Washington, DC, USA
Table 11: Marginal gain in successful matches as the number of generated passwords increases, with each column representing
generation intervals (e.g., 1M-2.5M indicates the marginal gain when expanding from 1M to 2.5M guesses). For a given interval
X-Y, Total indicates the marginal gain achieved when increasing the number of guesses from X to Y, expressed as a percentage
of the total number of test passwords. Relative indicates the marginal gain relative to the number of matches obtained at X.
Results are averaged across all password lengths.
.
1M-2.5M 2.5M-5M 5M-7.5M 7.5M-10M 10M-25M 25M-50M 50M-75M 75M-100M 100M-250M 250M-500M
Total Relative Total Relative Total Relative Total Relative Total Relative Total Relative Total Relative Total Relative Total Relative Total Relative
PassGAN +0.49% +83.75% +0.55% +50.94% +0.40% +24.84% +0.32% +15.95% +1.23% +53.74% +1.14% +32.85% +0.75% +16.47% +0.56% +10.69% +1.96% +34.15% +1.63% +21.38%
PLR-GAN +0.62% +97.50% +0.74% +59.45% +0.57% +28.94% +0.48% +18.91% +1.94% +65.41% +1.96% +40.34% +1.34% +19.54% +1.03% +12.55% +3.67% +40.28% +3.21% +25.59%
PassFlow +0.62% +69.67% +0.73% +48.88% +0.56% +25.15% +0.44% +15.72% +1.72% +52.72% +1.58% +31.53% +1.05% +15.85% +0.80% +10.38% +2.84% +33.66% +2.48% +21.97%
PassGPT +1.20% +114.72% +1.60% +71.91% +1.34% +35.32% +1.18% +22.97% +5.21% +83.58% +5.45% +48.49% +3.66% +22.15% +2.73% +13.56% +8.89% +39.13% +6.53% +20.63%
VGPT2 +0.18% +137.88% +0.28% +90.08% +0.25% +43.08% +0.24% +28.34% +1.18% +110.51% +1.45% +65.38% +1.11% +30.52% +0.92% +19.44% +3.65% +65.02% +3.43% +37.03%
FLA +4.33% +58.54% +4.17% +35.55% +2.66% +16.71% +1.93% +10.40% +6.57% +32.14% +5.55% +20.62% +3.33% +10.28% +2.34% +6.56% +7.44% +19.62% +5.84% +13.00%
(a) RockYou
 (b) Gmail
 (c) Libero
Figure 10: t-SNE plot showing the projection of Top 10% and Bottom 90% passwords from three test datasets in PassFlow’s latent
space. The embedding is done using PassFlow’s encoder.
𝐿=𝑑𝑖𝑠𝑡(𝐷test,𝐷train), 𝑈 =𝑑𝑖𝑠𝑡(𝐷test,𝑅) (5)
Next, for each model 𝑚𝑖, let𝑃𝑖be the set of passwords generated
by model𝑚𝑖across all datasets. The value 𝑑𝑖for a model 𝑚𝑖on a
given metric 𝑑𝑖𝑠𝑡is then computed using the following equation:
𝑑𝑖=𝑑𝑖𝑠𝑡(𝐷𝑡𝑒𝑠𝑡,𝑃𝑖)−𝐿
𝑈−𝐿∗100 (6)
We now provide further details regarding the four selected met-
rics, followed by an additional analysis on IMD.
CNN Divergence [ 39].Neural networks can be used to estimate
the divergence between two distributions, making them useful for
evaluating generative models. The idea is to employ an independent
critic network trained to distinguish between real and generated
samples. After sufficient training, the critic’s loss, based on WGAN-
GP [ 38], reflects the distance between the two distributions. Our
convolutional neural network (CNN) follows the architecture and
settings described in [ 39], based on the DCGAN discriminator [ 83].
IMD [ 103].IMD, which stands for Intrinsic Multi-scale Distance,
is a metric designed to compare the data manifolds of two distribu-
tions. IMD provides an intrinsic method to lower-bound the spectral
Gromov-Wasserstein distance between two manifolds. Unlike other
approaches that focus on extrinsic properties and are uni-scale,
IMD is intrinsic, meaning it is not dependent on the transformation
of the manifold and multi-scale, capturing both local and global
properties.𝛼-Precision 𝛽-Recall Authenticity [ 5].The𝛼-Precision𝛽-
Recall Authenticity is a three-dimensional metric, with each di-
mension corresponding to a distinct property:
•Fidelity (𝛼-Precision): Measures how closely the generated
samples resemble the most typical fraction ( 𝛼-support) of
real data.
•Diversity (𝛽-Recall): Represents the fraction of real samples
that lie within the 𝛽-support of the generated data distribu-
tion.
•Generalization (Authenticity): Reflects the model’s ability
to generalize, ensuring that the output is not limited to mere
copies of the training data.
Each dimension is computed after mapping generated and real data
into a hypersphere using a feature embedding, where most of the
data is concentrated near the center, while outliers are positioned
closer to the boundaries.
MTopDiv [ 8]:MTopDiv introduced a method that tracks multi-
scale topology discrepancies between two distributions, P and Q,
in a high-dimensional space. Unlike IMD, MTopDiv also consid-
ers extrinsic properties, using position and translation to capture
structural differences between the two distributions.
IMD Analysis .To better understand the rationale behind the
high values obtained with IMD across several models, we repeat
the metric analysis using 107generated passwords. The results,
shown in Table 13, differ significantly from those obtained with
5×108generated passwords (Table 7), particularly for PassFlow,
Conference’17, July 2017, Washington, DC, USA William Corrias, Fabio De Gaspari, Dorjan Hitaj, and Luigi V. Mancini
r1r2r3r4r5r6r7r8r9r10r11r12r13r14r15r16r17r18r19
Patterns020406080100Frequencies (%)
FLA
PassGPT
PassFlow
000webhost
Fully-Random-Psw
Figure 11: Distribution of 19 patterns in the 000webhost test
dataset and in passwords generated by FLA, PassGPT, and
PassFlow, all trained on 000webhost.
Table 13: Distance between human- and generative model-
created passwords when generating 107passwords.
Models CNN Div 𝛼−Precision 𝛽−Recall Auth IMD MTopDiv
PassGAN
 16%
 38%
 7%
 13%
 62%
 1%
PLR-GAN
 9%
 -13%
 5%
 9%
 18%
 0%
PassFlow
 69%
 31%
 59%
 35%
 500%
 52%
PassGPT
 6%
 -9%
 4%
 4%
 0%
 0%
VGPT2
 33%
 26%
 36%
 6%
 120%
 15%
FLA
 24%
 39%
 3%
 48%
 347%
 2%
Table 12: IMD outputs as the distributions 𝑃and𝑄vary.
IMD(P,Q) Output
P = 000webhost - Q = Random 7.9919
P = 000webhost - Q = PassFlow 000webhost 47.0466
P = PassFlow 000webhost - Q = Random 49.0287
P = PassFlow 000w. - Q = Rand with PassFlow 000w. Length 30.9260
PLR-GAN, and FLA, which exhibit notably higher IMD scores in
the smaller sample. To investigate this discrepancy, we compare
the length distributions of the 107and5×108generated samples.
We observe that, in the smaller set, models such as PassFlow and
PLR-GAN—based on GS and DPG techniques, respectively—exhibit
a strong bias toward shorter passwords. This behavior stems from
the tendency of such models to initially saturate the space of sim-
pler (i.e., shorter) passwords before extending to more complexones. A similar pattern is observed with FLA, which outputs the
most probable passwords. Consequently, when fewer samples are
generated, the length distribution exhibits a strong bias toward
shorter lengths, as shorter passwords are more probable. These
findings confirm our hypothesis that IMD is highly sensitive to
length distribution differences: the greater the deviation of the gen-
erated passwords’ length distribution from the real one, the higher
the distance measured by IMD.
We now focus on the 000webhost dataset, which yields a partic-
ularly high IMD score for PassFlow when evaluating 107generated
passwords. Although this score exceeds that obtained from random
passwords, it does not necessarily indicate that PassFlow’s outputs
are random or even close to random. To demonstrate this, we com-
pute the IMD metric between PassFlow’s generated passwords and
random passwords. The results, shown in Table 12, are compared
against the two baselines and reveal that PassFlow’s passwords
are farther from random passwords than from real ones. We fur-
ther examined the effect of aligning the length distribution of the
random passwords with that of PassFlow’s 000webhost-generated
passwords. As reported in Table 12, this adjustment results in a
higher IMD score, thereby confirming that IMD is sensitive to dif-
ferences in length distribution.
Lastly, we investigate the impact of various degrees of mode
failure, such as mode dropping and mode invention, on the IMD
metric. For this analysis, we used the 19 patterns listed in Table 3.
As shown in Figure 11, the pattern distribution of random pass-
words deviates substantially from the others. Mode invention is
observed in patterns r7, r9, r11, r13, r16, and r17, while mode drop-
ping occurs in commonly observed patterns such as r6, r10, and r19.
These findings suggest that the IMD metric does not adequately
capture the varying degrees of mode failure and further support
the hypothesis that IMD primarily outputs high values due to a
model’s inability to replicate the real password length distribution.
Interestingly, PassGPT performs particularly well in approximating
the pattern distribution of 000webhost, with its pattern distribution
closely aligning with that of the real passwords. Additionally, FLA
outperforms PassFlow in capturing this distribution, with the latter
struggling to replicate patterns r10, r14, and r19. These observa-
tions are consistent with the results obtained using the MTopDiv
metric in Table 7, which explicitly accounts for such mode-related
discrepancies.