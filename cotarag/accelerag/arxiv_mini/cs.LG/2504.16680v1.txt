Offline Robotic World Model: Learning Robotic
Policies without a Physics Simulator
Chenhao Li
ETH AI Center
ETH Zurich, Switzerland
chenhli@ethz.chAndreas Krause
Department of Computer Science
ETH Zurich, Switzerland
krausea@ethz.ch
Marco Hutter
Department of Mechanical Engineering
ETH Zurich, Switzerland
mahutter@ethz.ch
Abstract: Reinforcement Learning (RL) has demonstrated impressive capabilities
in robotic control but remains challenging due to high sample complexity, safety
concerns, and the sim-to-real gap. While offline RL eliminates the need for risky
real-world exploration by learning from pre-collected data, it suffers from distri-
bution shift, limiting policy generalization. Model-Based RL (MBRL) addresses
this by leveraging predictive models for synthetic rollouts, yet existing approaches
often lack robust uncertainty estimation, leading to compounding errors in offline
settings. We introduce Offline Robotic World Model (RWM-O), a model-based
approach that explicitly estimates epistemic uncertainty to improve policy learning
without reliance on a physics simulator. By integrating these uncertainty estimates
into policy optimization, our approach penalizes unreliable transitions, reducing
overfitting to model errors and enhancing stability. Experimental results show that
RWM-O improves generalization and safety, enabling policy learning purely from
offline data and advancing scalable, data-efficient RL for robotics.
Keywords: Offline Reinforcement Learning, Model-Based Reinforcement Learn-
ing, Uncertainty Estimation
𝑜𝑡−⋯𝑜𝑡−1𝑜𝑡𝑜′𝑡+1𝑜′𝑡+2𝑜′𝑡+𝑇
𝑜′𝑡𝑜′𝑡+1𝑜′𝑡+𝑇−1MOPO -PPO
ǁ𝑟=𝑟−𝜆𝑢uncertainty -aware 
imagination𝑜𝑡−𝑀+1𝑜𝑡
𝑜′𝑡+1𝑜′𝑡+2𝑜′𝑡+𝑁RWM -O
offline real -
world data
𝜋
policy learned 
without a simulator
Figure 1: Overview of RWM-O and MOPO-PPO. RWM-O is trained on offline data to predict
long-horizon dynamics with an ensemble-based uncertainty estimate. The shade visualizes epistemic
uncertainty, which is penalized during policy optimization to avoid overfitting to unreliable predictions.
Actions are omitted for clarity. Our framework enables direct policy learning without simulators,
bridging the gap between model-based offline RL and real-world robotic deployment.
1 Introduction
Reinforcement Learning (RL) has demonstrated significant success in robotic control, enabling
policies that outperform traditional control techniques in complex tasks [ 1,2,3,4]. However,
applying RL directly to real-world robotics remains challenging due to the high sample complexity
and the risks associated with exploratory interactions [ 5,6]. A common alternative is training policiesarXiv:2504.16680v1  [cs.RO]  23 Apr 2025
in high-fidelity physics simulators, which offer a safe and efficient environment for data collection
and optimization [ 7,8,9]. However, simulators struggle to capture real-world dynamics with perfect
accuracy, leading to the well-known sim-to-real gap, which degrades policy performance when
transferred to physical hardware [ 10,11]. To address these challenges, offline RL has emerged
as a promising paradigm by enabling policy learning entirely from pre-collected real-world data,
eliminating the need for risky online exploration [ 12,13,14,15]. Despite its promise, offline RL is
prone to distribution shift, where the learned policy encounters states that are insufficiently represented
in the dataset, leading to extrapolation errors and poor generalization [ 13,14,12]. Model-free offline
RL methods mitigate this by enforcing strict constraints to keep the policy within the dataset’s support,
but at the cost of limiting generalization beyond observed transitions [16, 17, 18].
Model-Based Reinforcement Learning (MBRL) provides an attractive alternative by learning a
predictive model of environment dynamics, allowing the generation of synthetic experience for policy
optimization [ 19,20]. This is particularly advantageous in offline RL for three reasons [ 15]. First,
model-based methods benefit from denser supervision, as the dynamics model is trained on all dataset
transitions, even when rewards are sparse [ 21]. Second, training the model via supervised learning
provides more stable gradients compared to bootstrapped value estimation. Finally, well-established
uncertainty estimation techniques from supervised learning can be leveraged to mitigate distribution
shift [ 22,23,24,25]. Recent work on the Robotic World Model (RWM) has demonstrated the
potential of neural network-based dynamics models for policy learning [ 26]. RWM effectively
captures long-horizon predictive dynamics without domain-specific inductive biases, enabling robust
policy optimization through Model-Based Policy Optimization with Proximal Policy Optimization
(MBPO-PPO) [ 27]. However, existing formulations lack explicit uncertainty estimation, making them
susceptible to distribution shift in offline settings. Without uncertainty awareness, RWM requires
online corrections to mitigate errors introduced by out-of-distribution states [ 12,28,20], which limits
its applicability in fully offline RL [29, 30].
In this work, we extend RWM by incorporating uncertainty-aware dynamics modeling to enable
robust policy learning in offline RL. We propose an ensemble-based architecture that explicitly
estimates model uncertainty, allowing the policy to incorporate uncertainty-aware rollouts. To
leverage this information, we adapt Model-Based Offline Policy Optimization (MOPO) [ 15] to PPO,
enabling safer policy learning by penalizing high-uncertainty state transitions without requiring
additional real-world interactions. Our approach enables policy learning purely from offline robotic
data, eliminating the need for a physics simulator or online interactions. To our knowledge, our
locomotion experiments mark the first demonstration of a locomotion policy learned entirely from
offline data in a model-based fashion, with successful deployment on hardware. By enhancing the
reliability and adaptability of MBRL, our approach contributes toward scalable, data-efficient policy
learning, facilitating broader adoption of offline RL in real-world robotics. Supplementary videos for
this work are available at https://sites.google.com/view/corl2025-rwm-o/home .
2 Related work
2.1 Model-Based Reinforcement Learning
Model-Based Reinforcement Learning (MBRL) constructs an explicit predictive model of envi-
ronment dynamics, improving sample efficiency and facilitating long-horizon planning. Unlike
model-free RL, which directly learns policies from data, MBRL generates synthetic rollouts to opti-
mize policies, reducing reliance on real-world interactions. Early approaches such as PILCO leverage
Gaussian processes to model system dynamics, achieving data-efficient learning but facing scalability
issues in high-dimensional spaces [ 12]. Local linear models, as seen in guided policy search [ 28],
improve tractability in complex control tasks [ 31]. Neural network-based dynamics models have
since become the dominant approach, allowing for greater expressivity and scalability [ 21,32]. One
notable example is Model-Based Policy Optimization (MBPO), which interleaves model-generated
rollouts with model-free updates, selectively trusting the learned model to improve stability [27].
2
Recent advancements in latent-space world models have further improved MBRL. PlaNet introduces
a compact latent dynamics model for planning directly in an abstract space, making it computationally
feasible for long-horizon tasks [ 19]. Dreamer extends this idea by integrating an actor-critic frame-
work, achieving strong performance in continuous control [ 20,33,34]. A major challenge in MBRL
is model fidelity and its impact on policy learning. Robotic World Model (RWM) addresses this by
introducing a self-supervised, autoregressive neural simulator that learns long-horizon environment
dynamics without domain-specific inductive biases [ 26]. Unlike previous methods, RWM enables
stable policy optimization via MBPO-PPO, demonstrating successful deployment in real-world
robotic systems.
2.2 Uncertainty Estimation and Distribution Shift Mitigation
Incorporating uncertainty estimation into MBRL is crucial, especially in offline settings where the
lack of online interactions limits the ability to correct model inaccuracies. One of the most effective
and widely used techniques for model uncertainty estimation is the use of bootstrap ensembles,
which have been successfully applied in various MBRL frameworks to mitigate overestimation
caused by distribution shifts in offline RL [ 23,13,15]. Probabilistic Ensembles with Trajectory
Sampling (PETS) pioneers the use of probabilistic ensembles in MBRL, improving robustness by
leveraging multiple network predictions for decision-making [ 13]. To further improve uncertainty
estimation and its integration into policy learning, several advanced techniques have been developed.
MOUP introduces ensemble dropout networks for uncertainty estimation while incorporating a
maximum mean discrepancy constraint into policy optimization, ensuring bounded state mismatch
and improving policy learning in offline settings [ 35]. SUMO characterizes uncertainty by measuring
the cross-entropy between model dynamics and true dynamics using a k-nearest neighbor search
method [ 36]. This technique provides an alternative perspective on estimating uncertainty beyond
conventional model ensembles.
Uncertainty-aware policy optimization is key to mitigating the risks of distribution shift. MORPO
explicitly quantifies the uncertainty of predicted dynamics and incorporates a reward penalty to
balance optimism and conservatism in policy learning [ 37]. Similarly, COMBO tackles uncertainty
estimation challenges by enforcing conservatism in value estimation without relying on explicit
uncertainty quantification [ 38]. Our work extends these ideas by integrating uncertainty estimation
into RWM and adapting MOPO to PPO, enhancing stability and robustness in offline robotic control.
3 Preliminaries
3.1 Offline Model-Based Reinforcement Learning
We formulate the problem by modeling the environment as a Partially Observable Markov Decision
Process (POMDP) [ 39], defined by the tuple (S,A,O, T, R, O, γ ), where S,A, andOdenote the
state, action, and observation spaces, respectively. The transition kernel T:S × A → S captures the
environment dynamics, while the reward function R:S × A × S → Rmaps transitions to scalar
rewards. The agent seeks to learn a policy πθ:O → A that maximizes the expected discounted
return EπθhP
t≥0γtrti
, where rtis the reward at time tandγ∈[0,1]is the discount factor.
In offline RL, the agent is trained using a fixed dataset of transitions collected by one or a mixture
of behavior policies. Unlike online RL, the agent cannot interact with the environment, making it
susceptible to extrapolation errors when encountering out-of-distribution states [ 29,30]. Many offline
RL methods impose conservatism by constraining the policy to remain within the dataset’s support,
limiting generalization beyond observed trajectories [ 17,18]. MBRL provides an alternative approach
by learning a world model of the environment, which can be leveraged for policy optimization in
imagination [40]. Instead of relying solely on offline data for policy learning, MBRL methods
construct a predictive model of the transition dynamics, allowing the agent to generate synthetic
rollouts and expand the effective training data. This enables more efficient learning and better
generalization, which is particularly important in offline settings [ 15]. However, model errors
3
can accumulate over long-horizon predictions, introducing bias and degrading policy performance.
Addressing these challenges requires accurate long-horizon modeling and robust handling of model
errors.
3.2 Robotic World Model
Robotic World Model (RWM) is a neural network-based world model designed to accurately capture
long-horizon robotic dynamics without requiring domain-specific inductive biases [ 26]. Unlike
conventional methods that rely on structured priors or handcrafted features, RWM generalizes across
diverse robotic systems using a self-supervised, dual-autoregressive training framework. RWM learns
to predict future observations autoregressively, feeding its own predictions back into the model to
simulate long-horizon rollouts. The training objective minimizes a multi-step prediction loss
L=1
NNX
k=1αk
Lo 
o′
t+k, ot+k
+Lc 
c′
t+k, ct+k
, (1)
where Ndenotes the autoregressive prediction steps, o′
t+kandc′
t+kare the predicted observations
and privileged information (e.g., contact information), LoandLcmeasure the prediction discrepancy,
andαkis a decay factor. The predicted observation ksteps ahead of time tcan be written as
o′
t+k∼pϕ 
· |ot−M+k:t, o′
t+1:t+k−1, at−M+k:t+k−1
, (2)
where Mdenotes the history horizon steps. Through this design, RWM achieves robust trajectory
forecasting across manipulation and locomotion tasks. Policies trained with RWM have been zero-
shot deployed on hardware, achieving high-fidelity control without requiring additional fine-tuning.
Model-Based Policy Optimization (MBPO) trains policies from learned world models by interleaving
real environment data with simulated imagination rollouts [ 27]. RWM extends MBPO to PPO with
extensive on-policy updates, resulting in MBPO-PPO, a model-based RL framework tailored for
long-horizon robotic control [ 26]. MBPO-PPO follows the standard MBPO approach but benefits
from high-fidelity RWM rollouts, which significantly improve stability and accuracy during policy
learning.
4 Approach
4.1 Uncertainty Quantification with Offline Robotic World Model
In MBRL, the learned dynamics model inevitably becomes inaccurate when making predictions for
state-action pairs that deviate from the distribution of the training data. This issue is particularly
critical in the offline setting, where errors in the learned dynamics cannot be corrected through
additional environment interactions. As a result, standard model-based policy optimization methods
risk overfitting to erroneous model predictions in out-of-distribution regions, leading to suboptimal
or unsafe policy behavior. To ensure reliable performance, it is essential to balance the trade-off
between policy improvement and model reliability: leveraging model-based imagination to explore
high-return policies beyond the support of the dataset while mitigating the risk of policy degradation
due to overfitting to model errors in regions with high uncertainty.
To this end, we introduce Offline Robotic World Model (RWM-O), where we explicitly incorporate
uncertainty quantification into the dynamics model based on RWM. We aim to design an uncertainty
estimator that captures both epistemic uncertainty, which arises due to limited training data, and
aleatoric uncertainty, which reflects inherent stochasticity in the environment. Bootstrap ensembles
have been shown to provide consistent estimates of uncertainty [ 41] and have been successfully
applied in MBRL to improve robustness [ 13]. RWM-O extends RWM by introducing an ensemble-
based uncertainty-aware architecture. Specifically, we apply bootstrap ensembles to the observation
prediction head of the model after a shared recurrent feature extractor (e.g., a GRU). Each ensemble
element bindependently predicts the mean and variance of a Gaussian distribution over the next
observation
o′b
t+k∼ N
µb
ot+k, σ2,b
ot+k
, (3)
4
where µb
ot+k, σ2,b
ot+kdenote the mean and variance predicted by ensemble member b, following the
autoregressive prediction scheme in Eq. 2. The learned variance component captures aleatoric
uncertainty effectively. The training objective is then computed as the average loss over all ensemble
members, following Eq. 1.
During inference, the ensemble mean is used as the predicted next observation, while the variance
across ensemble members provides an estimate of epistemic uncertainty upϕ, depending on the world
model pϕ. The training overview is visualized in Fig. 1.
o′
t+1=Ebh
µb
ot+1i
, ut+1=upϕ(ot−M+1:t, at−M+k:t) = Var bh
µb
ot+1i
. (4)
By incorporating uncertainty-aware modeling into the dynamics learning process, RWM-O enables
robust trajectory forecasting in offline settings. The explicit quantification of epistemic uncertainty
allows for uncertainty-informed policy optimization, preventing the policy from over-relying on
uncertain model predictions.
4.2 Policy Optimization on Uncertainty-Penalized Imagination
To ensure robust policy learning in offline MBRL, we introduce Model-Based Offline Policy Optimiza-
tion with Proximal Policy Optimization (MOPO-PPO), an adaptation of the MOPO framework [ 15]
to PPO [ 42]. MOPO-PPO allows cautious exploration beyond the behavioral distribution while
penalizing transitions with high epistemic uncertainty. Specifically, given the uncertainty estimator
upϕfrom Eq. 4, we modify the reward function to discourage the policy from exploiting unreliable
model predictions:
˜r(ot, at) =rt(ot, at)−λupϕ(ot−M+1:t, at−M+k:t), (5)
where λis a hyperparameter controlling the penalty strength. This formulation encourages the policy
to focus on high-confidence regions of the learned dynamics model while avoiding overfitting to
model errors.
The training process of MOPO-PPO using RWM-O is outlined in Fig. 1 and Algorithm 1.
Algorithm 1 Offline policy optimization with RWM-O
1:Input: Offline dataset D
2:Initialize policy πθ, world model pϕ
3:Train the world model pϕautoregressively using offline dataset Daccording to Eq. 1
4:forlearning iterations = 1,2, . . . do
5: Initialize imagination agents with observations sampled from D
6: Roll out uncertainty-aware trajectories using πθandpϕforTsteps according to Eq. 4
7: Compute uncertainty-penalized rewards for each imagination transition according to Eq. 5
8: Update πθusing PPO or another RL algorithm
9:end for
By integrating uncertainty-aware dynamics modeling with policy optimization, RWM-O and MOPO-
PPO enable safer and more reliable offline MBRL. Explicit uncertainty estimation mitigates distri-
bution shift, allowing cautious generalization beyond the dataset while ensuring stability through
trust-region optimization. By leveraging uncertainty-aware rollouts, MOPO-PPO enhances policy
robustness and efficiency, advancing scalable offline RL for real-world robotics.
5 Experiments
We evaluate RWM-O and MOPO-PPO through a comprehensive set of experiments spanning diverse
robotic platforms, tasks, and dataset sources. Our objective is to assess the capability of RWM-O
in quantifying uncertainty during long-horizon rollouts and to demonstrate the effectiveness of
MOPO-PPO in improving policy performance through uncertainty-aware optimization.
5
1
0v
[m/s]
0.5
0.0
0.5
[rad/s]
0
25
50
75
100
125
150
175
200
32
t
0.5
0.0
0.5q
[rad]
0
25
50
75
100
125
150
175
32
t
0.0
0.2
0.4
0.6
0.8
1.0
1.2u
prediction error
epistemic uncertainty
aleatoric uncertaintyFigure 2: Autoregressive trajectory prediction (left) and uncertainty estimation (right) by RWM-O.
Solid lines represent ground truth trajectories, while dashed lines denote predicted state evolution.
Predictions commence at t= 32 using historical observations, with future observations predicted
autoregressively by feeding prior predictions back into the model.
5.1 Autoregressive Uncertainty Estimation
A reliable estimate of epistemic uncertainty is critical for ensuring robust policy learning in model-
based offline RL. In particular, the ability of a world model to quantify its own uncertainty over
long-horizon rollouts determines the trustworthiness of its synthetic experience. To evaluate this
property in RWM-O, we analyze the alignment between its predicted epistemic uncertainty and actual
model prediction errors during autoregressive trajectory forecasting on ANYmal D. The observation
and action spaces of the world model are detailed in Table S1 and Table S3.
We train the world model on offline data collected from a velocity-tracking policy, operating at a
control frequency of 50Hz. The model is trained with a history horizon of M= 32 and a prediction
horizon of N= 8, using an ensemble of five networks to estimate epistemic uncertainty. To evaluate
uncertainty estimation, we compute epistemic uncertainty as the variance of the ensemble predictions,
following Eq. 4. Aleatoric uncertainty is estimated as the mean of the predicted standard deviations
across ensemble members. Details of the network architecture and training setup are provided in
Sec. A.2.1 and Sec. A.3.1. The predictive error, epistemic uncertainty, and aleatoric uncertainty over
autoregressive rollouts are visualized in Fig. 2.
Att= 32 , the model transitions from conditioning on ground-truth observations to fully autore-
gressive rollouts, wherein predicted states are recursively fed back into the model. As expected, the
accumulated prediction error (grey) increases over time due to compounding model inaccuracies.
Importantly, the estimated epistemic uncertainty (dark blue) closely follows the trend of the predic-
tion error, demonstrating that RWM-O effectively captures uncertainty in regions where the model
generalization deteriorates. In contrast, the aleatoric uncertainty (light blue) remains low, reflecting
small stochasticity in the environment. The strong correlation between epistemic uncertainty and
model prediction error justifies its role as a trust metric for policy optimization. By penalizing
high-uncertainty transitions, MOPO-PPO prevents policy exploitation in poorly modeled regions of
the state space, mitigating the risk of compounding errors in long-horizon rollouts. These results
validate the proposed uncertainty-aware regularization strategy, reinforcing the efficacy of RWM-O
in enabling robust policy learning in offline RL.
5.2 Uncertainty-Penalized Imagination
To evaluate the role of uncertainty-aware regularization in policy learning, we train RWM-O on an
offline dataset collected from an expert velocity-tracking policy on ANYmal D. We then optimize
policies with MOPO-PPO using different values of the uncertainty penalty coefficient λin Eq. 5,
which controls the trade-off between exploration and model reliability. Figure 3 shows the evolution
of the imagination reward and epistemic uncertainty over training iterations, as well as the final policy
evaluation performance.
6
0 100 200 300 400 500 eval
Training Iterations0.20.40.60.8r
0 100 200 300 400 500
Training Iterations0.51.01.52.02.5u
0.0
0.2
0.51.0
2.0
5.0Figure 3: Imagination reward (left) and epistemic uncertainty (right) during MOPO-PPO training. The
policy evaluation on realdynamics is visualized in dots (left). Small penalties lead to overconfident
but unreliable policies, while large penalties result in overly conservative behaviors. A well-calibrated
penalty (dark blue) achieves the exploration-exploitation balance.
When the penalty is small ( λ≤0.5), the policy attains high rewards in imagination but exploits
model inaccuracies, leading to overfitting to hallucinated rollouts. This results in poor evaluation
performance, as the learned policy exhibits overconfident behaviors in simulation but fails catas-
trophically when tested on real dynamics. In these cases, the robot often struggles to maintain stable
locomotion, exhibiting unintended collisions or failing to walk altogether. In contrast, large penalties
(λ≥2.0) force the policy to remain in low-uncertainty regions, limiting exploration and leading to
overly conservative behaviors. These policies exhibit excessive caution, often resulting in the robot
standing in place or making only minor, ineffective movements. Consequently, the learned policy
fails to make meaningful progress, as indicated by the low final evaluation rewards.
With an appropriate penalty ( λ= 1.0), the policy effectively balances exploration and exploitation.
The imagination reward increases steadily, while epistemic uncertainty remains controlled, allowing
the policy to leverage synthetic rollouts without overfitting to model errors. This results in effective
learning, where the robot is able to explore new behaviors in imagination while remaining in
trustworthy regions of the learned model. The strong alignment between training progress and
final evaluation performance highlights the effectiveness of MOPO-PPO in leveraging epistemic
uncertainty to mitigate distribution shift, ultimately enhancing policy robustness in offline MBRL.
5.3 Generality across Robotic Environments
To evaluate the generality and robustness of MOPO-PPO with RWM-O across diverse robotic
environments, we compare its performance against widely adopted offline RL baselines, including
Conservative Q-Learning (CQL) [ 43] with Soft Actor-Critic (SAC) [ 44], MBPO [ 27], MBPO-
PPO [ 26], and MOPO [ 15]. These baselines represent state-of-the-art approaches in model-free and
model-based offline RL. For a fair comparison, we train MBPO and MBPO-PPO in an offline setting,
treating them as uncertainty-unaware counterparts to MOPO and MOPO-PPO.
We benchmark these methods on three robotic tasks in simulation: Reach-Franka (manipulation),
Velocity-G1 (humanoid locomotion), and Velocity-ANYmal-D (quadruped locomotion), spanning a
broad range of dynamics complexities and control challenges. Each method is trained on four dataset
types: Random : trajectories collected from a randomly initialized policy, Medium : trajectories
from a partially trained PPO policy, Expert : trajectories from a fully trained PPO policy, Mixed :
a combination of replay buffer data from PPO training at various stages. All methods are trained
and evaluated under identical conditions. The normalized episodic rewards for each method across
different environments and dataset types are presented in Fig. 4.
Across all methods, performance improves with increasing dataset quality, as richer data distributions
reduce the impact of distribution shift. Policies trained on the Mixed dataset achieve the highest
rewards due to its broad state-action coverage, comparable to those trained on Expert data, which
tends to overfit to specific dynamics and biases the policy towards suboptimal generalization.
7
Random Medium Expert Mixed Random Medium Expert Mixed Random Medium Expert Mixed0.00.20.40.60.8rCQL
MBPO
MBPO-PPOMOPO
MOPO-PPOReach-Franka Velocity-G1 Velocity-Anymal-DFigure 4: Normalized episodic rewards across diverse robotic environments and offline RL algorithms
with different dataset types. MOPO-PPO consistently outperforms uncertainty-unaware baselines,
particularly in complex locomotion tasks.
While CQL and SAC-based model-based methods perform competitively in the manipulation task,
they demonstrate significantly weaker performance in locomotion tasks, where the complexity of
dynamic interactions amplifies the limitations of value-based learning. As highlighted in Sec. 5.2,
MBPO and MBPO-PPO, which lack explicit uncertainty estimation, tend to overfit to model in-
accuracies, leading to overconfident policies. In manipulation tasks, this overfitting is mitigated
when dataset coverage is sufficient. However, in locomotion tasks, it results in catastrophic failures,
including robot collisions and falls.
In contrast, MOPO-PPO consistently outperforms baselines across all tasks, particularly in Velocity-
G1 and Velocity-ANYmal-D, where uncertainty-aware rollouts significantly enhance policy stability.
By penalizing high-uncertainty transitions, MOPO-PPO prevents policies from exploiting hallucinated
model predictions, leading to safer and more reliable locomotion behaviors. The deployment of the
learned policy is visualized in Fig. S5. The strong performance of MOPO-PPO across different dataset
types and environments demonstrates its effectiveness in mitigating distribution shift, reinforcing its
applicability to real-world robotic control.
5.4 Offline Learning from Real-World Data
To validate the real-world applicability of RWM-O and MOPO-PPO, we train a locomotion policy
entirely from offline data collected on the ANYmal D robot, without any reliance on a physics
simulator. We develop an autonomous data collection pipeline with minimal human intervention
in Sec. A.4.2. This experiment demonstrates that our approach eliminates the sim-to-real gap by
learning directly from real-world experience. By leveraging uncertainty-aware rollouts, MOPO-PPO
enables safe and effective policy optimization purely from offline data. A video demonstrating the
learned policy running on hardware is available on our project webpage.
6 Conclusion
In this work, we introduce RWM-O, an uncertainty-aware MBRL framework designed to improve
offline policy learning. By explicitly estimating epistemic uncertainty through an ensemble-based
approach, RWM-O enables uncertainty-aware rollouts, reducing the risk of overfitting to inaccurate
model predictions. We further integrate this uncertainty estimation into policy optimization with
MOPO-PPO, penalizing unreliable transitions and enhancing policy stability. Our experiments
demonstrate that RWM-O improves policy generalization, robustness, and safety across various
robotic tasks, outperforming standard model-based and model-free offline RL baselines. Notably, our
approach enables policy learning purely from real-world data, eliminating the need for high-fidelity
physics simulators and addressing a key limitation of current RL methodologies. By bridging the gap
between model-based offline RL and real-world deployment, RWM-O and MOPO-PPO enhance the
scalability and data efficiency of RL for robotics, paving the way for more reliable and adaptable
policy learning.
8
Limitations
While RWM-O improves offline policy learning by incorporating uncertainty estimation, several
challenges remain. First, the accuracy of the learned dynamics model is inherently limited by the
quality and diversity of the offline dataset. If the dataset lacks sufficient coverage of the state-action
space, the model may struggle with generalization, leading to unreliable predictions in unobserved
regions. Second, although our ensemble-based uncertainty estimation helps mitigate distribution shift,
it does not entirely eliminate compounding errors over long-horizon rollouts. The effectiveness of the
uncertainty penalty in MOPO-PPO relies on careful tuning of hyperparameters, such as the penalty
weight, which may require task-specific adjustments. Finally, while RWM-O and MOPO-PPO remove
the need for a physics simulator, the reliance on offline real-world data means the policy cannot
explore beyond the dataset distribution. In cases where real-world data is insufficient, supplementing
it with simulation data could improve the model’s coverage, but this reintroduces dependence on a
physics simulator. This trade-off can limit applicability in scenarios requiring extensive exploration
or rapid adaptation to new environments.
Acknowledgments
This research was supported by the ETH AI Center.
References
[1]T. Miki, J. Lee, J. Hwangbo, L. Wellhausen, V . Koltun, and M. Hutter. Learning robust
perceptive locomotion for quadrupedal robots in the wild. Science robotics , 7(62):eabk2822,
2022.
[2]C. Li, M. Vlastelica, S. Blaes, J. Frey, F. Grimminger, and G. Martius. Learning agile skills via
adversarial imitation of rough partial demonstrations. In Conference on Robot Learning , pages
342–352. PMLR, 2023.
[3]C. Li, S. Blaes, P. Kolev, M. Vlastelica, J. Frey, and G. Martius. Versatile skill control via
self-supervised adversarial imitation of unlabeled mixed motions. In 2023 IEEE international
conference on robotics and automation (ICRA) , pages 2944–2950. IEEE, 2023.
[4]D. Hoeller, N. Rudin, D. Sako, and M. Hutter. Anymal parkour: Learning agile navigation for
quadrupedal robots. Science Robotics , 9(88):eadi7566, 2024.
[5]K. Cobbe, O. Klimov, C. Hesse, T. Kim, and J. Schulman. Quantifying generalization in
reinforcement learning. In International conference on machine learning , pages 1282–1289.
PMLR, 2019.
[6]E. Bengio, J. Pineau, and D. Precup. Interference and generalization in temporal difference
learning. In International Conference on Machine Learning , pages 767–777. PMLR, 2020.
[7]E. Todorov, T. Erez, and Y . Tassa. Mujoco: A physics engine for model-based control. In 2012
IEEE/RSJ international conference on intelligent robots and systems , pages 5026–5033. IEEE,
2012.
[8]V . Makoviychuk, L. Wawrzyniak, Y . Guo, M. Lu, K. Storey, M. Macklin, D. Hoeller, N. Rudin,
A. Allshire, A. Handa, et al. Isaac gym: High performance gpu-based physics simulation for
robot learning. arXiv preprint arXiv:2108.10470 , 2021.
[9]M. Mittal, C. Yu, Q. Yu, J. Liu, N. Rudin, D. Hoeller, J. L. Yuan, R. Singh, Y . Guo, H. Mazhar,
et al. Orbit: A unified simulation framework for interactive robot learning environments. IEEE
Robotics and Automation Letters , 8(6):3740–3747, 2023.
[10] J. Hwangbo, J. Lee, A. Dosovitskiy, D. Bellicoso, V . Tsounis, V . Koltun, and M. Hutter. Learning
agile and dynamic motor skills for legged robots. Science Robotics , 4(26):eaau5872, 2019.
9
[11] X. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel. Sim-to-real transfer of robotic
control with dynamics randomization. In 2018 IEEE international conference on robotics and
automation (ICRA) , pages 3803–3810. IEEE, 2018.
[12] M. Deisenroth and C. E. Rasmussen. Pilco: A model-based and data-efficient approach to policy
search. In Proceedings of the 28th International Conference on machine learning (ICML-11) ,
pages 465–472, 2011.
[13] K. Chua, R. Calandra, R. McAllister, and S. Levine. Deep reinforcement learning in a handful of
trials using probabilistic dynamics models. Advances in neural information processing systems ,
31, 2018.
[14] I. Clavera, J. Rothfuss, J. Schulman, Y . Fujita, T. Asfour, and P. Abbeel. Model-based reinforce-
ment learning via meta-policy optimization. In Conference on Robot Learning , pages 617–629.
PMLR, 2018.
[15] T. Yu, G. Thomas, L. Yu, S. Ermon, J. Y . Zou, S. Levine, C. Finn, and T. Ma. Mopo: Model-
based offline policy optimization. Advances in Neural Information Processing Systems , 33:
14129–14142, 2020.
[16] R. Agarwal, D. Schuurmans, and M. Norouzi. Striving for simplicity in off-policy deep
reinforcement learning. 2019.
[17] A. Kumar, J. Fu, M. Soh, G. Tucker, and S. Levine. Stabilizing off-policy q-learning via
bootstrapping error reduction. Advances in neural information processing systems , 32, 2019.
[18] Y . Wu, G. Tucker, and O. Nachum. Behavior regularized offline reinforcement learning. arXiv
preprint arXiv:1911.11361 , 2019.
[19] D. Hafner, T. Lillicrap, I. Fischer, R. Villegas, D. Ha, H. Lee, and J. Davidson. Learning latent
dynamics for planning from pixels. In International conference on machine learning , pages
2555–2565. PMLR, 2019.
[20] D. Hafner, T. Lillicrap, J. Ba, and M. Norouzi. Dream to control: Learning behaviors by latent
imagination. arXiv preprint arXiv:1912.01603 , 2019.
[21] F. Ebert, C. Finn, S. Dasari, A. Xie, A. Lee, and S. Levine. Visual foresight: Model-based
deep reinforcement learning for vision-based robotic control. arXiv preprint arXiv:1812.00568 ,
2018.
[22] Y . Gal, R. McAllister, and C. E. Rasmussen. Improving pilco with bayesian neural network
dynamics models. In Data-efficient machine learning workshop, ICML , volume 4, page 25,
2016.
[23] B. Lakshminarayanan, A. Pritzel, and C. Blundell. Simple and scalable predictive uncertainty
estimation using deep ensembles. Advances in neural information processing systems , 30, 2017.
[24] V . Kuleshov, N. Fenner, and S. Ermon. Accurate uncertainties for deep learning using calibrated
regression. In International conference on machine learning , pages 2796–2804. PMLR, 2018.
[25] Y . Ovadia, E. Fertig, J. Ren, Z. Nado, D. Sculley, S. Nowozin, J. Dillon, B. Lakshminarayanan,
and J. Snoek. Can you trust your model’s uncertainty? evaluating predictive uncertainty under
dataset shift. Advances in neural information processing systems , 32, 2019.
[26] C. Li, A. Krause, and M. Hutter. Robotic world model: A neural network simulator for robust
policy optimization in robotics. arXiv preprint arXiv:2501.10100 , 2025.
[27] M. Janner, J. Fu, M. Zhang, and S. Levine. When to trust your model: Model-based policy
optimization. Advances in neural information processing systems , 32, 2019.
10
[28] S. Levine and V . Koltun. Guided policy search. In International conference on machine learning ,
pages 1–9. PMLR, 2013.
[29] S. Fujimoto, D. Meger, and D. Precup. Off-policy deep reinforcement learning without explo-
ration. In International conference on machine learning , pages 2052–2062. PMLR, 2019.
[30] J. Fu, A. Kumar, O. Nachum, G. Tucker, and S. Levine. D4rl: Datasets for deep data-driven
reinforcement learning. arXiv preprint arXiv:2004.07219 , 2020.
[31] V . Kumar, E. Todorov, and S. Levine. Optimal control with learned local models: Application
to dexterous manipulation. In 2016 IEEE International Conference on Robotics and Automation
(ICRA) , pages 378–383. IEEE, 2016.
[32] L. Kaiser, M. Babaeizadeh, P. Milos, B. Osinski, R. H. Campbell, K. Czechowski, D. Erhan,
C. Finn, P. Kozakowski, S. Levine, et al. Model-based reinforcement learning for atari. arXiv
preprint arXiv:1903.00374 , 2019.
[33] D. Hafner, T. Lillicrap, M. Norouzi, and J. Ba. Mastering atari with discrete world models.
arXiv preprint arXiv:2010.02193 , 2020.
[34] D. Hafner, J. Pasukonis, J. Ba, and T. Lillicrap. Mastering diverse domains through world
models. arXiv preprint arXiv:2301.04104 , 2023.
[35] J. Zhu, C. Du, and G. E. Dullerud. Model-based offline reinforcement learning with uncertainty
estimation and policy constraint. IEEE Transactions on Artificial Intelligence , 2024.
[36] Z. Qiao, J. Lyu, K. Jiao, Q. Liu, and X. Li. Sumo: Search-based uncertainty estimation for
model-based offline reinforcement learning. arXiv preprint arXiv:2408.12970 , 2024.
[37] K. Guo, S. Yunfeng, and Y . Geng. Model-based offline reinforcement learning with pessimism-
modulated dynamics belief. Advances in Neural Information Processing Systems , 35:449–461,
2022.
[38] T. Yu, A. Kumar, R. Rafailov, A. Rajeswaran, S. Levine, and C. Finn. Combo: Conservative
offline model-based policy optimization. Advances in neural information processing systems ,
34:28954–28967, 2021.
[39] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction . MIT press, 2018.
[40] R. S. Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM Sigart
Bulletin , 2(4):160–163, 1991.
[41] P. J. Bickel and D. A. Freedman. Some asymptotic theory for the bootstrap. The annals of
statistics , 9(6):1196–1217, 1981.
[42] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization
algorithms. arXiv preprint arXiv:1707.06347 , 2017.
[43] A. Kumar, A. Zhou, G. Tucker, and S. Levine. Conservative q-learning for offline reinforcement
learning. Advances in neural information processing systems , 33:1179–1191, 2020.
[44] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy
deep reinforcement learning with a stochastic actor. In International conference on machine
learning , pages 1861–1870. PMLR, 2018.
11
A Appendix
A.1 Task Representation
A.1.1 Observation and action spaces
The observation space for the ANYmal world model is composed of base linear and angular velocities
v,ωin the robot frame, measurement of the gravity vector in the robot frame g, joint positions q,
velocities ˙qand torques τas in Table S1.
Table S1: World model observation space
Entry Symbol Dimensions
base linear velocity v 0:3
base angular velocity ω 3:6
projected gravity g 6:9
joint positions q 9:21
joint velocities ˙q 21:33
joint torques τ 33:45
The privileged information is used to provide an additional learning objective that implicitly embeds
critical information for accurate long-term predictions. The space is composed of knee and foot
contacts as in Table S2.
Table S2: World model privileged information space
Entry Symbol Dimensions
knee contact − 0:4
foot contact − 4:8
The action space is composed of joint position targets as in Table S3.
Table S3: Action space
Entry Symbol Dimensions
joint position targets q∗0:12
The observation space for the ANYmal velocity tracking policy is composed of base linear and
angular velocities v,ωin the robot frame, measurement of the gravity vector in the robot frame g,
velocity command c, joint positions qand velocities ˙qas in Table S4.
A.1.2 Reward functions
The total reward is sum of the following terms with weights detailed in Table S5.
Linear velocity tracking x, y
rvxy=wvxye−∥cxy−vxy∥2
2/σ2
vxy,
where σvxy= 0.25denotes a temperature factor, cxyandvxydenote the commanded and current
base linear velocity.
Angular velocity tracking
rωz=wωze−∥cz−ωz∥2
2/σ2
ωz,
12
Table S4: Policy observation space
Entry Symbol Dimensions
base linear velocity v 0:3
base angular velocity ω 3:6
projected gravity g 6:9
velocity command c 9:12
joint positions q 12:24
joint velocities ˙q 24:36
Table S5: Reward weights
Symbol wvxy wωz wvzwωxy wqτ
Value 1.0 0 .5−2.0−0.05−2.5e−5
Symbol w¨q w˙a wfa wc wg
Value −2.5e−7−0.01 0 .5−1.0 −5.0
where σωz= 0.25denotes a temperature factor, czandωzdenote the commanded and current base
angular velocity.
Linear velocity z
rvz=wvz∥vz∥2
2,
where vzdenotes the base vertical velocity.
Angular velocity x, y
rωxy=wωxy∥ωxy∥2
2,
where ωxydenotes the current base roll and pitch velocity.
Joint torque
rqτ=wqτ∥τ∥2
2,
where τdenotes the joint torques.
Joint acceleration
r¨q=w¨q∥¨q∥2
2,
where ¨qdenotes the joint acceleration.
Action rate
r˙a=w˙a∥a′−a∥2
2,
where a′andadenote the previous and current actions.
Feet air time
rfa=wfatfa,
where tfadenotes the sum of the time for which the feet are in the air.
13
Undesired contacts
rc=wccu,
where cudenotes the counts of the undesired knee contacts.
Flat orientation
rg=wgg2
xy,
where gxydenotes the xy-components of the projected gravity.
A.2 Network Architecture
A.2.1 RWM-O
The robotic world model consists of a GRU base and MLP heads predicting the mean and standard
deviation of the next observation and privileged information such as contacts, as detailed in Table S6.
Table S6: RWM-O architecture
Component Type Hidden Shape Activation
base GRU 256, 256 −
heads MLP 128 ReLU
A.2.2 MOPO-PPO
The network architectures of the policy and the value function used in MOPO-PPO are detailed in
Table S7.
Table S7: Policy and value function architecture
Network Type Hidden Shape Activation
policy MLP 128, 128, 128 ELU
value function MLP 128, 128, 128 ELU
A.3 Training Parameters
The learning networks and algorithm are implemented in PyTorch 2.4.0 with CUDA 12.6 and trained
on an NVIDIA RTX 4090 GPU.
A.3.1 RWM-O
The training information of RWM-O is summarized in Table S8.
A.3.2 MOPO-PPO
The training information of MOPO-PPO is summarized in Table S9.
A.4 Offline Policy Learning
A.4.1 Policy Deployment Learned with MOPO-PPO
The policy deployment learning with MOPO-PPO is visualized in Fig. S5.
14
Table S8: RWM-O training parameters
Parameter Symbol Value
step time seconds ∆t 0.02
max iterations − 2500
learning rate − 1e−4
weight decay − 1e−5
batch size − 1024
ensemble size B 5
history horizon M 32
forecast horizon N 8
forecast decay α 1.0
transitions in training data − 6M
approximate training hours − 1
number of seeds − 5
Table S9: MOPO-PPO training parameters
Parameter Symbol Value
imagination environments − 4096
imagination steps per iteration − 100
uncertainty penalty weight λ 1.0
step time seconds ∆t 0.02
buffer size |D| 1000
max iterations − 2500
learning rate − 0.001
weight decay − 0.0
learning epochs − 5
mini-batches − 4
KL divergence target − 0.01
discount factor γ 0.99
clip range ϵ 0.2
entropy coefficient − 0.005
number of seeds − 5
A.4.2 Real-World Data Collection
Although RWM-O can be trained on offline data from arbitrary sources, we develop an autonomous
pipeline for collecting additional real-world data with minimal human intervention. The robot
executes a velocity-tracking policy and records transitions as described in Sec. A.1.1. A visualization
of the collection setup is shown in Fig. S6. To ensure safety, a rectangular boundary is defined
relative to the odometry origin. Robot odometry is computed by fusing legged odometry and LiDAR
data. At each sampling interval, a base velocity command (green arrow) is randomly drawn, and the
resulting final pose (red sphere) is predicted via finite integration. If this predicted pose lies outside
the safety boundary, a new command is sampled. After a fixed number of retries, a fallback command
that drives the robot toward the origin is used instead. To mitigate odometry drift, the safety region
can be re-centered around the robot’s current position using a joystick override. In total, we collect
approximately one hour of data corresponding to 180K state transitions.
A.4.3 Additional Offline Data Sources
RWM-O can achieve reasonable performance when trained exclusively on real-world data, largely
due to the inherent distributional bias in the dataset. Since all trajectories are collected from an expert
policy without failures such as falls or collisions, the world model only needs to generalize within
a narrow, well-behaved region of the state-action space. MOPO-PPO capitalizes on this bias by
penalizing transitions with high epistemic uncertainty, discouraging the policy from entering regions
15
0.0
1.0
2.0
3.0
4.0
5.0
6.0
7.0t[s]Figure S5: MOPO-PPO policy deployment learned from offline Mixed dataset across various
environments.
Figure S6: RViz visualization of the real-world data collection process. The green arrow indicates a
sampled base velocity command; the red sphere marks the estimated final pose via finite integration.
The red box illustrates the safety boundary around the odometry origin, which the robot must remain
within during data collection.
outside the dataset’s support. However, this mechanism alone is insufficient for fully stable policy
learning. In particular, the absence of privileged signals, such as contact labels, limits the ability of
RWM-O to predict physically grounded dynamics, especially during transitions involving contact
events. These signals are critical for accurate reward computation, and their absence introduces noise
in policy gradients, leading to instability during training. To mitigate this, additional offline data from
simulation, with access to privileged information, can be incorporated to enhance the world model’s
predictive capabilities. This improves the fidelity of reward estimation during imagination rollouts,
resulting in more stable and robust policies that transfer reliably to hardware.
16